{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV - CNN Filter Pruning - MNIST - Part 2\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)\n",
    "\n",
    "This notebook takes you through the implementation of filter pruning of CNNs using the MNIST dataset on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f71f01d2c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train = datasets.MNIST('../../../data/computer_vision/mnist', train=True, download=True, transform=data_transform)\n",
    "\n",
    "test = datasets.MNIST('../../../data/computer_vision/mnist', train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images (batch, channels, height, width):  torch.Size([64, 1, 28, 28])\n",
      "Output labels:  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print(\"Input images (batch, channels, height, width): \", images.size())\n",
    "print(\"Output labels: \", labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example image from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71e830ac50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MnistClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, kernel_size=3, stride=1, bias=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(12*12*4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip the following part and directly load a saved model given a few cells below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 7\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = MnistClassifier()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for e in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_probs = model(X_batch)\n",
    "        loss = criterion(y_pred_probs, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, y_pred = torch.max(y_pred_probs, dim = 1)\n",
    "        batch_acc = (y_pred == y_batch).sum().item()/BATCH_SIZE\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += batch_acc\n",
    "    \n",
    "    print(f'Epoch {e+0:02}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.5f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist_2_layer_cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model (on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = MnistClassifier()\n",
    "model.load_state_dict(torch.load(\"mnist_2_layer_no_bias_cnn\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "\n",
      "Parameters before pruning:  6130\n"
     ]
    }
   ],
   "source": [
    "print(\"Device: \", device)\n",
    "params_before_pruning = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\nParameters before pruning: \", params_before_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77 s ± 458 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_probs = model(X_batch)\n",
    "        _, y_pred = torch.max(y_pred_probs, dim = 1)\n",
    "        \n",
    "        y_pred_list.append(y_pred.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      7840\n",
      "           1       0.97      0.98      0.98      9080\n",
      "           2       0.96      0.93      0.94      8256\n",
      "           3       0.99      0.87      0.93      8080\n",
      "           4       0.86      0.98      0.92      7856\n",
      "           5       0.94      0.95      0.94      7136\n",
      "           6       0.95      0.96      0.96      7664\n",
      "           7       0.98      0.92      0.95      8224\n",
      "           8       0.83      0.95      0.89      7792\n",
      "           9       0.97      0.84      0.90      8072\n",
      "\n",
      "    accuracy                           0.94     80000\n",
      "   macro avg       0.94      0.94      0.94     80000\n",
      "weighted avg       0.94      0.94      0.94     80000\n",
      "\n",
      "\n",
      "Parameters before pruning:  6130\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(\"\\nParameters before pruning: \", params_before_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruneModel():\n",
    "    def __init__(self, cnn_model):\n",
    "        self.cnn_model = cnn_model\n",
    "        \n",
    "    def get_model_layer_name(self, layer_type):\n",
    "        if layer_type == \"conv_weight\":\n",
    "            layer_names = [name for name, _ in self.cnn_model.named_parameters() if \"conv\" in name and \"weight\" in name]\n",
    "        elif layer_type == \"conv_bias\":\n",
    "            layer_names = [name for name, _ in self.cnn_model.named_parameters() if \"conv\" in name and \"bias\" in name]\n",
    "            \n",
    "        return layer_names\n",
    "            \n",
    "    \n",
    "    def calc_filter_norm(self, conv_weight_layer_name_list):\n",
    "        filter_norm_list = []\n",
    "        \n",
    "        for name, param in self.cnn_model.named_parameters():\n",
    "            if name in conv_weight_layer_name_list:\n",
    "                temp_conv_layer_norm_list = []\n",
    "                for wt in param.data:\n",
    "                    temp_conv_layer_norm_list.append(torch.norm(wt))\n",
    "                filter_norm_list.append(temp_conv_layer_norm_list)\n",
    "                    \n",
    "        return filter_norm_list\n",
    "        \n",
    "    \n",
    "    def worst_n_number_filters(self, norm_list, n):\n",
    "        norm_array_list = [np.array(i) for i in norm_list]\n",
    "        worst_norm_index_list = [list(np.argpartition(i, n))[:n] for i in norm_array_list]\n",
    "\n",
    "        return worst_norm_index_list\n",
    "    \n",
    "    \n",
    "    def remove_bad_filters(self, index_list, conv_weight_layer_name_list):\n",
    "        num_filters_remove = len(index_list[0])\n",
    "        \n",
    "        for i, (name, param) in enumerate(self.cnn_model.named_parameters()):\n",
    "            if name in conv_weight_layer_name_list:\n",
    "                mask = torch.zeros_like(param.data).bool()\n",
    "                for j in index_list[i]:\n",
    "                    mask[j] = 1\n",
    "\n",
    "                param.data = torch.masked_select(param.data, ~mask).view(param.data.shape[0]-num_filters_remove,\n",
    "                                                                         param.data.shape[1],\n",
    "                                                                         param.data.shape[2],\n",
    "                                                                         param.data.shape[3])\n",
    "\n",
    "        return self.cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_weight_layer_names:\n",
      " ['conv1.weight', 'conv2.weight']\n",
      "\n",
      "norm_list:\n",
      " [[tensor(16.5732), tensor(0.5417), tensor(15.6806), tensor(9.7491), tensor(18.3170), tensor(19.6589), tensor(19.1068), tensor(19.6257)], [tensor(50.5786), tensor(47.4790), tensor(44.8558), tensor(52.1951)]]\n",
      "\n",
      "index_list:\n",
      " [[1, 3], [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "prune_model = PruneModel(model)\n",
    "\n",
    "conv_weight_layer_names = prune_model.get_model_layer_name(layer_type = \"conv_weight\")\n",
    "print(\"conv_weight_layer_names:\\n\", conv_weight_layer_names)\n",
    "# conv_bias_layer_names = prune_model.get_model_layer_name(layer_type = \"conv_bias\")\n",
    "# conv_weight_bias_layer_names = conv_weight_layer_names+conv_bias_layer_names\n",
    "\n",
    "norm_list = prune_model.calc_filter_norm(conv_weight_layer_names)\n",
    "print(\"\\nnorm_list:\\n\", norm_list)\n",
    "\n",
    "index_list = prune_model.worst_n_number_filters(norm_list, 2)\n",
    "print(\"\\nindex_list:\\n\", index_list)\n",
    "\n",
    "\n",
    "pruned_model = prune_model.remove_bad_filters(index_list, conv_weight_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 3, 3]) \n",
      " tensor([[[[-6.5707, -3.7024, -3.1393],\n",
      "          [-5.2872, -5.7998, -5.7056],\n",
      "          [-5.2720, -6.3201, -6.7859]]],\n",
      "\n",
      "\n",
      "        [[[-1.6838, -0.4287, -0.1215],\n",
      "          [-5.8476, -5.0102, -6.3054],\n",
      "          [-6.9386, -6.8714, -6.9594]]],\n",
      "\n",
      "\n",
      "        [[[-6.3545, -6.7356, -6.7520],\n",
      "          [-4.6402, -6.4204, -6.7152],\n",
      "          [-4.0202, -6.3933, -6.2684]]],\n",
      "\n",
      "\n",
      "        [[[-6.7397, -6.7190, -6.9085],\n",
      "          [-6.2247, -6.6597, -6.6372],\n",
      "          [-6.7797, -6.5152, -5.7109]]],\n",
      "\n",
      "\n",
      "        [[[-6.2283, -6.5689, -6.9430],\n",
      "          [-6.4033, -6.9510, -6.4704],\n",
      "          [-4.9063, -6.7165, -5.8785]]],\n",
      "\n",
      "\n",
      "        [[[-6.8551, -6.8493, -6.8041],\n",
      "          [-6.1269, -6.6609, -6.1992],\n",
      "          [-6.6728, -6.2998, -6.3574]]]])\n",
      "conv2.weight torch.Size([2, 8, 3, 3]) \n",
      " tensor([[[[-5.7547e+00, -5.9268e+00, -6.4286e+00],\n",
      "          [ 5.7408e+00, -6.4684e+00, -6.4968e+00],\n",
      "          [ 3.4554e+00, -3.6581e+00, -5.1142e+00]],\n",
      "\n",
      "         [[-2.6155e+00, -4.3112e-02,  1.7984e-01],\n",
      "          [-3.0478e+00, -1.8799e-01,  6.9365e-03],\n",
      "          [ 4.2709e-01, -1.4878e-01,  4.0615e-01]],\n",
      "\n",
      "         [[-6.4812e+00, -6.5354e+00, -6.6560e+00],\n",
      "          [-1.0465e-01, -6.7412e+00, -6.7742e+00],\n",
      "          [-6.4677e+00, -6.3783e+00, -6.7566e+00]],\n",
      "\n",
      "         [[-5.0030e+00, -6.8696e+00, -6.7430e+00],\n",
      "          [-5.5756e+00, -6.4694e+00, -6.8513e+00],\n",
      "          [-5.9188e+00, -6.4289e+00, -5.6771e+00]],\n",
      "\n",
      "         [[-6.7907e+00, -6.8117e+00, -6.5584e+00],\n",
      "          [-6.8118e+00,  6.4348e+00,  6.6276e+00],\n",
      "          [-6.3359e+00,  6.4827e+00, -4.4146e+00]],\n",
      "\n",
      "         [[-6.6750e+00, -6.7007e+00,  6.7095e+00],\n",
      "          [-6.7617e+00, -6.2682e+00,  6.6705e+00],\n",
      "          [-6.6400e+00, -6.7095e+00, -6.6302e+00]],\n",
      "\n",
      "         [[-6.8331e+00, -6.7479e+00, -6.6370e+00],\n",
      "          [-6.7837e+00, -6.8485e+00, -6.7185e+00],\n",
      "          [-6.7675e+00, -6.7696e+00, -6.5923e+00]],\n",
      "\n",
      "         [[-6.7117e+00, -6.7134e+00, -6.7090e+00],\n",
      "          [-6.6962e+00, -6.4672e+00,  6.7115e+00],\n",
      "          [-6.7120e+00,  6.4080e+00, -6.3679e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.1840e+00, -6.2650e+00, -6.6146e+00],\n",
      "          [-6.2575e+00, -6.4676e+00, -6.5376e+00],\n",
      "          [-6.5961e+00, -6.6660e+00, -6.6658e+00]],\n",
      "\n",
      "         [[ 3.1041e-01, -3.6323e+00, -6.1893e+00],\n",
      "          [ 8.5224e-01, -2.7767e+00, -6.5406e+00],\n",
      "          [-2.0864e+00, -6.1187e+00, -6.8298e+00]],\n",
      "\n",
      "         [[-6.5574e+00, -6.2137e+00, -3.6228e+00],\n",
      "          [-6.6321e+00, -4.9894e+00, -4.2727e+00],\n",
      "          [-6.3118e+00, -5.1922e+00, -6.5925e+00]],\n",
      "\n",
      "         [[-6.4712e+00, -6.8282e+00, -6.9530e+00],\n",
      "          [-5.5499e+00, -6.7817e+00, -6.3059e+00],\n",
      "          [-6.7296e+00, -6.6468e+00, -6.4917e+00]],\n",
      "\n",
      "         [[-6.6894e+00, -6.7562e+00, -6.7525e+00],\n",
      "          [-5.2214e+00, -6.0059e+00, -5.6618e+00],\n",
      "          [-5.2212e+00, -5.1972e+00, -5.6609e+00]],\n",
      "\n",
      "         [[-6.6031e+00, -6.4367e+00,  6.6484e+00],\n",
      "          [-6.7868e+00,  6.7807e+00,  6.6579e+00],\n",
      "          [-6.6357e+00,  6.5845e+00, -5.3660e+00]],\n",
      "\n",
      "         [[-6.5797e+00, -6.6957e+00, -6.9156e+00],\n",
      "          [-6.5622e+00, -6.6638e+00, -6.9411e+00],\n",
      "          [-6.6686e+00,  6.0754e+00, -6.3393e+00]],\n",
      "\n",
      "         [[-6.5415e+00, -6.6721e+00, -6.5910e+00],\n",
      "          [-6.7444e+00, -6.7669e+00,  6.6224e+00],\n",
      "          [-6.7457e+00, -6.6446e+00,  6.4630e+00]]]])\n",
      "fc.weight torch.Size([10, 576]) \n",
      " tensor([[-6.8934, -6.7902, -0.8328,  ..., -6.8047, -6.8834, -6.8074],\n",
      "        [ 5.0174,  4.8428, -6.6273,  ..., -3.0143, -2.0679,  1.6380],\n",
      "        [ 6.7565, -2.9227,  4.6932,  ...,  0.0421, -6.4354, -6.2165],\n",
      "        ...,\n",
      "        [-6.8289, -6.8921, -6.6481,  ..., -3.5203, -6.5499, -6.8814],\n",
      "        [-4.9356,  6.2262,  3.3442,  ...,  5.5354,  5.3722, -0.0251],\n",
      "        [-6.7355, -6.7159, -6.7398,  ..., -1.5951,  6.4521,  6.7258]])\n",
      "fc.bias torch.Size([10]) \n",
      " tensor([-0.9988,  6.7844, -0.8754, -4.0525,  1.8455, -3.3575,  1.1344,  3.0825,\n",
      "        -6.0543, -3.8620])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape, \"\\n\", param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=1, bias=False)\n",
    "model.conv2 = nn.Conv2d(6, 2, kernel_size=3, stride=1, bias=False)\n",
    "model.fc = nn.Linear(288, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device: \", device)\n",
    "params_after_pruning = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy of changed architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.55 s ± 92.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_probs = model(X_batch)\n",
    "        _, y_pred = torch.max(y_pred_probs, dim = 1)\n",
    "        \n",
    "        y_pred_list.append(y_pred.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.03      0.04      7840\n",
      "           1       0.13      0.36      0.19      9080\n",
      "           2       0.00      0.00      0.00      8256\n",
      "           3       0.08      0.00      0.01      8080\n",
      "           4       0.00      0.00      0.00      7856\n",
      "           5       0.00      0.00      0.00      7136\n",
      "           6       0.03      0.14      0.05      7664\n",
      "           7       0.03      0.01      0.02      8224\n",
      "           8       0.15      0.25      0.19      7792\n",
      "           9       0.00      0.00      0.00      8072\n",
      "\n",
      "    accuracy                           0.08     80000\n",
      "   macro avg       0.05      0.08      0.05     80000\n",
      "weighted avg       0.05      0.08      0.05     80000\n",
      "\n",
      "Parameters after pruning:  3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshaj/miniconda3/envs/toothless/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(\"Parameters after pruning: \", params_after_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Pruned Model (on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "MnistClassifier(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (fc): Linear(in_features=288, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: | Loss: 0.32044 | Acc: 0.90577\n",
      "Epoch 01: | Loss: 0.20870 | Acc: 0.94066\n",
      "Epoch 02: | Loss: 0.19793 | Acc: 0.94261\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_probs = model(X_batch)\n",
    "        loss = criterion(y_pred_probs, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, y_pred = torch.max(y_pred_probs, dim = 1)\n",
    "        batch_acc = (y_pred == y_batch).sum().item()/BATCH_SIZE\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += batch_acc\n",
    "    \n",
    "    print(f'Epoch {e+0:02}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.5f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy of pruned model (on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistClassifier(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=288, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64 s ± 186 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_probs = model(X_batch)\n",
    "        _, y_pred = torch.max(y_pred_probs, dim = 1)\n",
    "        \n",
    "        y_pred_list.append(y_pred.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97     15680\n",
      "           1       0.98      0.99      0.99     18160\n",
      "           2       0.95      0.94      0.94     16512\n",
      "           3       0.95      0.95      0.95     16160\n",
      "           4       0.93      0.97      0.95     15712\n",
      "           5       0.91      0.95      0.93     14272\n",
      "           6       0.97      0.96      0.96     15328\n",
      "           7       0.89      0.95      0.92     16448\n",
      "           8       0.96      0.88      0.92     15584\n",
      "           9       0.94      0.89      0.91     16144\n",
      "\n",
      "    accuracy                           0.95    160000\n",
      "   macro avg       0.95      0.94      0.94    160000\n",
      "weighted avg       0.95      0.95      0.95    160000\n",
      "\n",
      "Parameters after pruning:  3052\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(\"Parameters after pruning: \", params_after_pruning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
