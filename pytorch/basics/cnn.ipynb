{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics - CNN\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)\n",
    "\n",
    "This notebook takes you through basics of CNN on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff753fe8eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of CNN operations\n",
    "\n",
    "CNNs are majorly used for applications sorrounding images, audio, videos, text, and time series modelling. There are 3 types of convolution operations.  \n",
    "\n",
    "1. 1D convolution\n",
    "2. 2D convolution\n",
    "3. 3D convolution\n",
    "\n",
    "\n",
    "1D convolutions are majorly used where the input is sequential such as text or audio. 2D convolutions are majorly used where the input is an image.\n",
    "\n",
    "\n",
    "#### 1D convolution for 1D input:\n",
    "\n",
    "<img src=\"../../assets/conv1d_input1d.jpg\" />\n",
    "\n",
    "\n",
    "#### 1D convolution for 2D input:\n",
    "\n",
    "<img src=\"../../assets/conv1d_input2d.jpg\" />\n",
    "\n",
    "#### 2D convolution for 2D input\n",
    "\n",
    "<img src=\"../../assets/conv2d_input2d.png\" />\n",
    "\n",
    "\n",
    "\n",
    "For more information about CNNs, check out the links from the references at the end. I've taken the diagrams and pictures from there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype = torch.float)\n",
    "input_2d = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], dtype = torch.float)\n",
    "input_2d_img = torch.tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]], dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1D:\n",
      "\n",
      "input_1d.shape:  torch.Size([10])\n",
      "\n",
      "input_1d: \n",
      " tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "=====================================================================\n",
      "\n",
      "Input 2D:\n",
      "\n",
      "input_2d.shape:  torch.Size([2, 5])\n",
      "\n",
      "input_2d:\n",
      " tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10.]])\n",
      "=====================================================================\n",
      "\n",
      "input_2d_img:\n",
      "\n",
      "input_2d_img.shape:  torch.Size([3, 3, 10])\n",
      "\n",
      "input_2d_img:\n",
      " tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input 1D:\\n\")\n",
    "print(\"input_1d.shape: \", input_1d.shape)\n",
    "print(\"\\ninput_1d: \\n\", input_1d)\n",
    "\n",
    "print(\"=\" * 69)\n",
    "\n",
    "print(\"\\nInput 2D:\\n\")\n",
    "print(\"input_2d.shape: \", input_2d.shape)\n",
    "print(\"\\ninput_2d:\\n\", input_2d)\n",
    "\n",
    "print(\"=\" * 69)\n",
    "\n",
    "print(\"\\ninput_2d_img:\\n\")\n",
    "print(\"input_2d_img.shape: \", input_2d_img.shape)\n",
    "print(\"\\ninput_2d_img:\\n\", input_2d_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d - Input 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d = input_1d.unsqueeze(0).unsqueeze(0)\n",
    "input_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_1: \n",
      "\n",
      "torch.Size([1, 1, 8]) \n",
      "\n",
      "tensor([[[-1.2353, -1.4051, -1.5749, -1.7447, -1.9145, -2.0843, -2.2541,\n",
      "          -2.4239]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn1d_1: \\n\")\n",
    "print(cnn1d_1(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_1(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_2: \n",
      "\n",
      "torch.Size([1, 1, 4]) \n",
      "\n",
      "tensor([[[0.5107, 0.3528, 0.1948, 0.0368]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n",
    "\n",
    "print(\"cnn1d_2: \\n\")\n",
    "print(cnn1d_2(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_2(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_3: \n",
      "\n",
      "torch.Size([1, 1, 9]) \n",
      "\n",
      "tensor([[[0.0978, 0.2221, 0.3465, 0.4708, 0.5952, 0.7196, 0.8439, 0.9683,\n",
      "          1.0926]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)\n",
    "\n",
    "print(\"cnn1d_3: \\n\")\n",
    "print(cnn1d_3(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_3(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_4: \n",
      "\n",
      "torch.Size([1, 5, 8]) \n",
      "\n",
      "tensor([[[-1.8410e+00, -2.8884e+00, -3.9358e+00, -4.9832e+00, -6.0307e+00,\n",
      "          -7.0781e+00, -8.1255e+00, -9.1729e+00],\n",
      "         [-4.6073e-02, -3.4436e-02, -2.2799e-02, -1.1162e-02,  4.7439e-04,\n",
      "           1.2111e-02,  2.3748e-02,  3.5385e-02],\n",
      "         [-1.5541e+00, -1.8505e+00, -2.1469e+00, -2.4433e+00, -2.7397e+00,\n",
      "          -3.0361e+00, -3.3325e+00, -3.6289e+00],\n",
      "         [ 6.6593e-01,  1.2362e+00,  1.8066e+00,  2.3769e+00,  2.9472e+00,\n",
      "           3.5175e+00,  4.0878e+00,  4.6581e+00],\n",
      "         [ 2.0414e-01,  6.0421e-01,  1.0043e+00,  1.4044e+00,  1.8044e+00,\n",
      "           2.2045e+00,  2.6046e+00,  3.0046e+00]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_4 = nn.Conv1d(in_channels=1, out_channels=5, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn1d_4: \\n\")\n",
    "print(cnn1d_4(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_4(input_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d - Input 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d = input_2d.unsqueeze(0)\n",
    "input_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_5: \n",
      "\n",
      "torch.Size([1, 1, 3]) \n",
      "\n",
      "tensor([[[-6.6836, -7.6893, -8.6950]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_5 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn1d_5: \\n\")\n",
    "print(cnn1d_5(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_5(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_6: \n",
      "\n",
      "torch.Size([1, 1, 2]) \n",
      "\n",
      "tensor([[[-3.4744, -3.7142]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_6 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=2)\n",
    "\n",
    "print(\"cnn1d_6: \\n\")\n",
    "print(cnn1d_6(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_6(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_7: \n",
      "\n",
      "torch.Size([1, 1, 4]) \n",
      "\n",
      "tensor([[[0.5619, 0.6910, 0.8201, 0.9492]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_7 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=2, stride=1)\n",
    "\n",
    "print(\"cnn1d_7: \\n\")\n",
    "print(cnn1d_7(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_7(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_8: \n",
      "\n",
      "torch.Size([1, 5, 3]) \n",
      "\n",
      "tensor([[[ 1.5024,  2.4199,  3.3373],\n",
      "         [ 0.2980, -0.0873, -0.4727],\n",
      "         [ 1.5443,  1.7086,  1.8729],\n",
      "         [ 2.6177,  3.2974,  3.9772],\n",
      "         [-2.5145, -2.2906, -2.0668]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_8 = nn.Conv1d(in_channels=2, out_channels=5, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn1d_8: \\n\")\n",
    "print(cnn1d_8(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_8(input_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d - Input 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d_img = input_2d_img.unsqueeze(0)\n",
    "input_2d_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_1: \n",
      "\n",
      "torch.Size([1, 1, 1, 8]) \n",
      "\n",
      "tensor([[[[-1.0716, -1.5742, -2.0768, -2.5793, -3.0819, -3.5844, -4.0870,\n",
      "           -4.5896]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn2d_1: \\n\")\n",
    "print(cnn2d_1(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_1(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_2: \n",
      "\n",
      "torch.Size([1, 1, 1, 4]) \n",
      "\n",
      "tensor([[[[-0.7407, -1.2801, -1.8195, -2.3590]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=2)\n",
    "\n",
    "print(\"cnn2d_2: \\n\")\n",
    "print(cnn2d_2(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_2(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_3: \n",
      "\n",
      "torch.Size([1, 1, 2, 9]) \n",
      "\n",
      "tensor([[[[-0.8046, -1.5066, -2.2086, -2.9107, -3.6127, -4.3147, -5.0167,\n",
      "           -5.7188, -6.4208],\n",
      "          [-0.8046, -1.5066, -2.2086, -2.9107, -3.6127, -4.3147, -5.0167,\n",
      "           -5.7188, -6.4208]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_3 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, stride=1)\n",
    "\n",
    "print(\"cnn2d_3: \\n\")\n",
    "print(cnn2d_3(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_3(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_4: \n",
      "\n",
      "torch.Size([1, 5, 1, 8]) \n",
      "\n",
      "tensor([[[[-2.0868e+00, -2.7669e+00, -3.4470e+00, -4.1271e+00, -4.8072e+00,\n",
      "           -5.4873e+00, -6.1673e+00, -6.8474e+00]],\n",
      "\n",
      "         [[-4.5052e-01, -5.5917e-01, -6.6783e-01, -7.7648e-01, -8.8514e-01,\n",
      "           -9.9380e-01, -1.1025e+00, -1.2111e+00]],\n",
      "\n",
      "         [[ 6.6228e-01,  8.3826e-01,  1.0142e+00,  1.1902e+00,  1.3662e+00,\n",
      "            1.5422e+00,  1.7181e+00,  1.8941e+00]],\n",
      "\n",
      "         [[-5.4425e-01, -1.2149e+00, -1.8855e+00, -2.5561e+00, -3.2267e+00,\n",
      "           -3.8973e+00, -4.5679e+00, -5.2385e+00]],\n",
      "\n",
      "         [[ 2.0564e-01,  1.6357e-01,  1.2150e-01,  7.9434e-02,  3.7365e-02,\n",
      "           -4.7036e-03, -4.6773e-02, -8.8842e-02]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_4 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, stride=1)\n",
    "\n",
    "print(\"cnn2d_4: \\n\")\n",
    "print(cnn2d_4(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_4(input_2d_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n\n",
    "\n",
    "2. https://medium.com/@santi.pdp/how-pytorch-transposed-convs1d-work-a7adac63c4a5\n",
    "3. https://www.oreilly.com/library/view/natural-language-processing/9781491978221/ch04.html\n",
    "4. https://pytorch.org/docs/stable/nn.html#conv1d\n",
    "5. https://www.youtube.com/watch?v=wNBaNhvL4pg\n",
    "6. https://arxiv.org/pdf/1708.02709.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
