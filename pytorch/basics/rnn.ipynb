{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics - RNN\n",
    "\n",
    "### By [Akshaj Verma](https://akshajverma.com)\n",
    "\n",
    "This notebook takes you through basics of RNN on PyTorch along with a small demo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8128032f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RNNs\n",
    "\n",
    "RNNs are mainly used in case of sequential data such as time series or NLP. There are multiple different types of RNNs which are used for different applications.  \n",
    "\n",
    "<img src=\"../../assets/rnn_karpathy.jpeg\" />\n",
    "\n",
    "For time series, we might use a many-to-many or many-to-one dependingn upon how many time-steps are we predicting for. We can either predict for 1 time-step in the future based on previous data or we can predict multiple days into the future based on previous data.  \n",
    "\n",
    "In case of NLP, different applications might be:  \n",
    "\n",
    "* Text Classification: many-to-one\n",
    "* Text Generation: many-to-many\n",
    "* Machine Translation: many-to-many\n",
    "* Named Entity Recognition: many-to-many\n",
    "* Image Captioning: one-to-many\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### many-to-many\n",
    "\n",
    "In case of many-to-one networks, we care about all the outputs in the network.\n",
    "\n",
    "<img src=\"../../assets/rnn_many_to_many.jpg\" />\n",
    "\n",
    "\n",
    "### many-to-one\n",
    "\n",
    "In case of many-to-one networks, we only care about the final output and not the intermediate outputs in the network.\n",
    "\n",
    "<img src=\"../../assets/rnn_many_to_one.jpg\" />\n",
    "\n",
    "\n",
    "\n",
    "### stacked rnns\n",
    "\n",
    "We often stack rnns together for better performance.\n",
    "\n",
    "<img src=\"../../assets/stacked_lstm.png\" />\n",
    "\n",
    "\n",
    "### Bidirectional RNN\n",
    "\n",
    "<img src=\"../../assets/birnn.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains RNNs in pytorch using a watered-down version of time series forecasting. We will build a many-to-many model to predict numbers. This can by converted to a many-to-one model by simply changing the hidden size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is:   \n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  \n",
    "\n",
    "We divide it into 4 batches of sequence length = 5.  \n",
    "[[1, 2, 3, 4, 5],  \n",
    "[6, 7, 8, 9, 10],  \n",
    "[11, 12, 13, 14, 15],  \n",
    "[16, 17, 18, 19, 20]]\n",
    "\n",
    "\n",
    "Batch Size = 4  \n",
    "Sequence length = 5  \n",
    "input size = 1 (Since, only one dimension)  \n",
    "\n",
    "`hidden_size` refers to the size of the output from RNN.  \n",
    "\n",
    "In our case, we're looking at 5 (seq_len) previous value to predict the next 2 (hidden_size) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      " tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16., 17., 18., 19., 20.]) \n",
      "\n",
      "Data Shape: \n",
      " torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# data = torch.Tensor([[1, 2, 3, 4, 5],\n",
    "#                     [6, 7, 8, 9, 10],\n",
    "#                     [11, 12, 13, 14, 15],\n",
    "#                     [16, 17, 18, 19, 20]])\n",
    "\n",
    "data = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "\n",
    "print(\"Data:\\n\", data, \"\\n\")\n",
    "print(\"Data Shape: \\n\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features used as input. (Number of columns)\n",
    "INPUT_SIZE = 1\n",
    "\n",
    "# Number of previous time stamps taken into account.\n",
    "SEQ_LENGTH = 5\n",
    "\n",
    "# Number of output units.\n",
    "HIDDEN_SIZE = 2\n",
    "\n",
    "# Number of stacked rnn layers.\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "# We have total of 20 rows in our input. \n",
    "# We divide into 4 batches of 5 rows each.\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rnn has two outputs - `out` and `hidden`.   \n",
    "\n",
    "* `out` is the output of the rnn at each layer.\n",
    "* `hidden` is output of the rnn at the last layer.  \n",
    "\n",
    "\n",
    "If we don't initialize the hidden layer, it will be auto-initiliased by PyTorch to be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 1, batch_first=True)\n",
    "\n",
    "# input size : (batch_size , seq_length, input_size)\n",
    "inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "\n",
    "# hidden state size : (num_layers * num_directions, batch, hidden_size)\n",
    "# hidden = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "out, hidden = rnn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output below, notice the last row in each batch of `output` is present in `hidden`. `output` contains all the outputs at each time step.  \n",
    "\n",
    "We have 4 batches as our output because we set the `BATCH_SIZE=4`. Each batch contains 5 rows because out `SEQ_LENGTH = 5`. In a batch, we have 2 columns as well because `HIDDEN_SIZE=2` ie. number of future time-step predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n",
      "\n",
      "Output:\n",
      " tensor([[[-0.0819,  0.8100],\n",
      "         [-0.4311,  0.9332],\n",
      "         [-0.3162,  0.9748],\n",
      "         [-0.3979,  0.9875],\n",
      "         [-0.3675,  0.9944]],\n",
      "\n",
      "        [[-0.1081,  0.9953],\n",
      "         [-0.5145,  0.9986],\n",
      "         [-0.3269,  0.9995],\n",
      "         [-0.4254,  0.9997],\n",
      "         [-0.3820,  0.9999]],\n",
      "\n",
      "        [[-0.1342,  0.9999],\n",
      "         [-0.5245,  1.0000],\n",
      "         [-0.3458,  1.0000],\n",
      "         [-0.4382,  1.0000],\n",
      "         [-0.3982,  1.0000]],\n",
      "\n",
      "        [[-0.1601,  1.0000],\n",
      "         [-0.5328,  1.0000],\n",
      "         [-0.3648,  1.0000],\n",
      "         [-0.4506,  1.0000],\n",
      "         [-0.4143,  1.0000]]], grad_fn=<TransposeBackward1>)\n",
      "\n",
      "Hidden:\n",
      " tensor([[[-0.3675,  0.9944],\n",
      "         [-0.3820,  0.9999],\n",
      "         [-0.3982,  1.0000],\n",
      "         [-0.4143,  1.0000]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', inputs)\n",
    "print('\\nOutput:\\n', out)\n",
    "print('\\nHidden:\\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNN\n",
    "\n",
    "If I change the `num_layers = 3`, we will have 3 rnn layers stacked next to each other. See how the `hidden` and `output` tensors change.   \n",
    "\n",
    "We now have 3 batches in the `hidden` tensor. The last batch contains the end-rows of each batch in the `output` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 3, batch_first=True)\n",
    "\n",
    "# input size : (batch_size , seq_length, input_size)\n",
    "inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "\n",
    "# hidden state size : (num_layers * num_directions, batch, hidden_size)\n",
    "# hidden = torch.randn(3, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "out, hidden = rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n",
      "\n",
      "Output:\n",
      " tensor([[[ 0.3144, -0.7527],\n",
      "         [-0.0597, -0.6038],\n",
      "         [ 0.0896, -0.7646],\n",
      "         [ 0.0608, -0.6358],\n",
      "         [ 0.1084, -0.6783]],\n",
      "\n",
      "        [[ 0.4442, -0.6350],\n",
      "         [ 0.0949, -0.3948],\n",
      "         [ 0.2715, -0.5962],\n",
      "         [ 0.1819, -0.4580],\n",
      "         [ 0.2529, -0.5213]],\n",
      "\n",
      "        [[ 0.4907, -0.5688],\n",
      "         [ 0.1671, -0.2976],\n",
      "         [ 0.3462, -0.4922],\n",
      "         [ 0.2388, -0.3768],\n",
      "         [ 0.3078, -0.4418]],\n",
      "\n",
      "        [[ 0.5041, -0.5466],\n",
      "         [ 0.1883, -0.2675],\n",
      "         [ 0.3684, -0.4576],\n",
      "         [ 0.2572, -0.3502],\n",
      "         [ 0.3238, -0.4167]]], grad_fn=<TransposeBackward1>)\n",
      "\n",
      "Hidden:\n",
      " tensor([[[-0.6480, -0.4044],\n",
      "         [-0.8912, -0.7801],\n",
      "         [-0.9808, -0.9366],\n",
      "         [-0.9975, -0.9836]],\n",
      "\n",
      "        [[-0.7848, -0.0118],\n",
      "         [-0.8707, -0.1721],\n",
      "         [-0.8955, -0.2411],\n",
      "         [-0.9016, -0.2605]],\n",
      "\n",
      "        [[ 0.1084, -0.6783],\n",
      "         [ 0.2529, -0.5213],\n",
      "         [ 0.3078, -0.4418],\n",
      "         [ 0.3238, -0.4167]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', inputs)\n",
    "print('\\nOutput:\\n', out)\n",
    "print('\\nHidden:\\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 1, batch_first=True, bidirectional = True)\n",
    "\n",
    "# input size : (batch_size , seq_length, input_size)\n",
    "inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "\n",
    "# hidden state size : (num_layers * num_directions, batch, hidden_size)\n",
    "# hidden = torch.randn(3, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "out, hidden = rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n",
      "\n",
      "Output:\n",
      " tensor([[[ 0.2184,  0.4086,  0.6418, -0.1677],\n",
      "         [-0.0222, -0.0095,  0.8794, -0.4927],\n",
      "         [-0.6716, -0.2802,  0.9585, -0.7248],\n",
      "         [-0.9387, -0.4152,  0.9846, -0.8646],\n",
      "         [-0.9841, -0.6164,  0.9789, -0.9192]],\n",
      "\n",
      "        [[-0.9813, -0.8829,  0.9979, -0.9721],\n",
      "         [-0.9986, -0.8902,  0.9992, -0.9877],\n",
      "         [-0.9995, -0.9449,  0.9997, -0.9946],\n",
      "         [-0.9998, -0.9729,  0.9999, -0.9977],\n",
      "         [-0.9999, -0.9868,  0.9998, -0.9987]],\n",
      "\n",
      "        [[-0.9999, -0.9968,  1.0000, -0.9996],\n",
      "         [-1.0000, -0.9969,  1.0000, -0.9998],\n",
      "         [-1.0000, -0.9985,  1.0000, -0.9999],\n",
      "         [-1.0000, -0.9993,  1.0000, -1.0000],\n",
      "         [-1.0000, -0.9997,  1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -0.9999,  1.0000, -1.0000],\n",
      "         [-1.0000, -0.9999,  1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000,  1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000,  1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000,  1.0000, -1.0000]]], grad_fn=<TransposeBackward1>)\n",
      "\n",
      "Forward Outputs: \n",
      " tensor([[[ 0.2184,  0.4086],\n",
      "         [-0.0222, -0.0095],\n",
      "         [-0.6716, -0.2802],\n",
      "         [-0.9387, -0.4152],\n",
      "         [-0.9841, -0.6164]],\n",
      "\n",
      "        [[-0.9813, -0.8829],\n",
      "         [-0.9986, -0.8902],\n",
      "         [-0.9995, -0.9449],\n",
      "         [-0.9998, -0.9729],\n",
      "         [-0.9999, -0.9868]],\n",
      "\n",
      "        [[-0.9999, -0.9968],\n",
      "         [-1.0000, -0.9969],\n",
      "         [-1.0000, -0.9985],\n",
      "         [-1.0000, -0.9993],\n",
      "         [-1.0000, -0.9997]],\n",
      "\n",
      "        [[-1.0000, -0.9999],\n",
      "         [-1.0000, -0.9999],\n",
      "         [-1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000]]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Backward Outpus: \n",
      " tensor([[[ 0.6418, -0.1677],\n",
      "         [ 0.8794, -0.4927],\n",
      "         [ 0.9585, -0.7248],\n",
      "         [ 0.9846, -0.8646],\n",
      "         [ 0.9789, -0.9192]],\n",
      "\n",
      "        [[ 0.9979, -0.9721],\n",
      "         [ 0.9992, -0.9877],\n",
      "         [ 0.9997, -0.9946],\n",
      "         [ 0.9999, -0.9977],\n",
      "         [ 0.9998, -0.9987]],\n",
      "\n",
      "        [[ 1.0000, -0.9996],\n",
      "         [ 1.0000, -0.9998],\n",
      "         [ 1.0000, -0.9999],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000]],\n",
      "\n",
      "        [[ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000]]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Hidden:\n",
      " tensor([[[-0.9841, -0.6164],\n",
      "         [-0.9999, -0.9868],\n",
      "         [-1.0000, -0.9997],\n",
      "         [-1.0000, -1.0000]],\n",
      "\n",
      "        [[ 0.6418, -0.1677],\n",
      "         [ 0.9979, -0.9721],\n",
      "         [ 1.0000, -0.9996],\n",
      "         [ 1.0000, -1.0000]]], grad_fn=<StackBackward>)\n",
      "\n",
      "Forward Hidden: \n",
      " tensor([[-0.9841, -0.9999, -1.0000, -1.0000],\n",
      "        [ 0.6418,  0.9979,  1.0000,  1.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Backward Hidden: \n",
      " tensor([[-0.6164, -0.9868, -0.9997, -1.0000],\n",
      "        [-0.1677, -0.9721, -0.9996, -1.0000]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', inputs)\n",
    "print('\\nOutput:\\n', out)\n",
    "# print(out.contiguous().view(SEQ_LENGTH, BATCH_SIZE, 2, HIDDEN_SIZE))\n",
    "print(\"\\nForward Outputs: \\n\", out[:, :, :HIDDEN_SIZE])\n",
    "print(\"\\nBackward Outpus: \\n\", out[:, :, HIDDEN_SIZE:])\n",
    "print('\\nHidden:\\n', hidden)\n",
    "print(\"\\nForward Hidden: \\n\", hidden[:, :, 0])\n",
    "print(\"\\nBackward Hidden: \\n\", hidden[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM\n",
    "\n",
    "The rnn has two outputs - `out` and `h,c` (hiddent layers).   \n",
    "\n",
    "* `out` is the output of the rnn at each layer.\n",
    "* `h,c` is output of the rnn at the last layer.  \n",
    "\n",
    "\n",
    "If we don't initialize the hidden layer, it will be auto-initiliased by PyTorch to be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size = INPUT_SIZE, hidden_size = HIDDEN_SIZE, num_layers = NUM_LAYERS, batch_first = True)\n",
    "\n",
    "# input size = (batch_size , seq_length, input_size)\n",
    "inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "\n",
    "# hidden state size = (num_layers * num_directions, batch, hidden_size)\n",
    "h_0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "c_0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "out, (h_n, c_n) = lstm(inputs, (h_0, c_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n",
      "\n",
      "Output:\n",
      " tensor([[[ 1.0726e-01,  3.0418e-03],\n",
      "         [ 9.1427e-02,  9.1830e-02],\n",
      "         [ 5.8402e-02,  1.0662e-01],\n",
      "         [ 3.5712e-02,  9.9316e-02],\n",
      "         [ 2.1396e-02,  8.5975e-02]],\n",
      "\n",
      "        [[ 1.5140e-02,  1.2776e-01],\n",
      "         [ 7.5433e-03,  7.4776e-02],\n",
      "         [ 4.3969e-03,  5.9002e-02],\n",
      "         [ 2.5599e-03,  4.5910e-02],\n",
      "         [ 1.4913e-03,  3.5681e-02]],\n",
      "\n",
      "        [[ 5.0291e-04, -9.5297e-03],\n",
      "         [ 4.6364e-04,  2.4555e-04],\n",
      "         [ 2.8614e-04,  3.5727e-03],\n",
      "         [ 1.6931e-04,  4.8141e-03],\n",
      "         [ 9.9062e-05,  4.9508e-03]],\n",
      "\n",
      "        [[ 6.7911e-06,  6.3028e-03],\n",
      "         [ 2.7046e-05,  5.8015e-03],\n",
      "         [ 1.8402e-05,  4.5307e-03],\n",
      "         [ 1.1091e-05,  3.5336e-03],\n",
      "         [ 6.5243e-06,  2.7519e-03]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "h_n:\n",
      " tensor([[[2.1396e-02, 8.5975e-02],\n",
      "         [1.4913e-03, 3.5681e-02],\n",
      "         [9.9062e-05, 4.9508e-03],\n",
      "         [6.5243e-06, 2.7519e-03]]], grad_fn=<StackBackward>)\n",
      "\n",
      "c_n:\n",
      " tensor([[[1.6021, 0.8705],\n",
      "         [1.9072, 1.8280],\n",
      "         [1.9916, 0.4998],\n",
      "         [2.0504, 1.6622]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', inputs)\n",
    "print('\\nOutput:\\n', out)\n",
    "print('\\nh_n:\\n', h_n)\n",
    "print('\\nc_n:\\n', c_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GRU\n",
    "\n",
    "The rnn has two outputs - `out` and `hidden`.   \n",
    "\n",
    "* `out` is the output of the rnn at each layer.\n",
    "* `hidden` is output of the rnn at the last layer.  \n",
    "\n",
    "\n",
    "If we don't initialize the hidden layer, it will be auto-initiliased by PyTorch to be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = NUM_LAYERS, batch_first=True)\n",
    "\n",
    "# input size = (batch_size , seq_length, input_size)\n",
    "inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "\n",
    "# hidden state size = (num_layers * num_directions, batch, hidden_size)\n",
    "hidden = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "out, hidden = gru(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n",
      "\n",
      "Output:\n",
      " tensor([[[ 0.1936,  0.3611],\n",
      "         [ 0.4350,  0.4892],\n",
      "         [ 0.6126,  0.6602],\n",
      "         [ 0.7295,  0.7821],\n",
      "         [ 0.8052,  0.8545]],\n",
      "\n",
      "        [[-0.1884, -0.0280],\n",
      "         [ 0.2012,  0.0870],\n",
      "         [ 0.4086,  0.1688],\n",
      "         [ 0.5362,  0.2237],\n",
      "         [ 0.6212,  0.2597]],\n",
      "\n",
      "        [[-0.3989,  0.7967],\n",
      "         [-0.1058,  0.8006],\n",
      "         [ 0.0780,  0.8033],\n",
      "         [ 0.2040,  0.8052],\n",
      "         [ 0.2949,  0.8064]],\n",
      "\n",
      "        [[ 0.3991,  1.0043],\n",
      "         [ 0.4479,  1.0043],\n",
      "         [ 0.4863,  1.0042],\n",
      "         [ 0.5170,  1.0042],\n",
      "         [ 0.5419,  1.0042]]], grad_fn=<TransposeBackward1>)\n",
      "\n",
      "Hidden:\n",
      " tensor([[[0.8052, 0.8545],\n",
      "         [0.6212, 0.2597],\n",
      "         [0.2949, 0.8064],\n",
      "         [0.5419, 1.0042]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', inputs)\n",
    "print('\\nOutput:\\n', out)\n",
    "print('\\nHidden:\\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End RNN Example [Time Series]\n",
    "\n",
    "This example is just to give you an idea into what training an rnn on PyTorch looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(101):\n",
    "    train_data.append(i)\n",
    "    \n",
    "for i in range(101, 131):\n",
    "    test_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "\n",
      "Test data: \n",
      " [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data: \\n\", train_data)\n",
    "print(\"\\nTest data: \\n\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train Data Structure  \n",
    "\n",
    "Here, we will look at 10 previous time-steps to predict 1 time-step in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "look_back = 10\n",
    "predict_ahead = 1\n",
    "num_features = 1\n",
    "\n",
    "for i in range(len(train_data) - look_back - predict_ahead):\n",
    "    X_train.append(train_data[i: i + look_back])\n",
    "    y_train.append(train_data[i + look_back: i + look_back + predict_ahead])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of X_train: \n",
      " [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [ 1  2  3  4  5  6  7  8  9 10]\n",
      " [ 2  3  4  5  6  7  8  9 10 11]\n",
      " [ 3  4  5  6  7  8  9 10 11 12]\n",
      " [ 4  5  6  7  8  9 10 11 12 13]\n",
      " [ 5  6  7  8  9 10 11 12 13 14]\n",
      " [ 6  7  8  9 10 11 12 13 14 15]\n",
      " [ 7  8  9 10 11 12 13 14 15 16]\n",
      " [ 8  9 10 11 12 13 14 15 16 17]\n",
      " [ 9 10 11 12 13 14 15 16 17 18]]\n",
      "\n",
      "First 10 rows of y_train: \n",
      " [[10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 rows of X_train: \\n\", X_train[0:10, ])\n",
    "\n",
    "print(\"\\nFirst 10 rows of y_train: \\n\", y_train[0:10, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (90, 10)\n",
      "y_train.shape:  (90, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "look_back = 10\n",
    "predict_ahead = 1\n",
    "num_features = 1\n",
    "\n",
    "for i in range(len(test_data) - look_back - predict_ahead):\n",
    "    X_test.append(test_data[i: i + look_back])\n",
    "    y_test.append(test_data[i + look_back: i + look_back + predict_ahead])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of X_test: \n",
      " [[101 102 103 104 105 106 107 108 109 110]\n",
      " [102 103 104 105 106 107 108 109 110 111]\n",
      " [103 104 105 106 107 108 109 110 111 112]\n",
      " [104 105 106 107 108 109 110 111 112 113]\n",
      " [105 106 107 108 109 110 111 112 113 114]\n",
      " [106 107 108 109 110 111 112 113 114 115]\n",
      " [107 108 109 110 111 112 113 114 115 116]\n",
      " [108 109 110 111 112 113 114 115 116 117]\n",
      " [109 110 111 112 113 114 115 116 117 118]\n",
      " [110 111 112 113 114 115 116 117 118 119]]\n",
      "\n",
      "First 10 rows of y_test: \n",
      " [[111]\n",
      " [112]\n",
      " [113]\n",
      " [114]\n",
      " [115]\n",
      " [116]\n",
      " [117]\n",
      " [118]\n",
      " [119]\n",
      " [120]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 rows of X_test: \\n\", X_test[0:10, ])\n",
    "\n",
    "print(\"\\nFirst 10 rows of y_test: \\n\", y_test[0:10, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (19, 10)\n",
      "y_test.shape:  (19, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "input_size = 1\n",
    "sequence_length = 10\n",
    "hidden_size = 1\n",
    "num_layer = 3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Class\n",
    "\n",
    "Here, we're not initialising the hidden layers. If you want to initialise hidden layers, follow the examples above and modify the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         hidden = torch.randn(num_layer, batch_size, hidden_size)\n",
    "\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        out, hidden = self.rnn(x)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelRNN(\n",
      "  (rnn): RNN(1, 1, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_rnn = ModelRNN()\n",
    "print(model_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_rnn.parameters(),lr=0.01)\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN - Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: | Loss: 3724.94150\n",
      "Epoch 02: | Loss: 3629.42670\n",
      "Epoch 03: | Loss: 3556.37166\n",
      "Epoch 04: | Loss: 3543.76887\n",
      "Epoch 05: | Loss: 3541.13346\n",
      "Epoch 06: | Loss: 3540.09723\n",
      "Epoch 07: | Loss: 3539.50432\n",
      "Epoch 08: | Loss: 3539.10224\n",
      "Epoch 09: | Loss: 3538.80756\n",
      "Epoch 10: | Loss: 3538.58223\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model_rnn(x_batch)\n",
    "        # hidden[-1] gives the last hidden layer ie. our output\n",
    "        # Print out the shapes of 'y_batch' and 'hidden' yourselves here to explore\n",
    "        loss = criterion(hidden[-1],  y_batch) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {e+1:02}: | Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Class\n",
    "\n",
    "Here, we're not initialising the hidden layers. If you want to initialise hidden layers, follow the examples above and modify the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         h_0 = torch.randn(num_layer, batch_size, hidden_size)\n",
    "#         c_0 = torch.randn(num_layer, batch_size, hidden_size)\n",
    "        \n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        return out, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelLSTM(\n",
      "  (lstm): LSTM(1, 1, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_lstm = ModelLSTM()\n",
    "print(model_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_lstm.parameters(),lr=0.01)\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: | Loss: 3538.50384\n",
      "Epoch 02: | Loss: 3538.50384\n",
      "Epoch 03: | Loss: 3538.50384\n",
      "Epoch 04: | Loss: 3538.50384\n",
      "Epoch 05: | Loss: 3538.50384\n",
      "Epoch 06: | Loss: 3538.50384\n",
      "Epoch 07: | Loss: 3538.50384\n",
      "Epoch 08: | Loss: 3538.50384\n",
      "Epoch 09: | Loss: 3538.50384\n",
      "Epoch 10: | Loss: 3538.50384\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model_rnn(x_batch)\n",
    "        # hidden[-1] gives the last hidden layer ie. our output\n",
    "        # Print out the shapes of 'y_batch' and 'hidden' yourselves here to explore\n",
    "        loss = criterion(hidden[-1],  y_batch) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {e+1:02}: | Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU Class  \n",
    "\n",
    "\n",
    "Here, we're not initialising the hidden layers. If you want to initialise hidden layers, follow the examples above and modify the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layer)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         hidden = torch.randn(num_layer, batch_size, hidden_size)\n",
    "\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelGRU(\n",
      "  (gru): GRU(1, 1, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_gru = ModelGRU()\n",
    "print(model_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_gru.parameters(),lr=0.01)\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU - Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: | Loss: 3538.50384\n",
      "Epoch 02: | Loss: 3538.50384\n",
      "Epoch 03: | Loss: 3538.50384\n",
      "Epoch 04: | Loss: 3538.50384\n",
      "Epoch 05: | Loss: 3538.50384\n",
      "Epoch 06: | Loss: 3538.50384\n",
      "Epoch 07: | Loss: 3538.50384\n",
      "Epoch 08: | Loss: 3538.50384\n",
      "Epoch 09: | Loss: 3538.50384\n",
      "Epoch 10: | Loss: 3538.50384\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model_rnn(x_batch)\n",
    "        # hidden[-1] gives the last hidden layer ie. our output\n",
    "        # Print out the shapes of 'y_batch' and 'hidden' yourselves here to explore\n",
    "        loss = criterion(hidden[-1],  y_batch) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {e+1:02}: | Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
