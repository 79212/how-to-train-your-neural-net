{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Sample Example | RNN | PyTorch\n",
    "\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)\n",
    "\n",
    "This notebook takes you through the basics of using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition sample example with 4 sentences of different lengths. The data is padded to a constant number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe81d4bbe30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"Ronaldo is from Portugal.\".split(), [\"PER\", \"O\", \"O\", \"LOC\"]),\n",
    "    (\"Rooney is from England.\".split(), [\"PER\", \"O\", \"O\", \"LOC\"]),\n",
    "    (\"Ronaldo was born in February.\".split(), [\"PER\", \"O\", \"O\", \"O\", \"DATE\"]),\n",
    "    (\"MUFC is an English club.\".split(), [\"ORG\", \"O\", \"O\", \"LOC\", \"O\"])\n",
    "]\n",
    "\n",
    "sentence_list = [training_data[x][0] for x in range(len(training_data))]\n",
    "tag_list = [training_data[x][1] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ronaldo', 'is', 'from', 'Portugal.'],\n",
       " ['Rooney', 'is', 'from', 'England.'],\n",
       " ['Ronaldo', 'was', 'born', 'in', 'February.'],\n",
       " ['MUFC', 'is', 'an', 'English', 'club.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PER', 'O', 'O', 'LOC'],\n",
       " ['PER', 'O', 'O', 'LOC'],\n",
       " ['PER', 'O', 'O', 'O', 'DATE'],\n",
       " ['ORG', 'O', 'O', 'LOC', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data by converting it into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_list = []\n",
    "for sentence, tags in training_data:\n",
    "    clean_sentence = [x.lower().split('.')[0] for x in sentence]\n",
    "    data_clean_list += [(clean_sentence, tags)]\n",
    "\n",
    "    \n",
    "sentence_clean_list = [data_clean_list[x][0] for x in range(len(data_clean_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ronaldo', 'is', 'from', 'portugal'],\n",
       " ['rooney', 'is', 'from', 'england'],\n",
       " ['ronaldo', 'was', 'born', 'in', 'february'],\n",
       " ['mufc', 'is', 'an', 'english', 'club']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 14\n",
      "\n",
      "['rooney', 'an', 'england', 'february', 'club', 'in', 'portugal', 'was', 'from', 'is', 'mufc', 'english', 'ronaldo', 'born']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_clean_list:\n",
    "    words += sentence\n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rooney': 0, 'an': 1, 'england': 2, 'february': 3, 'club': 4, 'in': 5, 'portugal': 6, 'was': 7, 'from': 8, 'is': 9, 'mufc': 10, 'english': 11, 'ronaldo': 12, 'born': 13}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 5\n",
      "\n",
      "['DATE', 'ORG', 'LOC', 'PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags += tag\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': 0, 'ORG': 1, 'LOC': 2, 'PER': 3, 'O': 4}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ronaldo', 'is', 'from', 'portugal'],\n",
       "  ['rooney', 'is', 'from', 'england'],\n",
       "  ['ronaldo', 'was', 'born', 'in', 'february'],\n",
       "  ['mufc', 'is', 'an', 'english', 'club']],\n",
       " [['PER', 'O', 'O', 'LOC'],\n",
       "  ['PER', 'O', 'O', 'LOC'],\n",
       "  ['PER', 'O', 'O', 'O', 'DATE'],\n",
       "  ['ORG', 'O', 'O', 'LOC', 'O']])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 9, 8, 6], [0, 9, 8, 2], [12, 7, 13, 5, 3], [10, 9, 1, 11, 4]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[w] for w in s] for s in sentence_clean_list]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 4, 2], [3, 4, 4, 2], [3, 4, 4, 4, 0], [1, 4, 4, 2, 4]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[tag2idx[t] for t in s] for s in tag_list]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input -> RNN -> Linear -> Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 10\n",
    "HIDDEN_SIZE = 20\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data, maxlen):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "#         self.X_data = self.pad_data(X_data)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        self.X_data[index] = self.pad_data(self.X_data[index])\n",
    "        self.y_data[index] = self.pad_data(self.y_data[index])        \n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "#         print(len(s))\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: \n",
    "            padded[:] = s[:self.maxlen]\n",
    "        else: \n",
    "            padded[:len(s)] = s\n",
    "        \n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(X, y, maxlen=10)\n",
    "# train_data = TrainData(torch.Tensor(X).to(torch.int64), torch.Tensor(y).to(torch.long))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  9,  8,  6,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  9,  8,  2,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[3, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 4, 4, 2, 0, 0, 0, 0, 0, 0]])\n",
      "==================================================\n",
      "tensor([[12,  7, 13,  5,  3,  0,  0,  0,  0,  0],\n",
      "        [10,  9,  1, 11,  4,  0,  0,  0,  0,  0]])\n",
      "tensor([[3, 4, 4, 4, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 4, 4, 2, 4, 0, 0, 0, 0, 0]])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_loader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUtagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(GRUtagger, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.gru = nn.GRU(input_size = embedding_size, hidden_size=hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds)\n",
    "        linear_out = self.linear(gru_out)\n",
    "        y_out = F.log_softmax(linear_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUtagger(\n",
      "  (word_embeddings): Embedding(14, 10)\n",
      "  (gru): GRU(10, 20, batch_first=True)\n",
      "  (linear): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GRUtagger(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx))\n",
    "print(model)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the output from the model looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[12,  9,  8,  6,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  9,  8,  2,  0,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Output:\n",
      "tensor([[[-1.9535, -1.7529, -1.9320, -2.5259, -2.5453],\n",
      "         [-2.2073, -2.0193, -2.2057, -2.3097, -2.4400],\n",
      "         [-2.1645, -2.0583, -2.0287, -2.4821, -2.3742],\n",
      "         [-2.1633, -2.1360, -2.1701, -2.2748, -2.2861],\n",
      "         [-2.3657, -2.4347, -2.3414, -2.2723, -2.2370],\n",
      "         [-2.4404, -2.5664, -2.4388, -2.2618, -2.2265],\n",
      "         [-2.4669, -2.6229, -2.4977, -2.2492, -2.2306],\n",
      "         [-2.4747, -2.6464, -2.5339, -2.2391, -2.2386],\n",
      "         [-2.4753, -2.6558, -2.5559, -2.2322, -2.2465],\n",
      "         [-2.4736, -2.6592, -2.5692, -2.2278, -2.2528]],\n",
      "\n",
      "        [[-2.2117, -2.2562, -2.1769, -2.3999, -2.3958],\n",
      "         [-2.2828, -2.1576, -2.3157, -2.2746, -2.4117],\n",
      "         [-2.1468, -2.0347, -2.0531, -2.4283, -2.3692],\n",
      "         [-2.2397, -2.1273, -2.2050, -2.3327, -2.3239],\n",
      "         [-2.3316, -2.3280, -2.2991, -2.3171, -2.2706],\n",
      "         [-2.3653, -2.4134, -2.3573, -2.2914, -2.2517],\n",
      "         [-2.3741, -2.4468, -2.3953, -2.2690, -2.2490],\n",
      "         [-2.3728, -2.4584, -2.4200, -2.2531, -2.2526],\n",
      "         [-2.3683, -2.4614, -2.4357, -2.2426, -2.2577],\n",
      "         [-2.3633, -2.4613, -2.4455, -2.2360, -2.2625]]]) torch.Size([2, 10, 5])\n",
      "\n",
      "Output Indices:\n",
      "tensor([[0, 0, 0, 9, 5],\n",
      "        [2, 2, 2, 9, 6]])\n",
      "\n",
      "Actual Output:\n",
      "tensor([[3, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 4, 4, 2, 0, 0, 0, 0, 0, 0]]) torch.Size([2, 10])\n",
      "==================================================\n",
      "Input:\n",
      "tensor([[12,  7, 13,  5,  3,  0,  0,  0,  0,  0],\n",
      "        [10,  9,  1, 11,  4,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Output:\n",
      "tensor([[[-2.0798, -1.8906, -2.0562, -2.3801, -2.4515],\n",
      "         [-2.0913, -1.9089, -2.0762, -2.5059, -2.4592],\n",
      "         [-2.0334, -1.9548, -2.0116, -2.4807, -2.5009],\n",
      "         [-2.1948, -2.1192, -2.2384, -2.3032, -2.4289],\n",
      "         [-2.1940, -2.2625, -2.1157, -2.4945, -2.3844],\n",
      "         [-2.4124, -2.5511, -2.3896, -2.3381, -2.2479],\n",
      "         [-2.5140, -2.6829, -2.5271, -2.2317, -2.1833],\n",
      "         [-2.5623, -2.7436, -2.6026, -2.1653, -2.1582],\n",
      "         [-2.5847, -2.7720, -2.6460, -2.1266, -2.1523],\n",
      "         [-2.5943, -2.7852, -2.6718, -2.1047, -2.1541]],\n",
      "\n",
      "        [[-2.1225, -2.1778, -2.1645, -2.3770, -2.3425],\n",
      "         [-2.2665, -2.1676, -2.3542, -2.2242, -2.3537],\n",
      "         [-2.1426, -1.9812, -2.1688, -2.4200, -2.4146],\n",
      "         [-2.2031, -2.0267, -2.0810, -2.4841, -2.4437],\n",
      "         [-2.1651, -2.2149, -2.0221, -2.4434, -2.3885],\n",
      "         [-2.3713, -2.4448, -2.2946, -2.3484, -2.2924],\n",
      "         [-2.4420, -2.5351, -2.4409, -2.2623, -2.2302],\n",
      "         [-2.4658, -2.5698, -2.5237, -2.2051, -2.2039],\n",
      "         [-2.4724, -2.5825, -2.5717, -2.1720, -2.1973],\n",
      "         [-2.4724, -2.5864, -2.6000, -2.1540, -2.1993]]]) torch.Size([2, 10, 5])\n",
      "\n",
      "Output Indices:\n",
      "tensor([[2, 0, 2, 9, 8],\n",
      "        [0, 2, 4, 9, 8]])\n",
      "\n",
      "Actual Output:\n",
      "tensor([[3, 4, 4, 4, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 4, 4, 2, 4, 0, 0, 0, 0, 0]]) torch.Size([2, 10])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "               \n",
    "        print(\"Input:\")\n",
    "        print(x_batch)\n",
    "        y_out = model(x_batch)\n",
    "        _, y_out_tags = torch.max(y_out.squeeze(), dim = 1)\n",
    "        \n",
    "        print(\"\\nOutput:\")\n",
    "        print(y_out, y_out.shape)\n",
    "        \n",
    "        print(\"\\nOutput Indices:\")\n",
    "        print(y_out_tags)\n",
    "        \n",
    "#         print(\"\\nOutput Tags:\")\n",
    "#         for i in y_out_tags.tolist():\n",
    "#             print(tags[i])\n",
    "\n",
    "        print(\"\\nActual Output:\")\n",
    "        print(y_batch, y_batch.shape)\n",
    "        \n",
    "#         print(\"\\nActual Tags:\")\n",
    "#         for i in y_batch.squeeze().tolist():\n",
    "#             print(tags[i])\n",
    "\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.NLLLoss()** expects input and target to be 2-dimensional and 1-dimensional respectively.\n",
    "\n",
    "So, we will reshape the tensors as follows:  \n",
    "* input tensor (y_pred) to a 2d tensor from a 3d tensor. So, from `[1, 4, 3]` to `[4, 3]`. \n",
    "* target tensor (y_batch) to a 1d tensor from a 2d tensor. So, from `[1, 4]` to `[4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 : loss = 2.4094676971435547\n",
      "Epoch 2/10 : loss = 2.3996338844299316\n",
      "Epoch 3/10 : loss = 2.3900833129882812\n",
      "Epoch 4/10 : loss = 2.380798578262329\n",
      "Epoch 5/10 : loss = 2.371765613555908\n",
      "Epoch 6/10 : loss = 2.3629705905914307\n",
      "Epoch 7/10 : loss = 2.3543994426727295\n",
      "Epoch 8/10 : loss = 2.346041440963745\n",
      "Epoch 9/10 : loss = 2.3378841876983643\n",
      "Epoch 10/10 : loss = 2.3299174308776855\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, EPOCH+1):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        y_pred = model(x_batch)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        y_pred = y_pred.view(-1, len(tag2idx))\n",
    "#         print(y_pred.shape)\n",
    "#         print(y_batch.shape)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "#         _, y_pred_idx = torch.max(y_pred.squeeze(), dim = 1)\n",
    "#         print(y_pred_idx, y_batch)\n",
    "\n",
    "    print(f'Epoch {e}/{EPOCH} : loss = {loss.item()}') \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
