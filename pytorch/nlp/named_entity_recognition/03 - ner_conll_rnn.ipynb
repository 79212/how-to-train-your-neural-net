{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Named Entity Recognition using RNNs.\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)\n",
    "\n",
    "This notebook takes you through the applying Named Entity Recognition using RNNs on PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "\n",
    "Link to dataset: https://www.clips.uantwerpen.be/conll2003/ner/  \n",
    "Easy downloadable data: https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003 or https://github.com/davidsbatista/NER-datasets/tree/master/CONLL2003\n",
    "\n",
    " Named entities are phrases that contain the names of persons, organizations, locations, times, and quantities. Example:\n",
    "\n",
    "    [ORG U.N. ] official [PER Ekeus ] heads for [LOC Baghdad ] . \n",
    "    \n",
    "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Here is an example:\n",
    "\n",
    "   U.N.         NNP  I-NP  I-ORG  \n",
    "   official     NN   I-NP  O   \n",
    "   Ekeus        NNP  I-NP  I-PER   \n",
    "   heads        VBZ  I-VP  O   \n",
    "   for          IN   I-PP  O   \n",
    "   Baghdad      NNP  I-NP  I-LOC   \n",
    "   .            .    O     O   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/nlp/ner/conll_ner.txt', sep=' ', skiprows = 1, index_col=False, names=['word', 'pos', 'syn', 'tag']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>syn</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  pos   syn     tag\n",
       "0       EU  NNP  I-NP   I-ORG\n",
       "1  rejects  VBZ  I-VP       O\n",
       "2   German   JJ  I-NP  I-MISC\n",
       "3     call   NN  I-NP       O\n",
       "4       to   TO  I-VP       O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentence ID. \n",
    "Each full-stop signifies the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_id'] = np.nan\n",
    "df = df[['sentence_id', 'word', 'pos', 'syn', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>syn</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id     word  pos   syn     tag\n",
       "0          1.0       EU  NNP  I-NP   I-ORG\n",
       "1          1.0  rejects  VBZ  I-VP       O\n",
       "2          1.0   German   JJ  I-NP  I-MISC\n",
       "3          1.0     call   NN  I-NP       O\n",
       "4          1.0       to   TO  I-VP       O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence_id'] = np.nan\n",
    "df.iloc[0, 0] = 1\n",
    "df.loc[df['word'] == '.', ['sentence_id']] = np.arange(1, len(df[df['word'] == '.'])+1)\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "df = df.loc[:204543, :]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset id, word, and tag.\n",
    "\n",
    "We will only use the **sentence_id**, **word**, and **tag** columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>German</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id     word     tag\n",
       "0          1.0       EU   I-ORG\n",
       "1          1.0  rejects       O\n",
       "2          1.0   German  I-MISC\n",
       "3          1.0     call       O\n",
       "4          1.0       to       O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['sentence_id', 'word', 'tag']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>eu</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>german</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>boycott</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id     word     tag\n",
       "0          1.0       eu   I-ORG\n",
       "2          1.0   german  I-MISC\n",
       "3          1.0     call       O\n",
       "4          1.0       to       O\n",
       "5          1.0  boycott       O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-alphabets\n",
    "df = df[~df['word'].str.contains('[^a-zA-Z]')]\n",
    "\n",
    "# Convert all words to lowercase\n",
    "df['word'] = df['word'].str.lower()\n",
    "\n",
    "# Strip white-space\n",
    "df['word'] = df['word'].str.strip()\n",
    "\n",
    "# Remove words that appear only once\n",
    "df = df[df.groupby('word').word.transform(len) > 1]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words = 9341\n"
     ]
    }
   ],
   "source": [
    "words = list(set(df[\"word\"].values))\n",
    "words.insert(0, \"PAD\")\n",
    "n_words = len(words)\n",
    "print(f\"Number of words = {n_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word2idx = 9341\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "print(f'Length of word2idx = {len(word2idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags = 9\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(df[\"tag\"].values))\n",
    "tags.insert(0, \"PAD-TAG\")\n",
    "n_tags = len(tags)\n",
    "print(f\"Number of tags = {n_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag2idx :\n",
      "{'PAD-TAG': 0, 'I-PER': 1, 'I-ORG': 2, 'I-MISC': 3, 'B-MISC': 4, 'B-ORG': 5, 'I-LOC': 6, 'B-LOC': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "print(f'tag2idx :\\n{tag2idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eu', 'I-ORG'),\n",
       " ('german', 'I-MISC'),\n",
       " ('call', 'O'),\n",
       " ('to', 'O'),\n",
       " ('boycott', 'O'),\n",
       " ('british', 'I-MISC'),\n",
       " ('lamb', 'O')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_func = lambda x: [(w, t) for w, t in zip(x['word'].values.tolist(), x['tag'].values.tolist())]\n",
    "grouped_data = df.groupby('sentence_id').apply(agg_func)\n",
    "sentence_tag_list = [x for x in grouped_data]\n",
    "\n",
    "sentence_tag_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in sentence_tag_list if len(s) < 200], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[w[0]] for w in s] for s in sentence_tag_list]\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentence_tag_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [4812, 6551, 8802, 8071, 5127, 1671, 8583]\n",
      "y:  [2, 3, 8, 8, 8, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X[0])\n",
    "print(\"y: \", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  6579\n",
      "X_test size:  731\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train size: \", len(X_train))\n",
    "print(\"X_test size: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sample Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_SAMPLE = 2\n",
    "EMBEDDING_SIZE_SAMPLE = 5\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE_SAMPLE = 3\n",
    "STACKED_LAYERS_SAMPLE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sample Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = SampleData(X_train, y_train)\n",
    "sample_loader = DataLoader(sample_data, batch_size=BATCH_SIZE_SAMPLE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5870, 6670, 5723, 8071, 77, 4614, 2170, 4523, 2140, 1899, 4614, 7056, 4523, 4614, 8281, 1117, 4614, 2891], [4614, 6747, 3883, 7627, 7081, 5348, 8560, 3766, 4884, 8614, 1899, 185, 3183, 6389, 8614, 8194, 7081, 4614, 8598, 4568, 8405]] \n",
      "\n",
      " [[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 3, 8, 8, 8, 6, 8, 8, 8, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tl = iter(sample_loader)\n",
    "\n",
    "i,j = map(list, zip(*next(tl)))\n",
    "\n",
    "print(i,\"\\n\\n\", j, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample RNN Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGRUSample(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size, stacked_layers):\n",
    "        super(ModelGRUSample, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.gru = nn.GRU(input_size = embedding_size, hidden_size = hidden_size, batch_first = True, num_layers=stacked_layers)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        print(\"\\nList of tensor lengths in a batch: \")\n",
    "        len_list = list(map(len, x_batch))\n",
    "        print(len_list)\n",
    "        \n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)\n",
    "        print(\"\\nPadded X_batch: \\n\", padded_batch, \"\\n\")\n",
    "\n",
    "        \n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        print(\"\\nEmbeddings: \\n\", embeds, embeds.size(), \"\\n\")\n",
    "\n",
    "        pack_embeds = pack_padded_sequence(embeds, lengths=len_list, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        rnn_out, rnn_hidden = self.gru(pack_embeds)\n",
    "        \n",
    "        rnn_out_padded = pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        print(\"\\nPadded RNN output each layer:\\n\", rnn_out_padded[0])        \n",
    "        \n",
    "        linear_out = self.linear(rnn_out_padded[0])\n",
    "        \n",
    "        y_out = torch.log_softmax(linear_out, dim = 1)\n",
    "        print(\"\\nLogSoftmax:\\n\", y_out)\n",
    "\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelGRUSample(\n",
      "  (word_embeddings): Embedding(9341, 5)\n",
      "  (gru): GRU(5, 3, batch_first=True)\n",
      "  (linear): Linear(in_features=3, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gru_model_sample = ModelGRUSample(embedding_size=EMBEDDING_SIZE_SAMPLE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE_SAMPLE, target_size=len(tag2idx), stacked_layers=STACKED_LAYERS_SAMPLE)\n",
    "print(gru_model_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch: \n",
      "[tensor([5870, 6670, 5723, 8071,   77, 4614, 2170, 4523, 2140, 1899, 4614, 7056,\n",
      "        4523, 4614, 8281, 1117, 4614, 2891]),\n",
      " tensor([4614, 6747, 3883, 7627, 7081, 5348, 8560, 3766, 4884, 8614, 1899,  185,\n",
      "        3183, 6389, 8614, 8194, 7081, 4614, 8598, 4568, 8405])]\n",
      "\n",
      "y batch: \n",
      "[tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]),\n",
      " tensor([8, 8, 8, 3, 8, 8, 8, 6, 8, 8, 8, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8])]\n",
      "\n",
      "List of tensor lengths in a batch: \n",
      "[18, 21]\n",
      "\n",
      "Padded X_batch: \n",
      " tensor([[5870, 6670, 5723, 8071,   77, 4614, 2170, 4523, 2140, 1899, 4614, 7056,\n",
      "         4523, 4614, 8281, 1117, 4614, 2891,    0,    0,    0],\n",
      "        [4614, 6747, 3883, 7627, 7081, 5348, 8560, 3766, 4884, 8614, 1899,  185,\n",
      "         3183, 6389, 8614, 8194, 7081, 4614, 8598, 4568, 8405]]) \n",
      "\n",
      "\n",
      "Embeddings: \n",
      " tensor([[[ 1.8870, -0.7260, -0.8494,  0.0380,  2.2705],\n",
      "         [ 0.8295,  0.4259,  0.9367, -1.0954, -1.6446],\n",
      "         [-0.7786, -0.1885, -1.1264,  1.2179,  1.1586],\n",
      "         [ 0.1069,  0.8882,  1.3638, -0.4585, -0.6989],\n",
      "         [-0.2322, -0.6765,  1.2901,  1.3827,  0.5190],\n",
      "         [-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [-0.0786,  0.7839, -0.1089,  1.1776,  1.3477],\n",
      "         [-0.1072,  1.2423,  0.0914,  1.5045,  0.8477],\n",
      "         [ 1.0555,  2.0568, -1.6013,  0.4536, -0.2584],\n",
      "         [ 0.7794, -2.4151,  1.3886,  0.4099, -0.6629],\n",
      "         [-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [-0.3440, -1.0359, -0.7878, -1.2631,  1.1068],\n",
      "         [-0.1072,  1.2423,  0.0914,  1.5045,  0.8477],\n",
      "         [-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [-1.3419, -0.0790,  0.7168, -0.0111,  0.9632],\n",
      "         [-0.4535,  0.7068,  1.1218,  0.1665, -0.0201],\n",
      "         [-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [ 0.9397, -1.6068,  1.1439,  0.9764, -1.4405],\n",
      "         [-0.0967,  0.4875, -0.9325,  0.5473,  1.3742],\n",
      "         [-0.0967,  0.4875, -0.9325,  0.5473,  1.3742],\n",
      "         [-0.0967,  0.4875, -0.9325,  0.5473,  1.3742]],\n",
      "\n",
      "        [[-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [-0.2225,  1.3176, -0.1600,  2.5387,  1.2249],\n",
      "         [ 1.2599, -0.3086, -1.0380, -0.9036,  0.2267],\n",
      "         [-1.0504,  1.1634, -0.0149,  0.0598, -0.9512],\n",
      "         [ 0.6521, -1.0570,  0.0079, -0.5083, -1.4603],\n",
      "         [-0.8314,  0.1368, -0.1724,  0.1923,  0.4838],\n",
      "         [-0.6262, -0.3868,  1.1314, -0.6559, -1.7098],\n",
      "         [ 0.4076,  0.5299,  0.9987,  0.7116, -0.9212],\n",
      "         [ 1.2971,  0.0842,  0.3979, -0.2301, -0.3870],\n",
      "         [ 0.1104,  0.5595, -0.2794,  0.3642,  0.4282],\n",
      "         [ 0.7794, -2.4151,  1.3886,  0.4099, -0.6629],\n",
      "         [ 0.0938, -0.1814, -1.4519, -1.5249, -1.6186],\n",
      "         [ 0.5790, -0.8402,  0.1940, -0.8548,  0.0167],\n",
      "         [ 0.8007,  1.5290,  0.0574, -0.2592, -1.6693],\n",
      "         [ 0.1104,  0.5595, -0.2794,  0.3642,  0.4282],\n",
      "         [ 0.9612,  0.9005,  0.0995,  0.3095, -0.6447],\n",
      "         [ 0.6521, -1.0570,  0.0079, -0.5083, -1.4603],\n",
      "         [-1.2819,  0.9360, -0.4448,  1.1039,  0.5569],\n",
      "         [-0.7475, -0.4089, -1.8373, -0.2933, -0.6110],\n",
      "         [ 0.4341, -1.5428,  0.6350, -0.8275, -0.8253],\n",
      "         [ 1.7481, -0.6084, -1.0782, -0.4065,  0.8629]]]) torch.Size([2, 21, 5]) \n",
      "\n",
      "\n",
      "Padded RNN output each layer:\n",
      " tensor([[[ 2.6663e-01, -8.2883e-02, -5.7778e-01],\n",
      "         [ 1.8505e-01, -4.0494e-01,  7.8119e-02],\n",
      "         [ 3.2094e-01, -1.6784e-01, -3.7872e-01],\n",
      "         [ 3.3264e-03, -2.8214e-01,  5.7601e-02],\n",
      "         [ 2.9944e-01,  1.3207e-01,  2.0315e-01],\n",
      "         [ 6.2571e-02,  2.4116e-01, -7.7602e-02],\n",
      "         [ 8.5072e-02,  3.0405e-01, -3.4800e-01],\n",
      "         [ 4.8300e-02,  3.4416e-01, -4.0384e-01],\n",
      "         [ 6.1500e-02,  2.7710e-01, -7.9104e-01],\n",
      "         [ 5.7964e-01, -1.5247e-01,  1.1521e-01],\n",
      "         [ 2.4427e-01,  5.1892e-04, -1.1595e-01],\n",
      "         [ 2.6377e-01, -9.7273e-02, -4.9251e-01],\n",
      "         [ 1.5383e-01, -1.7852e-02, -4.9301e-01],\n",
      "         [-1.4234e-02,  8.5717e-02, -4.5500e-01],\n",
      "         [-2.3162e-01,  2.8820e-01, -2.7583e-01],\n",
      "         [-3.1769e-01,  2.7221e-01, -9.6602e-03],\n",
      "         [-2.6606e-01,  3.5072e-01, -2.1542e-01],\n",
      "         [ 3.2942e-01, -1.5311e-01,  4.3606e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-1.0075e-01,  1.2206e-01, -2.0345e-01],\n",
      "         [ 7.5949e-02,  1.8283e-01, -3.8602e-01],\n",
      "         [ 2.2690e-01, -9.2427e-02, -7.5313e-01],\n",
      "         [-4.4704e-02, -9.7887e-02, -2.3617e-01],\n",
      "         [ 2.0920e-01, -4.9639e-01, -7.0462e-02],\n",
      "         [ 1.0039e-01, -2.4534e-01, -2.5187e-01],\n",
      "         [ 4.2287e-02, -4.4702e-01,  3.7314e-01],\n",
      "         [ 7.3680e-02, -4.0547e-01,  2.7859e-01],\n",
      "         [ 1.5825e-01, -5.4994e-01, -2.9281e-01],\n",
      "         [ 1.4681e-01, -4.5299e-01, -5.1198e-01],\n",
      "         [ 5.2601e-01, -4.4141e-01,  1.9231e-01],\n",
      "         [ 4.9147e-01, -6.5077e-01, -5.2653e-01],\n",
      "         [ 4.8888e-01, -6.7740e-01, -4.6924e-01],\n",
      "         [ 3.7020e-01, -7.2248e-01, -3.0349e-01],\n",
      "         [ 2.9505e-01, -5.9146e-01, -5.1041e-01],\n",
      "         [ 2.8554e-01, -6.1796e-01, -4.8653e-01],\n",
      "         [ 4.0710e-01, -7.5244e-01, -1.4640e-01],\n",
      "         [ 1.6874e-01, -5.1342e-01, -2.8882e-01],\n",
      "         [ 2.5414e-01, -5.0406e-01, -6.4154e-01],\n",
      "         [ 4.2009e-01, -6.6982e-01, -9.2702e-02],\n",
      "         [ 4.9043e-01, -7.1046e-01, -7.0694e-01]]])\n",
      "\n",
      "LogSoftmax:\n",
      " tensor([[[-3.4088, -2.9483, -3.2193, -2.9859, -3.1180, -3.0152, -3.0805,\n",
      "          -3.2710, -3.3113],\n",
      "         [-3.2047, -2.9026, -3.0018, -2.8280, -3.1832, -3.2664, -2.8474,\n",
      "          -2.7794, -2.8792],\n",
      "         [-3.3729, -2.9118, -3.1665, -2.9184, -3.1254, -3.0530, -3.0136,\n",
      "          -3.1374, -3.2136],\n",
      "         [-3.0750, -2.9970, -2.9704, -2.9349, -3.1563, -3.2748, -2.8945,\n",
      "          -2.8063, -2.8399],\n",
      "         [-2.9028, -3.0194, -2.9525, -2.9942, -2.9567, -2.9546, -3.0070,\n",
      "          -2.9052, -2.9539],\n",
      "         [-2.8927, -3.1211, -2.9992, -3.1351, -2.9659, -2.9712, -3.0988,\n",
      "          -3.0672, -3.0264],\n",
      "         [-3.0093, -3.1239, -3.0905, -3.1760, -2.9666, -2.9081, -3.1711,\n",
      "          -3.2449, -3.1878],\n",
      "         [-3.0018, -3.1459, -3.1007, -3.2081, -2.9604, -2.8978, -3.1955,\n",
      "          -3.2847, -3.2096],\n",
      "         [-3.2440, -3.1120, -3.2361, -3.2105, -3.0226, -2.8957, -3.2453,\n",
      "          -3.4836, -3.4054],\n",
      "         [-3.2154, -2.8438, -3.0458, -2.8018, -3.0460, -2.9808, -2.9242,\n",
      "          -2.8995, -3.0601],\n",
      "         [-3.1154, -2.9919, -3.0564, -2.9865, -3.0432, -3.0186, -3.0224,\n",
      "          -3.0341, -3.0757],\n",
      "         [-3.3709, -2.9475, -3.1909, -2.9739, -3.1154, -3.0306, -3.0596,\n",
      "          -3.2174, -3.2630],\n",
      "         [-3.2836, -3.0064, -3.1677, -3.0396, -3.0960, -3.0341, -3.0876,\n",
      "          -3.2287, -3.2339],\n",
      "         [-3.1393, -3.0924, -3.1201, -3.1297, -3.0693, -3.0516, -3.1170,\n",
      "          -3.2180, -3.1660],\n",
      "         [-2.8488, -3.2266, -3.0114, -3.2623, -2.9960, -3.0495, -3.1542,\n",
      "          -3.1534, -3.0223],\n",
      "         [-2.6833, -3.2580, -2.9080, -3.2600, -2.9842, -3.1138, -3.0992,\n",
      "          -2.9849, -2.8463],\n",
      "         [-2.7700, -3.2573, -2.9819, -3.2926, -2.9697, -3.0361, -3.1644,\n",
      "          -3.1353, -2.9882],\n",
      "         [-2.9453, -2.9355, -2.8944, -2.8522, -3.0388, -3.1077, -2.8653,\n",
      "          -2.6817, -2.7935],\n",
      "         [-2.9541, -3.0760, -2.9746, -3.0525, -3.0556, -3.1265, -3.0020,\n",
      "          -2.9333, -2.9186],\n",
      "         [-2.9541, -3.0760, -2.9746, -3.0525, -3.0556, -3.1265, -3.0020,\n",
      "          -2.9333, -2.9186],\n",
      "         [-2.9541, -3.0760, -2.9746, -3.0525, -3.0556, -3.1265, -3.0020,\n",
      "          -2.9333, -2.9186]],\n",
      "\n",
      "        [[-2.5955, -3.3282, -2.9339, -3.3792, -2.8594, -2.9037, -3.2304,\n",
      "          -3.1559, -2.9816],\n",
      "         [-2.7315, -3.2816, -3.0230, -3.3637, -2.8373, -2.7866, -3.2844,\n",
      "          -3.3051, -3.1576],\n",
      "         [-3.1305, -3.1439, -3.1866, -3.2391, -2.9630, -2.8376, -3.2582,\n",
      "          -3.4422, -3.3605],\n",
      "         [-2.7522, -3.2467, -2.9660, -3.2775, -2.9404, -2.9914, -3.1609,\n",
      "          -3.1086, -2.9820],\n",
      "         [-2.9824, -3.0546, -2.9756, -3.0265, -3.0504, -3.1073, -2.9915,\n",
      "          -2.9170, -2.9252],\n",
      "         [-2.8986, -3.1565, -3.0043, -3.1752, -2.9838, -3.0075, -3.1124,\n",
      "          -3.0887, -3.0214],\n",
      "         [-2.6565, -3.1362, -2.7951, -3.0602, -3.0042, -3.1849, -2.9261,\n",
      "          -2.6583, -2.6361],\n",
      "         [-2.6966, -3.1348, -2.8302, -3.0750, -2.9947, -3.1433, -2.9579,\n",
      "          -2.7303, -2.7052],\n",
      "         [-3.1051, -3.0503, -3.0438, -3.0402, -3.0974, -3.1374, -3.0153,\n",
      "          -3.0178, -3.0118],\n",
      "         [-3.1629, -3.0754, -3.1102, -3.1008, -3.0834, -3.0745, -3.0898,\n",
      "          -3.1730, -3.1391],\n",
      "         [-2.9481, -2.9722, -2.9405, -2.9272, -2.9732, -2.9727, -2.9588,\n",
      "          -2.8317, -2.9225],\n",
      "         [-3.4181, -2.9049, -3.1859, -2.9164, -3.1264, -3.0362, -3.0225,\n",
      "          -3.1640, -3.2477],\n",
      "         [-3.4014, -2.8998, -3.1676, -2.9018, -3.1311, -3.0556, -3.0027,\n",
      "          -3.1222, -3.2121],\n",
      "         [-3.2899, -2.9311, -3.0935, -2.9068, -3.1433, -3.1398, -2.9568,\n",
      "          -2.9961, -3.0706],\n",
      "         [-3.2969, -2.9872, -3.1429, -2.9997, -3.1214, -3.0861, -3.0411,\n",
      "          -3.1467, -3.1723],\n",
      "         [-3.2946, -2.9836, -3.1346, -2.9901, -3.1300, -3.1055, -3.0276,\n",
      "          -3.1229, -3.1514],\n",
      "         [-3.2395, -2.9147, -3.0490, -2.8700, -3.1357, -3.1530, -2.9168,\n",
      "          -2.9023, -2.9977],\n",
      "         [-3.0881, -3.0573, -3.0425, -3.0510, -3.0822, -3.1147, -3.0270,\n",
      "          -3.0293, -3.0204],\n",
      "         [-3.3018, -3.0219, -3.1751, -3.0582, -3.1051, -3.0471, -3.0960,\n",
      "          -3.2446, -3.2401],\n",
      "         [-3.1732, -2.9351, -3.0290, -2.8941, -3.0979, -3.1097, -2.9349,\n",
      "          -2.9015, -2.9894],\n",
      "         [-3.5431, -2.8834, -3.2492, -2.9085, -3.1669, -3.0527, -3.0360,\n",
      "          -3.2460, -3.3311]]])\n",
      "\n",
      "Y Output Tag: \n",
      " tensor([[1, 7, 1, 7, 0, 0, 5, 5, 5, 3, 3, 1, 1, 5, 0, 0, 0, 7, 8, 8, 8],\n",
      "        [0, 0, 5, 0, 7, 0, 8, 0, 8, 5, 7, 1, 1, 3, 1, 1, 3, 8, 1, 3, 1]])\n",
      "\n",
      "Actual Output: \n",
      "[tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]), tensor([8, 8, 8, 3, 8, 8, 8, 6, 8, 8, 8, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in sample_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i) for i in x_batch]\n",
    "        y_batch = [torch.tensor(i) for i in y_batch]\n",
    "        \n",
    "        \n",
    "        print(\"X batch: \")\n",
    "        pprint(x_batch)\n",
    "        print(\"\\ny batch: \")\n",
    "        pprint(y_batch)\n",
    "        \n",
    "        y_out = gru_model_sample(x_batch)\n",
    "                        \n",
    "        _, y_out_tag = torch.max(y_out, dim = 2)\n",
    "        print(\"\\nY Output Tag: \\n\", y_out_tag)\n",
    "        \n",
    "        print(\"\\nActual Output: \")\n",
    "        print(y_batch)\n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 256\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "STACKED_LAYERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestData(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size, stacked_layers):\n",
    "        super(ModelGRU, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.gru = nn.GRU(input_size = embedding_size, hidden_size = hidden_size, batch_first = True, num_layers=stacked_layers)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        len_list = list(map(len, x_batch))        \n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)\n",
    "    \n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        \n",
    "        pack_embeds = pack_padded_sequence(embeds, lengths=len_list, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        rnn_out, rnn_hidden = self.gru(pack_embeds)\n",
    "        \n",
    "        rnn_out_padded = pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        \n",
    "        linear_out = self.linear(rnn_out_padded[0])\n",
    "        \n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelGRU(\n",
      "  (word_embeddings): Embedding(9341, 256)\n",
      "  (gru): GRU(256, 8, num_layers=4, batch_first=True)\n",
      "  (linear): Linear(in_features=8, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gru_model = ModelGRU(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx), stacked_layers=STACKED_LAYERS)\n",
    "\n",
    "gru_model.to(device)\n",
    "print(gru_model)\n",
    "\n",
    "tag_weights = torch.tensor([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0])\n",
    "criterion = nn.CrossEntropyLoss(weight=tag_weights.cuda())\n",
    "\n",
    "optimizer =  optim.Adam(gru_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 2.00441\n",
      "Epoch 002: | Loss: 1.63730\n",
      "Epoch 003: | Loss: 1.45445\n",
      "Epoch 004: | Loss: 1.38089\n",
      "Epoch 005: | Loss: 1.34010\n",
      "Epoch 006: | Loss: 1.27391\n",
      "Epoch 007: | Loss: 1.16676\n",
      "Epoch 008: | Loss: 1.08682\n",
      "Epoch 009: | Loss: 1.04209\n",
      "Epoch 010: | Loss: 1.00331\n",
      "Epoch 011: | Loss: 0.97911\n",
      "Epoch 012: | Loss: 0.96183\n",
      "Epoch 013: | Loss: 0.94598\n",
      "Epoch 014: | Loss: 0.93243\n",
      "Epoch 015: | Loss: 0.92266\n",
      "Epoch 016: | Loss: 0.92143\n",
      "Epoch 017: | Loss: 0.91708\n",
      "Epoch 018: | Loss: 0.92382\n",
      "Epoch 019: | Loss: 0.92579\n",
      "Epoch 020: | Loss: 0.93433\n",
      "Epoch 021: | Loss: 0.93409\n",
      "Epoch 022: | Loss: 0.92882\n",
      "Epoch 023: | Loss: 0.94685\n",
      "Epoch 024: | Loss: 0.93152\n",
      "Epoch 025: | Loss: 0.93754\n",
      "Epoch 026: | Loss: 0.95781\n",
      "Epoch 027: | Loss: 0.92156\n",
      "Epoch 028: | Loss: 0.96707\n",
      "Epoch 029: | Loss: 0.94503\n",
      "Epoch 030: | Loss: 0.95098\n"
     ]
    }
   ],
   "source": [
    "gru_model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = [torch.tensor(i).to(device) for i in y_batch]\n",
    "        y_batch = pad_sequence(y_batch, batch_first=True)\n",
    "        \n",
    "        y_pred = gru_model(x_batch)\n",
    "                        \n",
    "        loss = criterion(y_pred.permute(0, 2, 1), y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_tags_list = []\n",
    "with torch.no_grad():        \n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = [torch.tensor(i).to(device) for i in y_batch]\n",
    "        y_batch = pad_sequence(y_batch, batch_first=True)\n",
    "\n",
    "    \n",
    "        y_pred = gru_model(x_batch)\n",
    "        y_pred_logsoftmax = torch.log_softmax(y_pred, dim = 2)\n",
    "        _, y_pred_tag = torch.max(y_pred, dim = 2)\n",
    "        y_out_tags_list.append(y_pred_tag.squeeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.08      0.85      0.14       639\n",
      "           2       0.12      0.80      0.21       787\n",
      "           3       0.04      0.25      0.07       337\n",
      "           4       0.00      0.00      0.00         1\n",
      "           6       0.38      0.02      0.04       606\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00     12015\n",
      "\n",
      "    accuracy                           0.09     14386\n",
      "   macro avg       0.09      0.27      0.07     14386\n",
      "weighted avg       0.03      0.09      0.02     14386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshaj/miniconda3/envs/toothless/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_out_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "testl = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                      : Pred           : Actual\n",
      "\n",
      "'there                    : I-PER          : O'\n",
      "'were                     : I-PER          : O'\n",
      "'big                      : I-PER          : O'\n",
      "'buyers                   : I-PER          : O'\n",
      "'at                       : I-PER          : O'\n",
      "'the                      : I-PER          : O'\n",
      "'base                     : I-PER          : O'\n",
      "'of                       : I-PER          : O'\n",
      "'where                    : I-PER          : O'\n",
      "'the                      : I-MISC         : O'\n",
      "'market                   : I-MISC         : O'\n",
      "'sold                     : I-MISC         : O'\n",
      "'to                       : I-MISC         : O'\n",
      "'and                      : I-MISC         : O'\n",
      "'when                     : I-PER          : O'\n",
      "'the                      : I-MISC         : O'\n",
      "'currency                 : I-MISC         : O'\n",
      "'got                      : I-MISC         : O'\n",
      "'bought                   : I-ORG          : O'\n",
      "'back                     : I-MISC         : O'\n",
      "'on                       : I-PER          : O'\n",
      "'talk                     : I-MISC         : O'\n",
      "'of                       : I-ORG          : O'\n",
      "'a                        : I-MISC         : O'\n",
      "'the                      : I-ORG          : O'\n",
      "'market                   : I-ORG          : O'\n",
      "'got                      : I-ORG          : O'\n",
      "'bought                   : I-ORG          : O'\n",
      "'back                     : I-ORG          : O'\n",
      "'again                    : I-PER          : O'\n",
      "'a                        : I-PER          : O'\n",
      "'dealer                   : I-PER          : O'\n",
      "'said                     : I-PER          : O'\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():        \n",
    "    batch = next(testl)\n",
    "    x_batch, y_batch = map(list, zip(*batch))\n",
    "    x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "    y_batch = [torch.tensor(i).to(device) for i in y_batch]\n",
    "    y_batch = pad_sequence(y_batch, batch_first=True)\n",
    "\n",
    "\n",
    "    y_pred = gru_model(x_batch)\n",
    "    y_pred_logsoftmax = torch.log_softmax(y_pred, dim = 2)\n",
    "    _, y_pred_tag = torch.max(y_pred, dim = 2)\n",
    "    \n",
    "    x_batch = x_batch[0].cpu().numpy()\n",
    "    y_batch = y_batch[0].cpu().numpy()\n",
    "    y_pred_tag = y_pred_tag[0].cpu().numpy()\n",
    "    \n",
    "    print('{:26}: {:15}: {}\\n'.format(\"Word\", \"Pred\", \"Actual\"))\n",
    "    \n",
    "    for w, pred, actual in zip(x_batch, y_pred_tag, y_batch):\n",
    "        pprint(\"{:25}: {:15}: {}\".format(words[w.item()], tags[pred], tags[actual.item()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
