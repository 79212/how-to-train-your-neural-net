{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word Vectors - GloVe\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66e911f5b96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument here means that the vectors were generated by training a corpus of 6 Billion words. The second dimenstion is arguement, which means that each word is a vector of length 100.   \n",
    "\n",
    "42B and 840B word vectors are also available, but only at a dimension of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name = '6B', dim = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total number of words in `glove`.   \n",
    "All the words are in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in glove = 400000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of words in glove = {len(glove.itos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functios that we use to convert a string to integer and an integer to string. The integer here refers to the index of the word in the vector. Each vector is of length 100 here.  \n",
    "\n",
    "\n",
    "`glove.itos[n]` is used to convert an index to word.  \n",
    "`glove.stoi[n]` is used to convert an word to index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th element in the glove vector list is : 'for'\n",
      "The index of word 'for' is: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"The 10th element in the glove vector list is : '{glove.itos[10]}'\")\n",
    "print(f\"The index of word 'for' is: {glove.stoi['for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of each vector should be 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the vector representation of a word by first converting the word into an integer (index) and then convert that into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vector is = torch.Size([100])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2493,  0.6832, -0.0447, -1.3842, -0.0073,  0.6510, -0.3396, -0.1979,\n",
       "        -0.3392,  0.2669, -0.0331,  0.1592,  0.8955,  0.5400, -0.5582,  0.4624,\n",
       "         0.3672,  0.1889,  0.8319,  0.8142, -0.1183, -0.5346,  0.2416, -0.0389,\n",
       "         1.1907,  0.7935, -0.1231,  0.6642, -0.7762, -0.4571, -1.0540, -0.2056,\n",
       "        -0.1330,  0.1224,  0.8846,  1.0240,  0.3229,  0.8210, -0.0694,  0.0242,\n",
       "        -0.5142,  0.8727,  0.2576,  0.9153, -0.6422,  0.0412, -0.6021,  0.5463,\n",
       "         0.6608,  0.1980, -1.1393,  0.7951,  0.4597, -0.1846, -0.6413, -0.2493,\n",
       "        -0.4019, -0.5079,  0.8058,  0.5336,  0.5273,  0.3925, -0.2988,  0.0096,\n",
       "         0.9995, -0.0613,  0.7194,  0.3290, -0.0528,  0.6714, -0.8025, -0.2579,\n",
       "         0.4961,  0.4808, -0.6840, -0.0122,  0.0482,  0.2946,  0.2061,  0.3356,\n",
       "        -0.6417, -0.6471,  0.1338, -0.1257, -0.4638,  1.3878,  0.9564, -0.0679,\n",
       "        -0.0017,  0.5296,  0.4567,  0.6104, -0.1151,  0.4263,  0.1734, -0.7995,\n",
       "        -0.2450, -0.6089, -0.3847, -0.4797])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of the vector is = {glove.vectors[glove.stoi['python']].shape}\\n\") \n",
    "glove.vectors[glove.stoi['python']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Used In Similar Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_vec(embeddings, word):\n",
    "    i = embeddings.stoi[word]\n",
    "    v = embeddings.vectors[i]\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(embeddings, vector, n = 5):\n",
    "    distances = [(word, torch.dist(vector, word_2_vec(embeddings, word)).item()) for word in embeddings.itos]    \n",
    "    shortest_distances = sorted(distances, key = lambda w: w[1])[:n]\n",
    "    \n",
    "    return shortest_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plane', 0.0),\n",
       " ('airplane', 3.212670087814331),\n",
       " ('jet', 3.7022786140441895),\n",
       " ('flight', 3.788144588470459),\n",
       " ('crashed', 3.8278510570526123)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_plane = word_2_vec(glove, \"plane\")\n",
    "similar_words(glove, vector_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try the famous **king - man + woman = queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(embeddings, w1, w2, w3, n = 5):\n",
    "    \n",
    "    # Convert word to vector\n",
    "    v1 = word_2_vec(embeddings, w1)\n",
    "    v2 = word_2_vec(embeddings, w2)\n",
    "    v3 = word_2_vec(embeddings, w3)\n",
    "    \n",
    "    # Perform vector arithmetic\n",
    "    analogy_vec = v1 - v2 + v3\n",
    "    \n",
    "    # Get closest word\n",
    "    closest_words = similar_words(embeddings, analogy_vec, n)\n",
    "    \n",
    "    # Remove words alread in w1, w2, oe w3.\n",
    "    possible_words = [(word, dist) for (word, dist) in closest_words if word not in [w1, w2, w3]]\n",
    "    \n",
    "    return possible_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 4.08107852935791),\n",
      " ('monarch', 4.642907619476318),\n",
      " ('throne', 4.905500888824463),\n",
      " ('elizabeth', 4.921558380126953)]\n",
      "\n",
      "king - man + woman = queen\n"
     ]
    }
   ],
   "source": [
    "a_word = get_analogy(glove, 'king', 'man', 'woman')\n",
    "pprint(a_word)\n",
    "print(f\"\\nking - man + woman = {a_word[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 2.8133397102355957),\n",
      " ('comedian', 5.003941535949707),\n",
      " ('actresses', 5.139926433563232),\n",
      " ('starred', 5.277286052703857)]\n",
      "\n",
      "actor - man + woman = actress\n"
     ]
    }
   ],
   "source": [
    "a_word = get_analogy(glove, 'actor', 'man', 'woman')\n",
    "pprint(a_word)\n",
    "print(f\"\\nactor - man + woman = {a_word[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paris', 2.9362051486968994),\n",
      " ('amsterdam', 5.051050186157227),\n",
      " ('lyon', 5.251315116882324)]\n",
      "\n",
      "britain:london = france:paris\n"
     ]
    }
   ],
   "source": [
    "a_word = get_analogy(glove, 'london', 'britain', 'france')\n",
    "pprint(a_word)\n",
    "print(f\"\\nbritain:london = france:{a_word[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
