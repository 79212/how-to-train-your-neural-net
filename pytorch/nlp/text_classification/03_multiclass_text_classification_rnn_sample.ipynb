{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Multi-Class Text Classification using RNNs - Sample\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)  \n",
    "\n",
    "This notebook takes you through a sample implementation of multi-class text classification in the form of sentiment analysis on yelp reviews using RNNs in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe59848d390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"Ronaldo plays football.\".split(), \"football\"),\n",
    "    (\"Cho likes quidditch.\".split(), \"quidditch\"),\n",
    "    (\"Jordan adores basketball.\".split(), \"basketball\"),\n",
    "    (\"McTominay plays football.\".split(), \"football\"),\n",
    "    (\"Woods likes quidditch.\".split(), \"quidditch\"),\n",
    "    (\"Kobe adores basketball.\".split(), \"basketball\"),\n",
    "    (\"Scholes likes quidditch.\".split(), \"football\"),\n",
    "    (\"Ginny adores quidditch.\".split(), \"quidditch\")\n",
    "\n",
    "]\n",
    "\n",
    "sentence_list = [training_data[x][0] for x in range(len(training_data))]\n",
    "tag_list = [training_data[x][1] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ronaldo', 'plays', 'football.'],\n",
       " ['Cho', 'likes', 'quidditch.'],\n",
       " ['Jordan', 'adores', 'basketball.'],\n",
       " ['McTominay', 'plays', 'football.'],\n",
       " ['Woods', 'likes', 'quidditch.'],\n",
       " ['Kobe', 'adores', 'basketball.'],\n",
       " ['Scholes', 'likes', 'quidditch.'],\n",
       " ['Ginny', 'adores', 'quidditch.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football',\n",
       " 'quidditch',\n",
       " 'basketball',\n",
       " 'football',\n",
       " 'quidditch',\n",
       " 'basketball',\n",
       " 'football',\n",
       " 'quidditch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data by converting it into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_list = []\n",
    "for sentence, tags in training_data:\n",
    "    clean_sentence = [x.lower().split('.')[0] for x in sentence]\n",
    "    data_clean_list += [(clean_sentence, tags)]\n",
    "\n",
    "    \n",
    "sentence_clean_list = [data_clean_list[x][0] for x in range(len(data_clean_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ronaldo', 'plays', 'football'],\n",
       " ['cho', 'likes', 'quidditch'],\n",
       " ['jordan', 'adores', 'basketball'],\n",
       " ['mctominay', 'plays', 'football'],\n",
       " ['woods', 'likes', 'quidditch'],\n",
       " ['kobe', 'adores', 'basketball'],\n",
       " ['scholes', 'likes', 'quidditch'],\n",
       " ['ginny', 'adores', 'quidditch']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 14\n",
      "\n",
      "['cho', 'woods', 'ginny', 'plays', 'scholes', 'adores', 'quidditch', 'ronaldo', 'likes', 'mctominay', 'kobe', 'basketball', 'football', 'jordan']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_clean_list:\n",
    "    words += sentence\n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cho': 0, 'woods': 1, 'ginny': 2, 'plays': 3, 'scholes': 4, 'adores': 5, 'quidditch': 6, 'ronaldo': 7, 'likes': 8, 'mctominay': 9, 'kobe': 10, 'basketball': 11, 'football': 12, 'jordan': 13}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 3\n",
      "\n",
      "['basketball', 'football', 'quidditch']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags.append(tag)\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basketball': 0, 'football': 1, 'quidditch': 2}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ronaldo', 'plays', 'football'],\n",
       "  ['cho', 'likes', 'quidditch'],\n",
       "  ['jordan', 'adores', 'basketball'],\n",
       "  ['mctominay', 'plays', 'football'],\n",
       "  ['woods', 'likes', 'quidditch'],\n",
       "  ['kobe', 'adores', 'basketball'],\n",
       "  ['scholes', 'likes', 'quidditch'],\n",
       "  ['ginny', 'adores', 'quidditch']],\n",
       " ['football',\n",
       "  'quidditch',\n",
       "  'basketball',\n",
       "  'football',\n",
       "  'quidditch',\n",
       "  'basketball',\n",
       "  'football',\n",
       "  'quidditch'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 3, 12],\n",
       " [0, 8, 6],\n",
       " [13, 5, 11],\n",
       " [9, 3, 12],\n",
       " [1, 8, 6],\n",
       " [10, 5, 11],\n",
       " [4, 8, 6],\n",
       " [2, 5, 6]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[w] for w in s] for s in sentence_clean_list]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 1, 2, 0, 1, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [tag2idx[t] for t in tag_list]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Params and Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input -> RNN -> Linear -> Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 6\n",
    "HIDDEN_SIZE = 2\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 10\n",
    "STACKED_LAYERS = 5\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(torch.Tensor(X).to(torch.long), torch.Tensor(y).to(torch.float32))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 7,  3, 12],\n",
      "        [ 0,  8,  6],\n",
      "        [13,  5, 11],\n",
      "        [ 9,  3, 12]]), tensor([1., 2., 0., 1.]))\n",
      "(tensor([[ 1,  8,  6],\n",
      "        [10,  5, 11],\n",
      "        [ 4,  8,  6],\n",
      "        [ 2,  5,  6]]), tensor([2., 0., 1., 2.]))\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_loader:\n",
    "    print((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUtagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size, stacked_layers):\n",
    "        super(GRUtagger, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.gru = nn.GRU(input_size = embedding_size, hidden_size=hidden_size, batch_first = True, num_layers = stacked_layers)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, gru_hidden = self.gru(embeds)\n",
    "        linear_out = self.linear(self.tanh(gru_hidden))\n",
    "        \n",
    "        y_out = linear_out[-1]\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUtagger(\n",
      "  (word_embeddings): Embedding(14, 6)\n",
      "  (gru): GRU(6, 2, num_layers=5, batch_first=True)\n",
      "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRUtagger(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx), stacked_layers=STACKED_LAYERS)\n",
    "print(gru_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  optim.Adam(gru_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the GRU output from the model looks.\n",
    "\n",
    "output = [batch size, sent len, hid dim]  \n",
    "hidden = [batch size, 1, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[ 7,  3, 12],\n",
      "        [ 0,  8,  6],\n",
      "        [13,  5, 11],\n",
      "        [ 9,  3, 12]])\n",
      "\n",
      "Linear Output:\n",
      "tensor([[-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714]])\n",
      "\n",
      "LogSoftmax Output:\n",
      "tensor([[-1.7522, -1.6066, -0.4684],\n",
      "        [-1.7521, -1.6067, -0.4683],\n",
      "        [-1.7521, -1.6067, -0.4683],\n",
      "        [-1.7522, -1.6066, -0.4683]])\n",
      "\n",
      "Output Indices:\n",
      "tensor([2, 2, 2, 2])\n",
      "\n",
      "Actual Output:\n",
      "tensor([1., 2., 0., 1.])\n",
      "==================================================\n",
      "Input:\n",
      "tensor([[ 1,  8,  6],\n",
      "        [10,  5, 11],\n",
      "        [ 4,  8,  6],\n",
      "        [ 2,  5,  6]])\n",
      "\n",
      "Linear Output:\n",
      "tensor([[-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714],\n",
      "        [-0.7124, -0.5669,  0.5714]])\n",
      "\n",
      "LogSoftmax Output:\n",
      "tensor([[-1.7521, -1.6067, -0.4683],\n",
      "        [-1.7521, -1.6067, -0.4683],\n",
      "        [-1.7522, -1.6066, -0.4683],\n",
      "        [-1.7521, -1.6067, -0.4683]])\n",
      "\n",
      "Output Indices:\n",
      "tensor([2, 2, 2, 2])\n",
      "\n",
      "Actual Output:\n",
      "tensor([2., 0., 1., 2.])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        print(\"Input:\")\n",
    "        print(x_batch)\n",
    "        y_out = gru_model(x_batch)\n",
    "        \n",
    "        y_out_softmax = torch.log_softmax(y_out, dim = 1)\n",
    "        _, y_out_tags = torch.max(y_out_softmax, dim = 1)\n",
    "        \n",
    "        print(\"\\nLinear Output:\")\n",
    "        print(y_out)\n",
    "        \n",
    "        print(\"\\nLogSoftmax Output:\")\n",
    "        print(y_out_softmax)\n",
    "        \n",
    "        print(\"\\nOutput Indices:\")\n",
    "        print(y_out_tags)\n",
    "        \n",
    "        print(\"\\nActual Output:\")\n",
    "        print(y_batch)\n",
    "        \n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 1.21583 | Accuracy: 0.5\n",
      "Epoch: 02 | Loss: 1.21336 | Accuracy: 0.5\n",
      "Epoch: 03 | Loss: 1.21102 | Accuracy: 0.5\n",
      "Epoch: 04 | Loss: 1.20873 | Accuracy: 0.5\n",
      "Epoch: 05 | Loss: 1.20647 | Accuracy: 0.5\n",
      "Epoch: 06 | Loss: 1.20425 | Accuracy: 0.5\n",
      "Epoch: 07 | Loss: 1.20205 | Accuracy: 0.5\n",
      "Epoch: 08 | Loss: 1.19989 | Accuracy: 0.5\n",
      "Epoch: 09 | Loss: 1.19776 | Accuracy: 0.5\n",
      "Epoch: 10 | Loss: 1.19566 | Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "gru_model.train()\n",
    "for e in range(1, EPOCH+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_out = gru_model(x_batch)\n",
    "               \n",
    "        loss = criterion(y_out, y_batch.squeeze().long())\n",
    "        acc = multi_acc(y_out, y_batch.squeeze().long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch: {e+0:02} | Loss: {epoch_loss/len(train_loader):.5f} | Accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
