{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Multi-Class Text Classification using RNNs\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)  \n",
    "\n",
    "This notebook takes you through the implementation of binary text classification in the form of sentiment analysis on yelp reviews using RNNs in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa45b4c02d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../data/nlp/text_classification/bbc-text.csv\")\n",
    "df = df.rename(columns = {'category':'tag'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [t for t in df['text'].to_list()]\n",
    "tag_list = [t for t in df['tag'].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.',\n",
       " 'worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tech', 'business']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "sentence_list = [s.lower() for s in sentence_list]\n",
    "\n",
    "# Remove non alphavets\n",
    "regex_remove_nonalphabets = re.compile('[^a-zA-Z]')\n",
    "sentence_list = [regex_remove_nonalphabets.sub(' ', s) for s in sentence_list]\n",
    "\n",
    "# Remove words with less than 2 letters\n",
    "# regex_remove_shortwords = re.compile(r'\\b\\w{1,2}\\b')\n",
    "# sentence_list = [regex_remove_shortwords.sub(\"\", s) for s in sentence_list]\n",
    "\n",
    "# Remove words that appear only once\n",
    "c = Counter(w for s in sentence_list for w in s.split())\n",
    "sentence_list = [' '.join(y for y in x.split() if c[y] > 1) for x in sentence_list]\n",
    "\n",
    "# Strip extra whitespaces\n",
    "sentence_list = [\" \".join(s.split()) for s in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv future in the hands of viewers with home theatre systems plasma high definition tvs and digital video recorders moving into the living room the way people watch tv will be radically different in five years time that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite with the us leading the trend programmes and other content will be delivered to viewers via home networks through cable satellite telecoms companies and broadband service providers to front rooms and portable devices one of the most talked about technologies of ces has been digital and personal video recorders dvr and pvr these set top boxes like the us s tivo and the uk s sky system allow people to record store play pause and forward wind tv programmes when they want essentially the technology allows for much more personalised tv they are also being built in to high definition tv sets which are big business in japan and the us but slower to take off in europe because of the lack of high definition programming not only can people forward wind through adverts they can also forget about abiding by network and channel schedules putting together their own a la carte entertainment but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as brand identity and viewer loyalty to channels although the us leads in this technology at the moment it is also a concern that is being raised in europe particularly with the growing uptake of services like sky what happens here today we will see in nine months to a years time in the uk adam the bbc broadcast s futurologist told the bbc news website for the likes of the bbc there are no issues of lost advertising revenue yet it is a more pressing issue at the moment for commercial uk broadcasters but brand loyalty is important for everyone we will be talking more about content brands rather than network brands said tim hanlon from brand communications firm the reality is that with broadband connections anybody can be the producer of content he added the challenge now is that it is hard to promote a programme with so much choice what this means said senior vice president of tv guide tv group is that the way people find the content they want to watch has to be simplified for tv viewers it means that networks in us terms or channels could take a leaf out of google s book and be the search engine of the future instead of the to help people find what they want to watch this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them but it might not suit everyone the panel recognised older generations are more comfortable with familiar schedules and channel brands because they know what they are getting they perhaps do not want so much of the choice put into their hands mr hanlon suggested on the other end you have the kids just out of who are pushing buttons already everything is possible and available to them said mr hanlon ultimately the consumer will tell the market they want of the new gadgets and technologies being showcased at ces many of them are about enhancing the tv watching experience high definition tv sets are everywhere and many new models of lcd liquid crystal display tvs have been launched with dvr capability built into them instead of being external boxes one such example launched at the show is s inch lcd tv with an hour tivo dvr and dvd recorder one of the us s biggest satellite tv companies has even launched its own branded dvr at the show with hours of recording capability instant replay and a search function the set can pause and rewind tv for up to hours and microsoft chief bill gates announced in his pre show keynote speech a partnership with tivo called which means people can play recorded programmes on windows pcs and mobile devices all these reflect the increasing trend of freeing up multimedia so that people can watch what they want when they want',\n",
       " 'worldcom boss left books alone former worldcom boss bernie ebbers who is accused of overseeing an bn bn fraud never made accounting decisions a witness has told jurors david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems the phone company collapsed in and prosecutors claim that losses were hidden to protect the firm s shares mr myers has already pleaded guilty to fraud and is assisting prosecutors on monday defence lawyer reid weingarten tried to distance his client from the allegations during cross examination he asked mr myers if he ever knew mr ebbers make an accounting decision not that i am aware of mr myers replied did you ever know mr ebbers to make an accounting entry into worldcom books mr weingarten pressed no replied the witness mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan defence lawyers have been trying to paint mr sullivan who has admitted fraud and will testify later in the trial as the mastermind behind worldcom s accounting house of cards mr ebbers team meanwhile are looking to portray him as an boss who by his own admission is more pe graduate than economist whatever his abilities mr ebbers transformed worldcom from a relative unknown into a bn telecoms giant and investor darling of the late s worldcom s problems mounted however as competition increased and the telecoms boom petered out when the firm finally collapsed shareholders lost about bn and workers lost their jobs mr ebbers trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence he has firmly declared his innocence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 18636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_list:\n",
    "    for w in sentence.split():\n",
    "        words.append(w)\n",
    "    \n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 5\n",
      "\n",
      "['politics', 'tech', 'entertainment', 'business', 'sport']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags.append(tag)\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'politics': 0, 'tech': 1, 'entertainment': 2, 'business': 3, 'sport': 4}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the input and output to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[w] for w in s.split()] for s in sentence_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [tag2idx[t] for t in tag_list]\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  1557\n",
      "X_test size:  668\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train size: \", len(X_train))\n",
    "print(\"X_test size: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_SAMPLE = 2\n",
    "EMBEDDING_SIZE_SAMPLE = 5\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE_SAMPLE = 3\n",
    "STACKED_LAYERS_SAMPLE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = SampleData(X_train, y_train)\n",
    "sample_loader = DataLoader(sample_data, batch_size=BATCH_SIZE_SAMPLE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14907, 9218, 15699, 1052, 2641, 8401, 334, 18346, 18407, 7297, 3470, 14480, 13709, 1495, 15055, 7794, 18084, 5795, 1346, 5795, 334, 15130, 334, 17560, 16640, 14480, 5994, 2641, 7763, 3639, 8401, 334, 17833, 16131, 11967, 16182, 334, 1442, 10192, 11798, 1495, 9484, 3126, 4132, 11870, 12269, 15213, 11967, 15510, 13077, 334, 10787, 4321, 11870, 9347, 3781, 11718, 16661, 17314, 9870, 5651, 3018, 13077, 8954, 1346, 334, 17907, 16730, 4321, 334, 8140, 4616, 11870, 5023, 14523, 2404, 11967, 10875, 2797, 16567, 4321, 13709, 2667, 334, 15251, 15447, 9245, 10525, 2428, 4321, 334, 1093, 16709, 1796, 4321, 11870, 2641, 11967, 11119, 14523, 2404, 11967, 18198, 12625, 16842, 9490, 16302, 3242, 11967, 1235, 7152, 6381, 11967, 15348, 5690, 6149, 1993, 11967, 4628, 8018, 5385, 4854, 7922, 334, 1442, 17786, 334, 4980, 11967, 4749, 7192, 4321, 3233, 4321, 13709, 8313, 15792, 11870, 15055, 7708, 18084, 5195, 11967, 9801, 9036, 9484, 2616, 4052, 1495, 13077, 15348, 10945, 15051, 16640, 18396, 10178, 17214, 12037, 3409, 13709, 17345, 69, 334, 1442, 9886, 15447, 10945, 4382, 9218, 11967, 14711, 17214, 1796, 5811, 4321, 9245, 8140, 3798, 8604, 15792, 11967, 14362, 13709, 9462, 334, 8469, 4425, 384, 4831, 5795, 1603, 16319, 12713, 11684, 11870, 198, 12247, 5795, 4870, 4854, 7625, 10929, 1085, 17315, 1346, 4321, 11870, 2641, 17699, 15416, 5795, 334, 15130, 6660, 10945, 5231, 12664, 15725, 10134, 7763, 14933, 1233, 15169, 11967, 11229, 7587, 16085, 6381, 16661, 15725, 10136, 3624, 11870, 8401, 10945, 8590, 18583, 18585, 4512, 10945, 4148, 5651, 12269, 12056, 1848, 14193, 13254, 11967, 9641, 5795, 11056, 334, 16855, 2641, 10910, 11967, 15375, 11796, 12321, 14205, 8401, 3836, 12354, 271, 4321, 11870, 334, 7110, 3836, 17461, 5651, 8140, 8117, 5109, 18601, 334, 3467, 9908, 4321, 5925, 17214, 16664, 893, 2641, 6132, 7587, 15298, 5651, 9810, 18061, 12081, 14202, 4321, 334, 893, 17540, 17786, 900, 8712, 18601, 17771, 13525, 14523, 15447, 1219, 11870, 16640, 4980, 11967, 16148, 11684, 863, 9844, 12489, 10220, 10945, 15103, 12269, 5586, 12190, 13077, 14523, 10113, 18084, 14351, 2207, 5028, 11967, 3907, 11870, 9507, 17174, 9870, 5651, 3018, 13077, 8954, 1346, 14523, 334, 12269, 4148, 4512, 14523, 10395, 4145, 792, 10945, 9986, 6381, 15705, 4321, 18267, 15293, 8401, 11870, 9347, 3781, 15725, 10113, 14677, 2158, 3781, 11967, 1848, 5927, 5231, 13605, 8401, 3172, 10147, 9036, 619, 3226, 16661, 4749, 10945, 9240, 16131, 6381, 15447, 10945, 12354, 4052, 18300, 4512, 2368, 334, 15023, 15293, 334, 1442, 2641, 8954, 1174, 5195, 11300, 13334, 16848, 17786, 3572, 10696, 5385, 334, 13261, 9347, 334, 9245, 4382, 8140, 1442, 4512, 8349, 13821, 17214, 8786, 17296, 5651, 16848, 18300, 4512, 13077, 11870, 10395, 18471, 14703, 8572, 11967, 17361, 11684, 1207, 14523, 16640, 3206, 17936, 10945, 11937, 18581, 11309, 334, 2041, 18300, 4512, 18300, 4512, 13077, 3467, 17347, 8401, 13670, 11967, 6285, 11684, 11870, 5661, 11684, 17347, 8401, 334, 18464, 10112, 5651, 14523, 10113, 3639, 16112, 271, 17114, 11967, 11796, 271, 18601, 10945, 16131, 5651, 18396, 17214, 1207, 11967, 13992, 11870, 10395, 5195, 18601, 10945, 5221, 1442, 14523, 2641, 5195, 11967, 2424, 11798, 12708, 18601, 1872, 6660, 334, 17828, 4321, 4100, 6381, 11967, 3639, 17278, 13744, 5690, 2667, 119], [7885, 7490, 8401, 573, 16095, 3391, 7885, 2585, 9545, 8159, 16640, 17725, 1233, 5465, 1473, 14857, 7110, 11221, 11967, 12123, 8401, 12303, 5651, 334, 3467, 573, 10613, 5795, 13254, 334, 17693, 2942, 10395, 14471, 10192, 16318, 18083, 17214, 5795, 7985, 9186, 16640, 3467, 7110, 11221, 10613, 1915, 939, 924, 11967, 484, 8401, 334, 6020, 14523, 10395, 5465, 4986, 1967, 1915, 2236, 16923, 14537, 4321, 5984, 17214, 1003, 3309, 8401, 334, 573, 7499, 939, 4512, 6124, 4321, 18558, 5984, 2236, 2696, 16822, 8458, 13845, 9218, 3913, 5795, 15076, 7885, 68, 6096, 13177, 5795, 334, 16923, 9462, 334, 15756, 9592, 8113, 334, 5246, 7110, 11221, 14863, 10131, 334, 14863, 14641, 1280, 5795, 7985, 9186, 939, 2641, 16709, 11967, 15375, 6919, 16923, 7499, 11684, 10945, 484, 5795, 14756, 13621, 6381, 2316, 9462, 334, 16406, 4321, 8159]] \n",
      "\n",
      " [1, 4] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tl = iter(sample_loader)\n",
    "\n",
    "i,j = map(list, zip(*next(tl)))\n",
    "\n",
    "print(i,\"\\n\\n\", j, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGRUSample(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size, stacked_layers):\n",
    "        super(ModelGRUSample, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.gru = nn.GRU(input_size = embedding_size, hidden_size = hidden_size, batch_first = True, num_layers=stacked_layers)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        print(\"\\nList of tensor lengths in a batch: \")\n",
    "        len_list = list(map(len, x_batch))\n",
    "        print(len_list)\n",
    "        \n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)\n",
    "        print(\"\\nPadded X_batch: \\n\", padded_batch, \"\\n\")\n",
    "\n",
    "        \n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        print(\"\\nEmbeddings:\", embeds, embeds.size(), \"\\n\")\n",
    "\n",
    "        pack_embeds = pack_padded_sequence(embeds, lengths=len_list, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        rnn_out, rnn_hidden = self.gru(pack_embeds)\n",
    "        print(\"\\nRNN hidden last layer:\\n\", rnn_hidden)\n",
    "        \n",
    "        linear_out = self.linear(rnn_hidden)\n",
    "        print(\"\\nLinear Output:\\n\", linear_out)\n",
    "        \n",
    "        y_out = torch.log_softmax(linear_out, dim = 1)\n",
    "        y_out = y_out[-1]\n",
    "        print(\"\\nLogSoftmax:\\n\", y_out)\n",
    "\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelGRUSample(\n",
      "  (word_embeddings): Embedding(18636, 5)\n",
      "  (gru): GRU(5, 3, num_layers=4, batch_first=True)\n",
      "  (linear): Linear(in_features=3, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gru_model_sample = ModelGRUSample(embedding_size=EMBEDDING_SIZE_SAMPLE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE_SAMPLE, target_size=len(tag2idx), stacked_layers=STACKED_LAYERS_SAMPLE)\n",
    "print(gru_model_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output.\n",
    "\n",
    "output = [batch size, sent len, hid dim]  \n",
    "hidden = [batch size, 1, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch: \n",
      "[tensor([14907,  9218, 15699,  1052,  2641,  8401,   334, 18346, 18407,  7297,\n",
      "         3470, 14480, 13709,  1495, 15055,  7794, 18084,  5795,  1346,  5795,\n",
      "          334, 15130,   334, 17560, 16640, 14480,  5994,  2641,  7763,  3639,\n",
      "         8401,   334, 17833, 16131, 11967, 16182,   334,  1442, 10192, 11798,\n",
      "         1495,  9484,  3126,  4132, 11870, 12269, 15213, 11967, 15510, 13077,\n",
      "          334, 10787,  4321, 11870,  9347,  3781, 11718, 16661, 17314,  9870,\n",
      "         5651,  3018, 13077,  8954,  1346,   334, 17907, 16730,  4321,   334,\n",
      "         8140,  4616, 11870,  5023, 14523,  2404, 11967, 10875,  2797, 16567,\n",
      "         4321, 13709,  2667,   334, 15251, 15447,  9245, 10525,  2428,  4321,\n",
      "          334,  1093, 16709,  1796,  4321, 11870,  2641, 11967, 11119, 14523,\n",
      "         2404, 11967, 18198, 12625, 16842,  9490, 16302,  3242, 11967,  1235,\n",
      "         7152,  6381, 11967, 15348,  5690,  6149,  1993, 11967,  4628,  8018,\n",
      "         5385,  4854,  7922,   334,  1442, 17786,   334,  4980, 11967,  4749,\n",
      "         7192,  4321,  3233,  4321, 13709,  8313, 15792, 11870, 15055,  7708,\n",
      "        18084,  5195, 11967,  9801,  9036,  9484,  2616,  4052,  1495, 13077,\n",
      "        15348, 10945, 15051, 16640, 18396, 10178, 17214, 12037,  3409, 13709,\n",
      "        17345,    69,   334,  1442,  9886, 15447, 10945,  4382,  9218, 11967,\n",
      "        14711, 17214,  1796,  5811,  4321,  9245,  8140,  3798,  8604, 15792,\n",
      "        11967, 14362, 13709,  9462,   334,  8469,  4425,   384,  4831,  5795,\n",
      "         1603, 16319, 12713, 11684, 11870,   198, 12247,  5795,  4870,  4854,\n",
      "         7625, 10929,  1085, 17315,  1346,  4321, 11870,  2641, 17699, 15416,\n",
      "         5795,   334, 15130,  6660, 10945,  5231, 12664, 15725, 10134,  7763,\n",
      "        14933,  1233, 15169, 11967, 11229,  7587, 16085,  6381, 16661, 15725,\n",
      "        10136,  3624, 11870,  8401, 10945,  8590, 18583, 18585,  4512, 10945,\n",
      "         4148,  5651, 12269, 12056,  1848, 14193, 13254, 11967,  9641,  5795,\n",
      "        11056,   334, 16855,  2641, 10910, 11967, 15375, 11796, 12321, 14205,\n",
      "         8401,  3836, 12354,   271,  4321, 11870,   334,  7110,  3836, 17461,\n",
      "         5651,  8140,  8117,  5109, 18601,   334,  3467,  9908,  4321,  5925,\n",
      "        17214, 16664,   893,  2641,  6132,  7587, 15298,  5651,  9810, 18061,\n",
      "        12081, 14202,  4321,   334,   893, 17540, 17786,   900,  8712, 18601,\n",
      "        17771, 13525, 14523, 15447,  1219, 11870, 16640,  4980, 11967, 16148,\n",
      "        11684,   863,  9844, 12489, 10220, 10945, 15103, 12269,  5586, 12190,\n",
      "        13077, 14523, 10113, 18084, 14351,  2207,  5028, 11967,  3907, 11870,\n",
      "         9507, 17174,  9870,  5651,  3018, 13077,  8954,  1346, 14523,   334,\n",
      "        12269,  4148,  4512, 14523, 10395,  4145,   792, 10945,  9986,  6381,\n",
      "        15705,  4321, 18267, 15293,  8401, 11870,  9347,  3781, 15725, 10113,\n",
      "        14677,  2158,  3781, 11967,  1848,  5927,  5231, 13605,  8401,  3172,\n",
      "        10147,  9036,   619,  3226, 16661,  4749, 10945,  9240, 16131,  6381,\n",
      "        15447, 10945, 12354,  4052, 18300,  4512,  2368,   334, 15023, 15293,\n",
      "          334,  1442,  2641,  8954,  1174,  5195, 11300, 13334, 16848, 17786,\n",
      "         3572, 10696,  5385,   334, 13261,  9347,   334,  9245,  4382,  8140,\n",
      "         1442,  4512,  8349, 13821, 17214,  8786, 17296,  5651, 16848, 18300,\n",
      "         4512, 13077, 11870, 10395, 18471, 14703,  8572, 11967, 17361, 11684,\n",
      "         1207, 14523, 16640,  3206, 17936, 10945, 11937, 18581, 11309,   334,\n",
      "         2041, 18300,  4512, 18300,  4512, 13077,  3467, 17347,  8401, 13670,\n",
      "        11967,  6285, 11684, 11870,  5661, 11684, 17347,  8401,   334, 18464,\n",
      "        10112,  5651, 14523, 10113,  3639, 16112,   271, 17114, 11967, 11796,\n",
      "          271, 18601, 10945, 16131,  5651, 18396, 17214,  1207, 11967, 13992,\n",
      "        11870, 10395,  5195, 18601, 10945,  5221,  1442, 14523,  2641,  5195,\n",
      "        11967,  2424, 11798, 12708, 18601,  1872,  6660,   334, 17828,  4321,\n",
      "         4100,  6381, 11967,  3639, 17278, 13744,  5690,  2667,   119]),\n",
      " tensor([ 7885,  7490,  8401,   573, 16095,  3391,  7885,  2585,  9545,  8159,\n",
      "        16640, 17725,  1233,  5465,  1473, 14857,  7110, 11221, 11967, 12123,\n",
      "         8401, 12303,  5651,   334,  3467,   573, 10613,  5795, 13254,   334,\n",
      "        17693,  2942, 10395, 14471, 10192, 16318, 18083, 17214,  5795,  7985,\n",
      "         9186, 16640,  3467,  7110, 11221, 10613,  1915,   939,   924, 11967,\n",
      "          484,  8401,   334,  6020, 14523, 10395,  5465,  4986,  1967,  1915,\n",
      "         2236, 16923, 14537,  4321,  5984, 17214,  1003,  3309,  8401,   334,\n",
      "          573,  7499,   939,  4512,  6124,  4321, 18558,  5984,  2236,  2696,\n",
      "        16822,  8458, 13845,  9218,  3913,  5795, 15076,  7885,    68,  6096,\n",
      "        13177,  5795,   334, 16923,  9462,   334, 15756,  9592,  8113,   334,\n",
      "         5246,  7110, 11221, 14863, 10131,   334, 14863, 14641,  1280,  5795,\n",
      "         7985,  9186,   939,  2641, 16709, 11967, 15375,  6919, 16923,  7499,\n",
      "        11684, 10945,   484,  5795, 14756, 13621,  6381,  2316,  9462,   334,\n",
      "        16406,  4321,  8159])]\n",
      "\n",
      "y batch: \n",
      "[tensor(1), tensor(4)]\n",
      "\n",
      "List of tensor lengths in a batch: \n",
      "[509, 133]\n",
      "\n",
      "Padded X_batch: \n",
      " tensor([[14907,  9218, 15699,  ...,  5690,  2667,   119],\n",
      "        [ 7885,  7490,  8401,  ...,     0,     0,     0]]) \n",
      "\n",
      "\n",
      "Embeddings: tensor([[[ 0.7562,  0.9117,  0.0760,  0.5329,  1.4589],\n",
      "         [ 0.4810,  0.4049,  1.3887, -0.0586,  0.1128],\n",
      "         [-1.2064, -1.1030,  0.4383, -0.6569,  1.3019],\n",
      "         ...,\n",
      "         [-1.0337, -0.6993, -0.5577, -0.0487,  0.4066],\n",
      "         [ 0.3419, -1.2997,  0.7457, -1.7634,  0.1463],\n",
      "         [ 0.9247,  0.2938,  3.0301,  0.4879, -0.3087]],\n",
      "\n",
      "        [[-0.9314,  0.2102,  0.6677, -1.7740,  0.4361],\n",
      "         [-0.5177,  1.2333, -1.0623,  1.4994, -0.8037],\n",
      "         [ 0.8078,  0.1862, -0.4698,  1.5391, -1.2358],\n",
      "         ...,\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798],\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798],\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798]]]) torch.Size([2, 509, 5]) \n",
      "\n",
      "\n",
      "RNN hidden last layer:\n",
      " tensor([[[ 0.1999, -0.5095, -0.4384],\n",
      "         [-0.5698,  0.7290,  0.4800]],\n",
      "\n",
      "        [[ 0.6099, -0.2046,  0.1753],\n",
      "         [ 0.8026, -0.2757, -0.1242]],\n",
      "\n",
      "        [[-0.1143, -0.0523,  0.1444],\n",
      "         [-0.1824,  0.0305,  0.2206]],\n",
      "\n",
      "        [[ 0.2862, -0.5049,  0.1637],\n",
      "         [ 0.2651, -0.4649,  0.1571]]])\n",
      "\n",
      "Linear Output:\n",
      " tensor([[[ 0.7190, -0.1234,  0.0016, -0.5510, -0.1578],\n",
      "         [ 0.0810,  0.3396, -0.3772, -0.4098,  0.6670]],\n",
      "\n",
      "        [[ 0.5668, -0.1036, -0.4976, -0.5910, -0.1347],\n",
      "         [ 0.6352, -0.1342, -0.4868, -0.5924, -0.3099]],\n",
      "\n",
      "        [[ 0.4597,  0.0374, -0.1867, -0.5146,  0.2196],\n",
      "         [ 0.4146,  0.0695, -0.2074, -0.5050,  0.2860]],\n",
      "\n",
      "        [[ 0.6690, -0.1662, -0.2018, -0.6016, -0.0501],\n",
      "         [ 0.6519, -0.1497, -0.2064, -0.5946, -0.0340]]])\n",
      "\n",
      "LogSoftmax:\n",
      " tensor([[-0.6846, -0.7015, -0.6908, -0.6966, -0.7013],\n",
      "        [-0.7018, -0.6849, -0.6955, -0.6897, -0.6851]])\n",
      "\n",
      "Y Output Tag: \n",
      " tensor([0, 1])\n",
      "\n",
      "Actual Output: \n",
      "[tensor(1), tensor(4)]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in sample_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i) for i in x_batch]\n",
    "        y_batch = [torch.tensor(i) for i in y_batch]\n",
    "        \n",
    "        \n",
    "        print(\"X batch: \")\n",
    "        pprint(x_batch)\n",
    "        print(\"\\ny batch: \")\n",
    "        pprint(y_batch)\n",
    "        \n",
    "        y_out = gru_model_sample(x_batch)\n",
    "                        \n",
    "        _, y_out_tag = torch.max(y_out, dim = 1)\n",
    "        print(\"\\nY Output Tag: \\n\", y_out_tag)\n",
    "        \n",
    "        print(\"\\nActual Output: \")\n",
    "        print(y_batch)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 300\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "STACKED_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestData(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size, stacked_layers):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_size, hidden_size = hidden_size, batch_first = True, num_layers = stacked_layers, dropout = 0.2)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        len_list = list(map(len, x_batch))\n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)\n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        pack_embeds = pack_padded_sequence(embeds, lengths=len_list, batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, (rnn_h, _) = self.lstm(pack_embeds)\n",
    "        linear_out = self.linear(self.tanh(rnn_h))\n",
    "        y_out = linear_out[-1]\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelLSTM(\n",
      "  (word_embeddings): Embedding(18636, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (linear): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model = ModelLSTM(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx), stacked_layers=STACKED_LAYERS)\n",
    "\n",
    "lstm_model.to(device)\n",
    "print(lstm_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer =  optim.Adam(lstm_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.59706 | Acc: 0.0\n",
      "Epoch 002: | Loss: 1.44596 | Acc: 100.0\n",
      "Epoch 003: | Loss: 0.90228 | Acc: 100.0\n",
      "Epoch 004: | Loss: 0.42660 | Acc: 100.0\n",
      "Epoch 005: | Loss: 0.23653 | Acc: 100.0\n",
      "Epoch 006: | Loss: 0.13660 | Acc: 100.0\n",
      "Epoch 007: | Loss: 0.05966 | Acc: 100.0\n",
      "Epoch 008: | Loss: 0.02126 | Acc: 100.0\n",
      "Epoch 009: | Loss: 0.01265 | Acc: 100.0\n",
      "Epoch 010: | Loss: 0.00860 | Acc: 100.0\n",
      "Epoch 011: | Loss: 0.00571 | Acc: 100.0\n",
      "Epoch 012: | Loss: 0.00461 | Acc: 100.0\n",
      "Epoch 013: | Loss: 0.00408 | Acc: 100.0\n",
      "Epoch 014: | Loss: 0.00340 | Acc: 100.0\n",
      "Epoch 015: | Loss: 0.00294 | Acc: 100.0\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = lstm_model(x_batch)        \n",
    "        \n",
    "        loss = criterion(y_pred.squeeze(0), y_batch)\n",
    "        acc = multi_acc(y_pred.squeeze(0), y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_tags_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "        \n",
    "        y_pred = lstm_model(x_batch)\n",
    "        _, y_pred_tag = torch.max(y_pred, dim = 1)\n",
    "\n",
    "        y_out_tags_list.append(y_pred_tag.squeeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 78  10   9  16   2]\n",
      " [ 14  94   3  10   4]\n",
      " [ 11   7  80   7  16]\n",
      " [ 15  10   3 120   2]\n",
      " [  3   1   7   1 145]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_out_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_tags_list = [a.squeeze().tolist() for a in y_out_tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       115\n",
      "           1       0.77      0.75      0.76       125\n",
      "           2       0.78      0.66      0.72       121\n",
      "           3       0.78      0.80      0.79       150\n",
      "           4       0.86      0.92      0.89       157\n",
      "\n",
      "    accuracy                           0.77       668\n",
      "   macro avg       0.77      0.76      0.76       668\n",
      "weighted avg       0.77      0.77      0.77       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_out_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                                                                            : Class          \n",
      "\n",
      "mg rover china tie up delayed mg rover s proposed tie up with china s top carmaker has been delayed due to concerns by chinese regulators according to the financial times the paper said chinese officials had been irritated by rover s disclosure of its talks with shanghai automotive industry corp in october the proposed deal was seen as crucial to safeguarding the future of rover s longbridge plant in the west midlands however there are growing fears that the deal could result in job losses the observer reported on sunday that nearly half the workforce at longbridge could be under threat if the deal goes ahead shanghai automotive s proposed bn investment in rover is awaiting approval by its owner the shanghai city government and by the national development and reform commission which oversees foreign investment by chinese firms according to the ft the regulator has been annoyed by rover s decision to talk publicly about the deal and the intense speculation which has about what it will mean for rover s future as a result hopes that approval of the deal may be fast tracked have disappeared the paper said there has been continued speculation about the viability of rover s longbridge plant because of falling sales and models according to the observer jobs out of a total workforce of could be lost if the deal goes ahead the paper said that chinese officials believe cutbacks will be required to keep the mg rover s costs in line with revenues it also said that the production of new models through the joint venture would take at least eighteen months neither rover nor shanghai automotive commented on the reports:     3\n",
      "\n",
      "davenport hits out at wimbledon world number one lindsay davenport has criticised wimbledon over the issue of equal prize money for women reacting to a disputed comment by all england club chairman tim phillips the american said i think it is highly insulting if prize money is taken away somebody i think it was mr phillips said they won t have money for flowers at wimbledon that s insulting an all england club spokesperson denied phillips made the remark insisting he definitely didn t say it the statement added it was said by someone else and was a humorous aside at the end of a radio interview when the conversation had moved to talking about the wimbledon grounds davenport was speaking following the announcement that this week s dubai duty free event will join the us and australian opens in offering equal prize money for women you hear about women playing only three sets while men play five said daveport and the best women are never going to beat the best men but it s a different game you go to watch with the women it doesn t make it better or worse hopefully we will be able to change people s minds serena williams who is also in dubai added i m obviously for equal prize money women s tennis is exciting men s tennis is exciting as well but the women have it right now if you are bringing in the spectators you should be able to reap what everyone else is able to reap:     4\n",
      "\n",
      "controversial film tops festival a controversial film starring hollywood actor kevin bacon as a convicted won top honours at the london film festival on thursday the woodsman won the ray award named after the indian director the low budget film directed by nicole is about a convicted child trying to rebuild his life after years in jail judges said the film tackled the contentious subject with great insight and sensitivity previous films to take the prize include the oscar winning boys don t cry which was about the true life story of murdered british writer director amma asante won the uk film talent award this year for her debut feature a way of life set in south wales the film is about a teenage single mother who becomes embroiled in a tense stand off with a turkish neighbour also on thursday night the international critics awards went to a belgian film about the and the sutherland trophy which was won by jonathan for his film the festival closed with a screening of the film i heart huckabees starring jude law and dustin hoffman and directed by three kings film maker david o russell the festival this year also included the first european screening of the new animation the incredibles and the british film bullet boy starring so solid crew rapper asher d:     2\n",
      "\n",
      "off colour gardener storms to win britain s jason gardener shook off an upset stomach to win the m at sunday s leipzig international meeting gardener clocked seconds to equal the meeting record and finished well ahead of germany s marc blume who crossed the line in secs the world indoor champion said i got to the airport and my stomach was upset and i was i almost went home i felt a little better sunday morning but decided i d only run in the main race then everything went perfectly gardener part of the great britain x m quartet that won gold at the athens olympics will now turn his attention to next weekend s norwich union european indoor trials in sheffield given i am still off colour i know there is plenty more in the tank and i expect to get faster in the next few weeks he said it s just a case of chipping away as i have done in previous years and the results will come scotland s ian mackie was also in action in leipzig he stepped down from his favoured m to m to finish third in secs germany s alexander won the race in secs with dutchman patrick van second in secs there were plenty of other senior british athletes showing their indoor form over the weekend promising m hurdler clocked a new uk record of seconds at a meeting in norway the year old reached the mark in her heat but had to settle for joint first place with former aaa champion diane allahgreen in the final who broke onto the international scene at the olympic games last season set an indoor personal best of m in the triple jump at a meeting in that leap cm short of brazilian winner s effort was good enough to qualify for the european indoor championships at the same meeting finished third in seconds in a high class women s m the event was won by european medal favourite christine of france while belgium rival kim gevaert was second britain s joice maduaka finished fifth in olympic bronze heptathlon medallist made a low key return to action at an indoor meeting in birmingham the year old cleared m to win the high jump and threw m in the women s shot put:     4\n",
      "\n",
      "williams battles to aussie title serena williams staged a remarkable recovery to beat lindsay davenport and win her second australian open title the champion claimed her seventh grand slam title and her first since wimbledon in williams had looked close to quitting with a rib injury when she left the court for treatment after five games she quickly dropped the first set but from in the second found another gear as davenport s game collapsed allowing williams to take the title williams later explained her injury problem saying it was the result of for a ball early in the first set lindsay had me on the run out wide on my backhand and my back went but eventually i was able to come back thank goodness she said davenport paid tribute to williams saying she s had a tough couple of years and she s come back like a champion the year old former world number one was in desperate trouble in the early stages as the injury hampered her service action davenport took advantage to race through the opening set and held six break points in game five of the second i kept thinking to myself i m not losing this game said williams afterwards i don t care if my arm falls off i m not losing this game and i guess that s what happened i didn t want to lose that particular game because it would have given her a lot of momentum and a lot of confidence despite letting the break points slip away davenport looked comfortable enough at but from up she just williams won nine consecutive points for the set and powered through the decider as a lacklustre davenport looked to have run out of energy after a gruelling two weeks i felt like i was playing well and in control pretty much of the match said davenport then i just had that horrible i think serving up and made a few errors and opened up the door for her and she just kept going through it at the end i think i was a little bit but she took advantage of it and kept going she s a great frontrunner when she gets going:     4\n",
      "\n",
      "m indecency fine for viacom media giant viacom has paid out m m to end investigations into indecency in its us radio and tv shows the settlement to the federal communications commission fcc ends a long running saga dating back to the fcc was looking into shows including those by shock howard stern and two new york djs stern recently announced he was leaving viacom while the two djs were sacked after their show featured a couple purporting to have sex in a church after the church incident two years ago viacom agreed to install audio delay equipment at its radio stations that broadcast live programming it also agreed to train its broadcasters and employees about indecency laws the agreement investigations into about radio and television shows said richard diamond fcc deputy secretary of communications the shows were broadcast by viacom owned stations across the united states viacom has five days to pay the m fine according to the agreement the payment is not related to the fcc s fine levied against viacom after the exposure of singer janet jackson s breast during the cbs super bowl show in january viacom is contesting that fine it is not the first time that viacom has paid out over indecency charges broadcasting which is owned by viacom paid cumulative fines totalling m in to settle fcc violations by stern:     3\n",
      "\n",
      "tobacco giants hail court ruling us tobacco companies have welcomed an appeal court s decision to reject the government s bn bn claim for alleged deceit about smoking dangers tobacco stocks rose sharply on wall street after the decision the court in washington found the case filed by the clinton administration in could not be brought under federal anti racketeering laws anti smoking groups urge the government to fight on but the justice department has not said if it will appeal among the accused were altria group rj reynolds tobacco lorillard tobacco liggett group and brown and williamson they were delighted by the decision which sent reynolds shares up and altria shares up charles a executive vice president of rj reynolds tobacco said the ruling dramatically the government s lawsuit altria group said in a statement the government now must not only prove that the companies have engaged in fraudulent behaviour in the past but that they are likely to do so in the future the government had claimed tobacco firms manipulated nicotine levels to increase addiction targeted teenagers with multi billion dollar advertising campaigns lied about the dangers of smoking and ignored research to the contrary prosecutors wanted the cigarette firms to bn in profits accumulated over the past years and impose tougher rules on marketing their products they brought the case under racketeering laws which were passed to deny mafia gangs the profits of their crimes but the tobacco companies denied that they illegally conspired to promote smoking and defraud the public they also said they had already met many of the government s demands in a landmark bn settlement reached with states in the three judge panel in the district of columbia s court of appeals ruled on friday that the us government could not sue the firms under the anti racketeering laws judge david in his ruling said such laws were aimed at putting an end to illegal conduct going forward we hold that the language of the law and the comprehensive remedial scheme of the law as a possible remedy in this case he wrote the justice department refused to say if it would appeal all we re saying today is that we have received the ruling and are reviewing it a spokeswoman said on friday but william of the campaign for tobacco free kids urged the government to continue pressing its case today s ruling should not be an excuse for this administration to seek a weak settlement that lets the tobacco industry off the hook he said:     0\n",
      "\n",
      "half life sweeps bafta awards pc first person shooter half life has won six bafta awards including best game and best online game the title developed by valve was released last year to universal acclaim receiving special praise for its immersive plot and physics engine the game also won baftas for best action adventure best pc game art direction and animation burnout won three awards in the categories for racing technical direction and best playstation game grant dean chairman of the bafta games awards said at a ceremony in london on tuesday the last year has been a great year for the interactive entertainment industry these awards reflect the enormous achievements progress and diversity that we have seen in that time halo won the best xbox game category while prince of persia warrior within was adjudged the best gamecube title the sports award went to konami s pro evolution soccer bafta said the significant feature of this year s awards was the number of non traditional games the originality award was won by playstation title while the children s award went to gamecube rhythm game donkey the handheld award went to colin mcrae rally while the mobile category was won by blue tooth the audio award was won by call of duty finest hour and contracts won the music award:     3\n",
      "\n",
      "dvd review i robot only one man recognises that robots are a threat to humanity but that s fine because it only takes one man to save the day in the thriller i robot will smith co stars alongside more cgi robots than you can count and as a thrill a minute kind of action film it s perfectly adequate you ll have forgotten it all tomorrow but you ll have a fun night with the film and all the extras there is a one disc version that has commentaries and a making of but the two disc adds more unusually for this kind of film the extras don t solely concentrate on the special effects they re covered but there s also a general production diary and a post production feature remember the national lottery s draw machines arthur and they were more accurate than this glossy hollywood version of the tale but as long as you re not expecting a documentary live with it king arthur is a fun exciting totally shallow experience and looks excellent clive owen is the brooding king keira rises above her costume and ray gives it all some grit it s at its best in its battle scenes which are well done and are also the best part of the making of extra less flashy than a david attenborough show and less detailed than a simon one the bbc series british isles nevertheless turned out to be quite engrossing admit it the fact that this is one of the shows alan titchmarsh left ground force to present did mean that you expected something equally frothy but titchmarsh turns out to know his subject and the sight of our present day landscape being back to reveal the past was fascinating he s now written an accompanying book too:     0\n",
      "\n",
      "super union merger plan touted two of britain s big trade unions could merge to form a super union of two million members the move by amicus and the transport and general workers union tgwu would be a seen as a bid to carry more weight with ministers and employers amicus has million members and the tgwu has any merger would have to be approved by the unions executives and their membership it is understood meetings will be held on wednesday about the proposal along with the gmb and unison the tgwu and amicus worked closely together in the last year to hammer out a point deal with labour s leadership over equality at work holidays and pensions the warwick agreement both unions are remaining tight lipped about the merger rumours but one insider pointed out to the bbc news website that nobody is denying suggestions a merger could be on the agenda when the two unions executives hold their meetings on wednesday amicus s executive was due to meet in any case although the tgwu is holding specially scheduled talks:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{:80}: {:15}\\n'.format(\"Word\", \"Class\"))\n",
    "for sentence, tag in zip(X_test[:10], y_out_tags_list[:10]):\n",
    "    s = \" \".join([idx2word[w] for w in sentence])\n",
    "    print('{:80}: {:5}\\n'.format(s, tag))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
