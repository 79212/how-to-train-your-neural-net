{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Multi-Class Text Classification using CNN+RNN - Sample\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)  \n",
    "\n",
    "This notebook takes you through a sample implementation of multi-class text classification in the form of sentiment analysis on yelp reviews using CNN+RNN in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1648b02f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"Ronaldo plays football a lot.\".split(), \"football\"),\n",
    "    (\"Cho likes quidditch very much.\".split(), \"quidditch\"),\n",
    "    (\"Jordan adores basketball a lot.\".split(), \"basketball\"),\n",
    "    (\"McTominay plays football very well.\".split(), \"football\"),\n",
    "    (\"Woods likes quidditch a lot.\".split(), \"quidditch\"),\n",
    "    (\"Kobe adores basketball very much.\".split(), \"basketball\"),\n",
    "    (\"Scholes likes quidditch a lot.\".split(), \"football\"),\n",
    "    (\"Ginny adores quidditch very much.\".split(), \"quidditch\")\n",
    "\n",
    "]\n",
    "\n",
    "sentence_list = [training_data[x][0] for x in range(len(training_data))]\n",
    "tag_list = [training_data[x][1] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ronaldo', 'plays', 'football', 'a', 'lot.'],\n",
       " ['Cho', 'likes', 'quidditch', 'very', 'much.'],\n",
       " ['Jordan', 'adores', 'basketball', 'a', 'lot.'],\n",
       " ['McTominay', 'plays', 'football', 'very', 'well.'],\n",
       " ['Woods', 'likes', 'quidditch', 'a', 'lot.'],\n",
       " ['Kobe', 'adores', 'basketball', 'very', 'much.'],\n",
       " ['Scholes', 'likes', 'quidditch', 'a', 'lot.'],\n",
       " ['Ginny', 'adores', 'quidditch', 'very', 'much.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football',\n",
       " 'quidditch',\n",
       " 'basketball',\n",
       " 'football',\n",
       " 'quidditch',\n",
       " 'basketball',\n",
       " 'football',\n",
       " 'quidditch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data by converting it into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_list = []\n",
    "for sentence, tags in training_data:\n",
    "    clean_sentence = [x.lower().split('.')[0] for x in sentence]\n",
    "    data_clean_list += [(clean_sentence, tags)]\n",
    "\n",
    "    \n",
    "sentence_clean_list = [data_clean_list[x][0] for x in range(len(data_clean_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ronaldo', 'plays', 'football', 'a', 'lot'],\n",
       " ['cho', 'likes', 'quidditch', 'very', 'much'],\n",
       " ['jordan', 'adores', 'basketball', 'a', 'lot'],\n",
       " ['mctominay', 'plays', 'football', 'very', 'well'],\n",
       " ['woods', 'likes', 'quidditch', 'a', 'lot'],\n",
       " ['kobe', 'adores', 'basketball', 'very', 'much'],\n",
       " ['scholes', 'likes', 'quidditch', 'a', 'lot'],\n",
       " ['ginny', 'adores', 'quidditch', 'very', 'much']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 19\n",
      "\n",
      "['woods', 'football', 'mctominay', 'ginny', 'quidditch', 'ronaldo', 'much', 'plays', 'cho', 'basketball', 'jordan', 'adores', 'lot', 'well', 'scholes', 'very', 'kobe', 'likes', 'a']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_clean_list:\n",
    "    words += sentence\n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'woods': 0, 'football': 1, 'mctominay': 2, 'ginny': 3, 'quidditch': 4, 'ronaldo': 5, 'much': 6, 'plays': 7, 'cho': 8, 'basketball': 9, 'jordan': 10, 'adores': 11, 'lot': 12, 'well': 13, 'scholes': 14, 'very': 15, 'kobe': 16, 'likes': 17, 'a': 18}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 3\n",
      "\n",
      "['quidditch', 'football', 'basketball']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags.append(tag)\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quidditch': 0, 'football': 1, 'basketball': 2}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ronaldo', 'plays', 'football', 'a', 'lot'],\n",
       "  ['cho', 'likes', 'quidditch', 'very', 'much'],\n",
       "  ['jordan', 'adores', 'basketball', 'a', 'lot'],\n",
       "  ['mctominay', 'plays', 'football', 'very', 'well'],\n",
       "  ['woods', 'likes', 'quidditch', 'a', 'lot'],\n",
       "  ['kobe', 'adores', 'basketball', 'very', 'much'],\n",
       "  ['scholes', 'likes', 'quidditch', 'a', 'lot'],\n",
       "  ['ginny', 'adores', 'quidditch', 'very', 'much']],\n",
       " ['football',\n",
       "  'quidditch',\n",
       "  'basketball',\n",
       "  'football',\n",
       "  'quidditch',\n",
       "  'basketball',\n",
       "  'football',\n",
       "  'quidditch'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 7, 1, 18, 12],\n",
       " [8, 17, 4, 15, 6],\n",
       " [10, 11, 9, 18, 12],\n",
       " [2, 7, 1, 15, 13],\n",
       " [0, 17, 4, 18, 12],\n",
       " [16, 11, 9, 15, 6],\n",
       " [14, 17, 4, 18, 12],\n",
       " [3, 11, 4, 15, 6]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[w] for w in s] for s in sentence_clean_list]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 1, 0, 2, 1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [tag2idx[t] for t in tag_list]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Params and Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input -> RNN -> Linear -> Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 6\n",
    "HIDDEN_SIZE = 7\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 10\n",
    "STACKED_LAYERS = 5\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(torch.Tensor(X).to(torch.long), torch.Tensor(y).to(torch.float32))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 5,  7,  1, 18, 12],\n",
      "        [ 8, 17,  4, 15,  6],\n",
      "        [10, 11,  9, 18, 12],\n",
      "        [ 2,  7,  1, 15, 13]]), tensor([1., 0., 2., 1.]))\n",
      "(tensor([[ 0, 17,  4, 18, 12],\n",
      "        [16, 11,  9, 15,  6],\n",
      "        [14, 17,  4, 18, 12],\n",
      "        [ 3, 11,  4, 15,  6]]), tensor([0., 2., 1., 0.]))\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_loader:\n",
    "    print((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN MODEL OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnRnnModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(CnnRnnModel, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.cnn = nn.Conv1d(in_channels=embedding_size, out_channels=2, kernel_size=3, stride=1)\n",
    "        self.gru = nn.GRU(input_size = 2, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        print(\"Embeds: \", embeds.size())\n",
    "        embeds_t = embeds.transpose(1, 2)\n",
    "        print(\"Embeds_t: \", embeds_t.size())\n",
    "        cnn = torch.relu(self.cnn(embeds_t))\n",
    "        print(\"CNN: \", cnn.size())\n",
    "        \n",
    "        gru_input = cnn.transpose(1, 2)\n",
    "        print(\"GRU Input: \", gru_input.size())\n",
    "        gru_out, gru_hidden = self.gru(gru_input)\n",
    "        print(\"GRU Out: \", gru_out.size())\n",
    "        print(gru_out)\n",
    "        print(\"GRU Hidden: \", gru_hidden.size())\n",
    "        print(gru_hidden)\n",
    "        linear_input = gru_hidden.squeeze()\n",
    "        print(\"Linear Input: \", linear_input.size())\n",
    "        print(linear_input)\n",
    "        linear = self.linear(linear_input)\n",
    "        \n",
    "        return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnRnnModel(\n",
      "  (word_embeddings): Embedding(19, 6)\n",
      "  (cnn): Conv1d(6, 2, kernel_size=(3,), stride=(1,))\n",
      "  (gru): GRU(2, 7, batch_first=True)\n",
      "  (linear): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model = CnnRnnModel(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size = HIDDEN_SIZE, target_size=len(tag2idx))\n",
    "print(cnn_rnn_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  optim.Adam(cnn_rnn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the CNN+RNN output from the model looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([4, 5])\n",
      "tensor([[ 5,  7,  1, 18, 12],\n",
      "        [ 8, 17,  4, 15,  6],\n",
      "        [10, 11,  9, 18, 12],\n",
      "        [ 2,  7,  1, 15, 13]]) \n",
      "\n",
      "Embeds:  torch.Size([4, 5, 6])\n",
      "Embeds_t:  torch.Size([4, 6, 5])\n",
      "CNN:  torch.Size([4, 2, 3])\n",
      "GRU Input:  torch.Size([4, 3, 2])\n",
      "GRU Out:  torch.Size([4, 3, 7])\n",
      "tensor([[[-0.0966, -0.0887,  0.0539, -0.0422,  0.2357,  0.2448,  0.0331],\n",
      "         [ 0.0904,  0.0666,  0.1903, -0.0488,  0.3392,  0.2914,  0.0434],\n",
      "         [-0.0473, -0.0096,  0.1078, -0.1012,  0.3943,  0.3564,  0.0842]],\n",
      "\n",
      "        [[ 0.0319,  0.0872,  0.1105, -0.0453,  0.2328,  0.1404, -0.0304],\n",
      "         [ 0.0401,  0.1305,  0.1386, -0.0795,  0.3389,  0.2078, -0.0176],\n",
      "         [ 0.0417,  0.1515,  0.1414, -0.1021,  0.3893,  0.2417, -0.0011]],\n",
      "\n",
      "        [[ 0.0319,  0.0872,  0.1105, -0.0453,  0.2328,  0.1404, -0.0304],\n",
      "         [ 0.0401,  0.1305,  0.1386, -0.0795,  0.3389,  0.2078, -0.0176],\n",
      "         [-0.0426,  0.0456,  0.0936, -0.0993,  0.3918,  0.2981,  0.0338]],\n",
      "\n",
      "        [[-0.0858, -0.0857,  0.0678, -0.0329,  0.2357,  0.2444,  0.0356],\n",
      "         [-0.0380,  0.0137,  0.1103, -0.1015,  0.3453,  0.2931,  0.0385],\n",
      "         [ 0.0034,  0.0860,  0.1285, -0.1337,  0.3972,  0.3026,  0.0362]]])\n",
      "GRU Hidden:  torch.Size([1, 4, 7])\n",
      "tensor([[[-0.0473, -0.0096,  0.1078, -0.1012,  0.3943,  0.3564,  0.0842],\n",
      "         [ 0.0417,  0.1515,  0.1414, -0.1021,  0.3893,  0.2417, -0.0011],\n",
      "         [-0.0426,  0.0456,  0.0936, -0.0993,  0.3918,  0.2981,  0.0338],\n",
      "         [ 0.0034,  0.0860,  0.1285, -0.1337,  0.3972,  0.3026,  0.0362]]])\n",
      "Linear Input:  torch.Size([4, 7])\n",
      "tensor([[-0.0473, -0.0096,  0.1078, -0.1012,  0.3943,  0.3564,  0.0842],\n",
      "        [ 0.0417,  0.1515,  0.1414, -0.1021,  0.3893,  0.2417, -0.0011],\n",
      "        [-0.0426,  0.0456,  0.0936, -0.0993,  0.3918,  0.2981,  0.0338],\n",
      "        [ 0.0034,  0.0860,  0.1285, -0.1337,  0.3972,  0.3026,  0.0362]])\n",
      "\n",
      "Linear Output:  torch.Size([4, 3])\n",
      "tensor([[-0.1922,  0.2649, -0.0614],\n",
      "        [-0.1312,  0.2887, -0.0773],\n",
      "        [-0.1609,  0.2701, -0.0813],\n",
      "        [-0.1597,  0.2704, -0.0681]])\n",
      "\n",
      "LogSoftmax Output:  torch.Size([4, 3])\n",
      "tensor([[-1.3135, -0.8564, -1.1828],\n",
      "        [-1.2746, -0.8546, -1.2207],\n",
      "        [-1.2869, -0.8559, -1.2074],\n",
      "        [-1.2902, -0.8601, -1.1985]])\n",
      "\n",
      "Output Indices:  torch.Size([4])\n",
      "tensor([1, 1, 1, 1])\n",
      "\n",
      "Actual Output:  torch.Size([4])\n",
      "tensor([1., 0., 2., 1.])\n",
      "==================================================\n",
      "Input:  torch.Size([4, 5])\n",
      "tensor([[ 0, 17,  4, 18, 12],\n",
      "        [16, 11,  9, 15,  6],\n",
      "        [14, 17,  4, 18, 12],\n",
      "        [ 3, 11,  4, 15,  6]]) \n",
      "\n",
      "Embeds:  torch.Size([4, 5, 6])\n",
      "Embeds_t:  torch.Size([4, 6, 5])\n",
      "CNN:  torch.Size([4, 2, 3])\n",
      "GRU Input:  torch.Size([4, 3, 2])\n",
      "GRU Out:  torch.Size([4, 3, 7])\n",
      "tensor([[[ 0.0319,  0.0872,  0.1105, -0.0453,  0.2328,  0.1404, -0.0304],\n",
      "         [ 0.0401,  0.1305,  0.1386, -0.0795,  0.3389,  0.2078, -0.0176],\n",
      "         [-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352]],\n",
      "\n",
      "        [[ 0.0319,  0.0872,  0.1105, -0.0453,  0.2328,  0.1404, -0.0304],\n",
      "         [-0.0692, -0.0164,  0.0771, -0.0762,  0.3419,  0.2891,  0.0303],\n",
      "         [-0.0056,  0.0687,  0.1133, -0.1223,  0.3948,  0.3016,  0.0331]],\n",
      "\n",
      "        [[ 0.0319,  0.0872,  0.1105, -0.0453,  0.2328,  0.1404, -0.0304],\n",
      "         [ 0.0401,  0.1305,  0.1386, -0.0795,  0.3389,  0.2078, -0.0176],\n",
      "         [-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352]],\n",
      "\n",
      "        [[ 0.0120,  0.0667,  0.1039, -0.0449,  0.2332,  0.1535, -0.0211],\n",
      "         [ 0.0188,  0.1065,  0.1302, -0.0826,  0.3400,  0.2245, -0.0061],\n",
      "         [ 0.0320,  0.1378,  0.1376, -0.1079,  0.3907,  0.2538,  0.0066]]])\n",
      "GRU Hidden:  torch.Size([1, 4, 7])\n",
      "tensor([[[-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352],\n",
      "         [-0.0056,  0.0687,  0.1133, -0.1223,  0.3948,  0.3016,  0.0331],\n",
      "         [-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352],\n",
      "         [ 0.0320,  0.1378,  0.1376, -0.1079,  0.3907,  0.2538,  0.0066]]])\n",
      "Linear Input:  torch.Size([4, 7])\n",
      "tensor([[-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352],\n",
      "        [-0.0056,  0.0687,  0.1133, -0.1223,  0.3948,  0.3016,  0.0331],\n",
      "        [-0.0458,  0.0405,  0.0913, -0.0991,  0.3919,  0.3007,  0.0352],\n",
      "        [ 0.0320,  0.1378,  0.1376, -0.1079,  0.3907,  0.2538,  0.0066]])\n",
      "\n",
      "Linear Output:  torch.Size([4, 3])\n",
      "tensor([[-0.1620,  0.2695, -0.0814],\n",
      "        [-0.1567,  0.2719, -0.0723],\n",
      "        [-0.1620,  0.2695, -0.0814],\n",
      "        [-0.1371,  0.2848, -0.0761]])\n",
      "\n",
      "LogSoftmax Output:  torch.Size([4, 3])\n",
      "tensor([[-1.2874, -0.8560, -1.2068],\n",
      "        [-1.2873, -0.8588, -1.2030],\n",
      "        [-1.2874, -0.8560, -1.2068],\n",
      "        [-1.2776, -0.8556, -1.2165]])\n",
      "\n",
      "Output Indices:  torch.Size([4])\n",
      "tensor([1, 1, 1, 1])\n",
      "\n",
      "Actual Output:  torch.Size([4])\n",
      "tensor([0., 2., 1., 0.])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        print(\"Input: \", x_batch.size())\n",
    "        print(x_batch, \"\\n\")\n",
    "        y_out = cnn_rnn_model(x_batch)\n",
    "        \n",
    "        y_out_softmax = torch.log_softmax(y_out, dim = 1)\n",
    "        _, y_out_tags = torch.max(y_out_softmax, dim = 1)\n",
    "        \n",
    "        print(\"\\nLinear Output: \", y_out.size())\n",
    "        print(y_out)\n",
    "        \n",
    "        print(\"\\nLogSoftmax Output: \", y_out_softmax.size())\n",
    "        print(y_out_softmax)\n",
    "        \n",
    "        print(\"\\nOutput Indices: \", y_out_tags.size())\n",
    "        print(y_out_tags)\n",
    "        \n",
    "        print(\"\\nActual Output: \", y_batch.size())\n",
    "        print(y_batch)\n",
    "        \n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnRnnModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(CnnRnnModel, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.cnn = nn.Conv1d(in_channels=embedding_size, out_channels=2, kernel_size=3, stride=1)\n",
    "        self.gru = nn.GRU(input_size = 2, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds_t = embeds.transpose(1, 2)\n",
    "        cnn = torch.relu(self.cnn(embeds_t))\n",
    "        \n",
    "        gru_input = cnn.transpose(1, 2)\n",
    "        gru_out, gru_hidden = self.gru(gru_input)\n",
    "        linear_input = gru_hidden.squeeze()\n",
    "        linear = self.linear(linear_input)\n",
    "        \n",
    "        return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnRnnModel(\n",
      "  (word_embeddings): Embedding(19, 6)\n",
      "  (cnn): Conv1d(6, 2, kernel_size=(3,), stride=(1,))\n",
      "  (gru): GRU(2, 7, batch_first=True)\n",
      "  (linear): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model = CnnRnnModel(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size = HIDDEN_SIZE, target_size=len(tag2idx))\n",
    "print(cnn_rnn_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  optim.Adam(cnn_rnn_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 1.13776 | Accuracy: 0.25\n",
      "Epoch: 02 | Loss: 1.13474 | Accuracy: 0.25\n",
      "Epoch: 03 | Loss: 1.13200 | Accuracy: 0.25\n",
      "Epoch: 04 | Loss: 1.12931 | Accuracy: 0.25\n",
      "Epoch: 05 | Loss: 1.12665 | Accuracy: 0.25\n",
      "Epoch: 06 | Loss: 1.12400 | Accuracy: 0.25\n",
      "Epoch: 07 | Loss: 1.12138 | Accuracy: 0.25\n",
      "Epoch: 08 | Loss: 1.11877 | Accuracy: 0.25\n",
      "Epoch: 09 | Loss: 1.11624 | Accuracy: 0.25\n",
      "Epoch: 10 | Loss: 1.11380 | Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model.train()\n",
    "for e in range(1, EPOCH+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_out = cnn_rnn_model(x_batch)\n",
    "                       \n",
    "        loss = criterion(y_out.squeeze(0), y_batch.long())\n",
    "        acc = multi_acc(y_out.squeeze(0), y_batch.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch: {e+0:02} | Loss: {epoch_loss/len(train_loader):.5f} | Accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
