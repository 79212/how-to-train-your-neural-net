{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Multi-Class Text Classification using CNN+RNN\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)  \n",
    "\n",
    "This notebook takes you through the implementation of binary text classification in the form of sentiment analysis on yelp reviews using CNN+RNN in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9d806f43d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../data/nlp/text_classification/bbc-text.csv\")\n",
    "df = df.rename(columns = {'category':'tag'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [t for t in df['text'].to_list()]\n",
    "tag_list = [t for t in df['tag'].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.',\n",
       " 'worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tech', 'business']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "sentence_list = [s.lower() for s in sentence_list]\n",
    "\n",
    "# Remove non alphavets\n",
    "regex_remove_nonalphabets = re.compile('[^a-zA-Z]')\n",
    "sentence_list = [regex_remove_nonalphabets.sub(' ', s) for s in sentence_list]\n",
    "\n",
    "# Remove words with less than 2 letters\n",
    "regex_remove_shortwords = re.compile(r'\\b\\w{1,2}\\b')\n",
    "sentence_list = [regex_remove_shortwords.sub(\"\", s) for s in sentence_list]\n",
    "\n",
    "# Remove words that appear only once\n",
    "c = Counter(w for s in sentence_list for w in s.split())\n",
    "sentence_list = [' '.join(y for y in x.split() if c[y] > 1) for x in sentence_list]\n",
    "\n",
    "# Strip extra whitespaces\n",
    "sentence_list = [\" \".join(s.split()) for s in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['future the hands viewers with home theatre systems plasma high definition tvs and digital video recorders moving into the living room the way people watch will radically different five years time that according expert panel which gathered the annual consumer electronics show las vegas discuss how these new technologies will impact one our favourite with the leading the trend programmes and other content will delivered viewers via home networks through cable satellite telecoms companies and broadband service providers front rooms and portable devices one the most talked about technologies ces has been digital and personal video recorders dvr and pvr these set top boxes like the tivo and the sky system allow people record store play pause and forward wind programmes when they want essentially the technology allows for much more personalised they are also being built high definition sets which are big business japan and the but slower take off europe because the lack high definition programming not only can people forward wind through adverts they can also forget about abiding network and channel schedules putting together their own carte entertainment but some networks and cable and satellite companies are worried about what means for them terms advertising revenues well brand identity and viewer loyalty channels although the leads this technology the moment also concern that being raised europe particularly with the growing uptake services like sky what happens here today will see nine months years time the adam the bbc broadcast futurologist told the bbc news website for the likes the bbc there are issues lost advertising revenue yet more pressing issue the moment for commercial broadcasters but brand loyalty important for everyone will talking more about content brands rather than network brands said tim hanlon from brand communications firm the reality that with broadband connections anybody can the producer content added the challenge now that hard promote programme with much choice what this means said senior vice president guide group that the way people find the content they want watch has simplified for viewers means that networks terms channels could take leaf out google book and the search engine the future instead the help people find what they want watch this kind channel model might work for the younger ipod generation which used taking control their gadgets and what they play them but might not suit everyone the panel recognised older generations are more comfortable with familiar schedules and channel brands because they know what they are getting they perhaps not want much the choice put into their hands hanlon suggested the other end you have the kids just out who are pushing buttons already everything possible and available them said hanlon ultimately the consumer will tell the market they want the new gadgets and technologies being showcased ces many them are about enhancing the watching experience high definition sets are everywhere and many new models lcd liquid crystal display tvs have been launched with dvr capability built into them instead being external boxes one such example launched the show inch lcd with hour tivo dvr and dvd recorder one the biggest satellite companies has even launched its own branded dvr the show with hours recording capability instant replay and search function the set can pause and rewind for hours and microsoft chief bill gates announced his pre show keynote speech partnership with tivo called which means people can play recorded programmes windows pcs and mobile devices all these reflect the increasing trend freeing multimedia that people can watch what they want when they want',\n",
       " 'worldcom boss left books alone former worldcom boss bernie ebbers who accused overseeing fraud never made accounting decisions witness has told jurors david myers made the comments under questioning defence lawyers who have been arguing that ebbers was not responsible for worldcom problems the phone company collapsed and prosecutors claim that losses were hidden protect the firm shares myers has already pleaded guilty fraud and assisting prosecutors monday defence lawyer reid weingarten tried distance his client from the allegations during cross examination asked myers ever knew ebbers make accounting decision not that aware myers replied did you ever know ebbers make accounting entry into worldcom books weingarten pressed replied the witness myers has admitted that ordered false accounting entries the request former worldcom chief financial officer scott sullivan defence lawyers have been trying paint sullivan who has admitted fraud and will testify later the trial the mastermind behind worldcom accounting house cards ebbers team meanwhile are looking portray him boss who his own admission more graduate than economist whatever his abilities ebbers transformed worldcom from relative unknown into telecoms giant and investor darling the late worldcom problems mounted however competition increased and the telecoms boom petered out when the firm finally collapsed shareholders lost about and workers lost their jobs ebbers trial expected last two months and found guilty the former ceo faces substantial jail sentence has firmly declared his innocence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 18430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_list:\n",
    "    for w in sentence.split():\n",
    "        words.append(w)\n",
    "    \n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab and dictionary for output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 5\n",
      "\n",
      "['tech', 'business', 'sport', 'entertainment', 'politics']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags.append(tag)\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tech': 0, 'business': 1, 'sport': 2, 'entertainment': 3, 'politics': 4}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the input and output to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[w] for w in s.split()] for s in sentence_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [tag2idx[t] for t in tag_list]\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  1557\n",
      "X_test size:  668\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train size: \", len(X_train))\n",
    "print(\"X_test size: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_SAMPLE = 2\n",
    "EMBEDDING_SIZE_SAMPLE = 5\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE_SAMPLE = 3\n",
    "STACKED_LAYERS_SAMPLE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = SampleData(X_train, y_train)\n",
    "sample_loader = DataLoader(sample_data, batch_size=BATCH_SIZE_SAMPLE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12508, 16942, 10655, 13370, 8064, 16942, 10655, 499, 4314, 7250, 7752, 5892, 7093, 12508, 9554, 2674, 10075, 499, 2965, 10734, 4237, 12615, 16565, 12508, 12, 17086, 6941, 16673, 16423, 17822, 99, 17261, 16565, 3915, 17261, 9707, 1484, 731, 4043, 11953, 4237, 3873, 11242, 16841, 10528, 4016, 1484, 1181, 460, 11642, 4237, 16565, 12508, 17914, 2372, 10655, 5192, 17261, 16565, 4237, 16846, 9610, 3873, 3734, 9515, 6381, 6941, 17261, 3873, 3915, 17914, 7233, 17261, 7726, 8748, 15295, 16366, 4059, 4237, 17086, 2372, 12813, 9003, 3120, 4237, 17261, 1177, 9707, 17897, 3915, 11528, 8795, 4237, 17261, 1177, 1749, 11242, 1484, 5460, 14984, 4237, 16565, 10655, 499, 2965, 4094, 4680, 17261, 16565, 12508, 9554, 15095, 8195, 3915, 6761, 10644, 3915, 499, 4094, 18297, 1484, 5786, 13043, 16862, 1749, 1576, 12183, 7233, 4237, 17086, 17261, 1749, 1484, 4237, 14016, 5333, 17465, 9876, 15451, 2674, 13008, 16273, 7424, 10185, 499, 7233, 1484, 3196, 16624, 4237, 1698, 4023, 16565, 7977, 15253, 4585, 8980, 6381, 6941, 17261, 15095, 3873, 1484, 5918, 13823, 16796, 13095, 9807, 1825, 1627, 84, 12532, 7233, 4237, 17261, 84, 5617, 602, 17325, 4667, 7233, 17261, 11923, 16565, 4237, 12615, 12508, 17191, 15009, 872, 3492, 1484, 4037, 5460, 7424, 3086, 3688, 3158], [12103, 3636, 13303, 2841, 163, 17191, 4237, 12103, 6978, 2536, 3282, 16207, 9195, 3469, 12283, 4237, 12103, 2501, 15295, 2472, 1833, 3636, 1796, 6381, 9716, 16732, 18276, 17131, 17891, 10374, 10454, 16732, 15718, 15095, 11806, 3915, 14475, 12200, 2828, 2418, 9933, 2559, 13986, 4237, 17261, 16624, 4237, 15267, 1484, 6381, 2844, 1487, 2095, 4237, 13079, 15925, 1310, 15628, 8889, 7952, 7759, 14355, 3196, 10166, 14475, 3915, 10185, 13374, 827, 386, 10185, 3688, 10790, 9626, 3636, 11953, 163, 6381, 14542, 13150, 6532, 16624, 6381, 827, 10185, 6381, 1287, 13340, 16923, 4237, 7158, 4037, 5649, 14720, 3852, 499, 4094, 16916, 17261, 6330, 1484, 3940, 4637, 2537, 3915, 9195, 10185, 6381, 9875, 546, 2844, 3915, 8161, 827, 10185, 11073, 4237, 1110, 14720, 4946, 4946, 4152, 3688, 6494, 2844, 1114, 4237, 7648, 4152, 12761, 4613, 2867, 6381, 10166, 14475, 3915, 14475, 2462, 7250, 13062, 16103, 435, 17082, 4237, 7648, 13079, 1982, 9195, 4237, 12433, 3688, 15952, 5030, 386, 15665, 11713, 15003, 4237, 15700, 9622, 11090, 745, 15295, 5918, 9063, 11953, 17914, 15295, 5918, 10446, 139, 14355, 435, 4237, 2730, 4237, 18251, 9483, 4314, 5827, 15295, 435, 11895, 4237, 5527, 3915, 6381, 6805, 9100, 10454, 3808, 8082, 14475, 1473, 14542, 14602, 3915, 499, 3940, 5634, 5262, 14887, 435, 11953, 10164, 15665, 14728, 499, 12285, 4237, 18276, 5533, 4237, 5534, 6381, 11090, 602, 2548, 1484, 15665, 16996, 14892, 827, 9716, 16624, 873, 11501, 15136, 11953, 9452, 7033, 3940, 1340, 3915, 14119, 827, 10164, 4237, 5634, 5262, 3688, 9275, 16466, 5730, 6035, 4237, 18061, 11543, 13220, 3582, 4314, 14542, 16643, 3915, 1257, 15666, 10185, 3981, 10185, 10734, 5030, 2418, 9933, 4237, 17325, 16996, 3807, 16996, 3915, 13079, 3915, 5652, 4945, 12314, 1484, 7250, 3631, 827, 13220, 3688, 5955, 6031, 6798, 4237, 4443, 4237, 12103, 2501, 11953, 10185, 499, 10550, 13104, 3636, 4237, 829, 163, 17904, 11114, 10185, 499, 12027, 17629, 17440, 9522, 16291, 6643, 4102, 3196, 15295, 16291, 7933, 11953, 2662, 11227, 6805, 16207, 11056, 7600, 9716, 17187, 12251, 11543, 10450, 4237, 3372, 12200, 6305, 16291, 16055, 6027, 3360, 17574, 17191, 7233, 11953, 11056, 5918, 17261, 2286, 435, 1128, 3915, 2754, 6381, 12400, 16209, 1484, 435, 14475, 13389, 3915, 4237, 13713, 2003, 8485, 11073, 5634, 5004, 8082, 14475, 4237, 1463, 4152, 9311, 6381, 4975, 111, 5955, 11609, 2031, 7000, 2124, 4237, 16774, 3118, 13351, 5785, 13471, 11953, 7000, 4314, 7067, 8082, 15295, 7398, 10115, 10185, 4314, 10308, 4237, 11463, 9195, 9311, 11334, 17191, 1912, 5785, 17084, 11553, 7480, 13449, 16207, 4237, 12723, 6670, 3196, 17393, 17995, 7093, 16846, 14995, 1246, 11953, 4237, 9629, 6305, 1225, 4237, 1170, 13837, 827, 10185, 16916, 17261, 12723, 15175, 17325, 4018, 3915, 827, 3394, 4237, 2361, 4237, 863, 499, 2820, 14531, 14718, 11553, 499, 4094, 8131, 3915, 14475, 6381, 7250, 7929, 5527, 9195, 827, 11334, 4102, 3196, 14636, 13713, 2107, 6616, 5875, 8082, 1484, 3118, 4237, 17432, 3781, 1113, 3170, 14805, 3915, 16429, 9595, 14805, 10581, 499, 11675, 17070, 14240, 3915, 12145, 11953, 499, 18177, 111, 1484, 8350, 3469, 7146, 3648, 1484, 14542, 10277, 9595, 10581, 10651, 7002, 4237, 16103, 4237, 1414, 11953, 18320, 6408, 16862, 17432, 10651, 10277, 3807, 5096, 11953, 7214, 5918, 2013, 17820, 8007, 7250, 15925, 3915, 18025, 5918, 15136, 13665, 17191, 16732, 163, 10576, 10450, 8350]] \n",
      "\n",
      " [3, 2] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tl = iter(sample_loader)\n",
    "\n",
    "i,j = map(list, zip(*next(tl)))\n",
    "\n",
    "print(i,\"\\n\\n\", j, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample CNN+RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCnnRnnSample(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(ModelCnnRnnSample, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_size, out_channels=100, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=10, kernel_size=3, stride=1)\n",
    "        self.gru = nn.GRU(input_size=10, hidden_size = hidden_size, batch_first=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_batch):        \n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)\n",
    "        print(\"\\nPadded X_batch: \", padded_batch.size(), \"\\n\")\n",
    "\n",
    "        \n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        print(\"\\nEmbeddings: \", embeds.size(), \"\\n\", embeds, \"\\n\")\n",
    "    \n",
    "        embeds_t = embeds.transpose(1, 2)\n",
    "        print(\"\\nEmbeddings transposed for CNN: \", embeds_t.size(), \"\\n\", embeds_t, \"\\n\")\n",
    "\n",
    "        cnn1 = torch.relu(self.conv1(embeds_t))\n",
    "        cnn2 = torch.relu(self.conv2(cnn1))\n",
    "        print(\"\\nCNN output: \", cnn2.size(), \"\\n\", cnn2)\n",
    "        \n",
    "        maxpool1 = self.maxpool(cnn2)\n",
    "        print(\"\\nMaxpool output: \", maxpool1.size(), \"\\n\", maxpool1)\n",
    "        \n",
    "        gru_input = maxpool1.transpose(1, 2)\n",
    "        print(\"\\nRNN Input: \", gru_input.size(), \"\\n\", gru_input)\n",
    "        \n",
    "        _, gru_hidden = self.gru(gru_input)\n",
    "        print(\"\\nRNN Last Hidden: \", gru_hidden.size(), \"\\n\", gru_hidden)       \n",
    "        \n",
    "#         linear_in, _ = torch.max(maxpool1, dim = 2)\n",
    "#         print(\"\\nLinear input: \", linear_in.size(), \"\\n\", linear_in)\n",
    "\n",
    "\n",
    "        linear_out = self.linear(gru_hidden.squeeze())\n",
    "        print(\"\\nLinear Output:\\n\", linear_out)\n",
    "        \n",
    "        y_out = torch.log_softmax(linear_out, dim = 1)\n",
    "        print(\"\\nLog Softmax:\\n\", y_out)\n",
    "\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCnnRnnSample(\n",
      "  (word_embeddings): Embedding(18430, 5)\n",
      "  (conv1): Conv1d(5, 100, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(100, 10, kernel_size=(3,), stride=(1,))\n",
      "  (gru): GRU(10, 3, batch_first=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear): Linear(in_features=3, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model_sample = ModelCnnRnnSample(embedding_size=EMBEDDING_SIZE_SAMPLE, vocab_size=len(word2idx), hidden_size = HIDDEN_SIZE_SAMPLE, target_size=len(tag2idx))\n",
    "print(cnn_rnn_model_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output.\n",
    "\n",
    "output = [batch size, sent len, hid dim]  \n",
    "hidden = [batch size, 1, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded X_batch:  torch.Size([2, 528]) \n",
      "\n",
      "\n",
      "Embeddings:  torch.Size([2, 528, 5]) \n",
      " tensor([[[-1.2969,  1.2651,  0.3382,  1.5103, -0.4060],\n",
      "         [-0.5082, -0.8698,  2.2355, -0.1555, -0.2195],\n",
      "         [-2.1836,  0.2777,  0.6615,  0.4673, -0.5454],\n",
      "         ...,\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798],\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798],\n",
      "         [-0.6540, -1.6095, -0.1002, -0.6092, -0.9798]],\n",
      "\n",
      "        [[-0.4253,  0.2918, -0.1760,  0.1699, -0.4444],\n",
      "         [ 0.7468, -0.5874, -0.0963, -0.7011, -1.2802],\n",
      "         [-0.9968, -0.4778, -0.7409, -0.3505,  0.0995],\n",
      "         ...,\n",
      "         [-0.2624, -1.3074, -0.8999,  1.9583, -0.9732],\n",
      "         [-1.2004,  1.2386, -0.2015,  0.9880,  1.7878],\n",
      "         [ 1.8535,  0.1446, -0.2912, -0.2779,  1.5642]]]) \n",
      "\n",
      "\n",
      "Embeddings transposed for CNN:  torch.Size([2, 5, 528]) \n",
      " tensor([[[-1.2969, -0.5082, -2.1836,  ..., -0.6540, -0.6540, -0.6540],\n",
      "         [ 1.2651, -0.8698,  0.2777,  ..., -1.6095, -1.6095, -1.6095],\n",
      "         [ 0.3382,  2.2355,  0.6615,  ..., -0.1002, -0.1002, -0.1002],\n",
      "         [ 1.5103, -0.1555,  0.4673,  ..., -0.6092, -0.6092, -0.6092],\n",
      "         [-0.4060, -0.2195, -0.5454,  ..., -0.9798, -0.9798, -0.9798]],\n",
      "\n",
      "        [[-0.4253,  0.7468, -0.9968,  ..., -0.2624, -1.2004,  1.8535],\n",
      "         [ 0.2918, -0.5874, -0.4778,  ..., -1.3074,  1.2386,  0.1446],\n",
      "         [-0.1760, -0.0963, -0.7409,  ..., -0.8999, -0.2015, -0.2912],\n",
      "         [ 0.1699, -0.7011, -0.3505,  ...,  1.9583,  0.9880, -0.2779],\n",
      "         [-0.4444, -1.2802,  0.0995,  ..., -0.9732,  1.7878,  1.5642]]]) \n",
      "\n",
      "\n",
      "CNN output:  torch.Size([2, 10, 524]) \n",
      " tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0434,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1187, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0979, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4478, 0.2870, 0.3797,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0343, 0.0000, 0.0881,  ..., 0.0744, 0.1300, 0.1997],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],\n",
      "         [0.0000, 0.2147, 0.0418,  ..., 0.0000, 0.1958, 0.2644],\n",
      "         [0.1729, 0.0187, 0.1401,  ..., 0.2704, 0.0000, 0.1496]]])\n",
      "\n",
      "Maxpool output:  torch.Size([2, 10, 174]) \n",
      " tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0434, 0.0000, 0.1459,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1187, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0979, 0.2362, 0.4588,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4478, 0.2571, 0.3801,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1197, 0.0000],\n",
      "         [0.0881, 0.1657, 0.0976,  ..., 0.1268, 0.0156, 0.0744],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0542,  ..., 0.0742, 0.2620, 0.2890],\n",
      "         [0.2147, 0.0973, 0.3792,  ..., 0.1256, 0.4152, 0.0135],\n",
      "         [0.1729, 0.4282, 0.2988,  ..., 0.2768, 0.3120, 0.7133]]])\n",
      "\n",
      "RNN Input:  torch.Size([2, 174, 10]) \n",
      " tensor([[[0.0000, 0.0000, 0.0434,  ..., 0.1187, 0.0979, 0.4478],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2362, 0.2571],\n",
      "         [0.0000, 0.0000, 0.1459,  ..., 0.0749, 0.4588, 0.3801],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0881,  ..., 0.0000, 0.2147, 0.1729],\n",
      "         [0.0000, 0.0000, 0.1657,  ..., 0.0000, 0.0973, 0.4282],\n",
      "         [0.0000, 0.0000, 0.0976,  ..., 0.0542, 0.3792, 0.2988],\n",
      "         ...,\n",
      "         [0.1254, 0.0000, 0.1268,  ..., 0.0742, 0.1256, 0.2768],\n",
      "         [0.0000, 0.1197, 0.0156,  ..., 0.2620, 0.4152, 0.3120],\n",
      "         [0.0000, 0.0000, 0.0744,  ..., 0.2890, 0.0135, 0.7133]]])\n",
      "\n",
      "RNN Last Hidden:  torch.Size([1, 2, 3]) \n",
      " tensor([[[-0.5219, -0.2524,  0.2478],\n",
      "         [-0.2662, -0.1773,  0.2733]]])\n",
      "\n",
      "Linear Output:\n",
      " tensor([[ 0.3286,  0.4842,  0.5770, -0.0452,  0.0443],\n",
      "        [ 0.4093,  0.4469,  0.4572, -0.1054,  0.1956]])\n",
      "\n",
      "Log Softmax:\n",
      " tensor([[-1.5874, -1.4318, -1.3390, -1.9612, -1.8717],\n",
      "        [-1.5024, -1.4648, -1.4546, -2.0171, -1.7162]])\n",
      "\n",
      "Y Output Tag: \n",
      " tensor([2, 2])\n",
      "\n",
      "Actual Output: \n",
      "[tensor(3), tensor(2)]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in sample_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i) for i in x_batch]\n",
    "        y_batch = [torch.tensor(i) for i in y_batch]\n",
    "        \n",
    "        \n",
    "#         print(\"X batch: \", x_batch.size())\n",
    "#         print(\"\\ny batch: \", y_batch.size())\n",
    "        \n",
    "        y_out = cnn_rnn_model_sample(x_batch)\n",
    "                        \n",
    "        _, y_out_tag = torch.max(y_out, dim = 1)\n",
    "        print(\"\\nY Output Tag: \\n\", y_out_tag)\n",
    "        \n",
    "        print(\"\\nActual Output: \")\n",
    "        print(y_batch)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 512\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "STACKED_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestData(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Model Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCnnRnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(ModelCnnRnn, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_size, out_channels=512, kernel_size=3, stride=1, padding = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding = 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "#         self.gru = nn.GRU(input_size=128, hidden_size = hidden_size, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size = hidden_size, batch_first=True)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features = 512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_features = 256)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(num_features = 128)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=target_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x_batch):        \n",
    "        padded_batch = pad_sequence(x_batch, batch_first=True)        \n",
    "        embeds = self.word_embeddings(padded_batch)\n",
    "        embeds_t = embeds.transpose(1, 2)\n",
    "        \n",
    "        cnn1 = self.relu(self.conv1(embeds_t))\n",
    "        cnn1 = self.batchnorm1(cnn1)\n",
    "        \n",
    "        cnn2 = self.relu(self.conv2(cnn1))\n",
    "        cnn2 = self.batchnorm2(cnn2)\n",
    "        \n",
    "        cnn3 = self.relu(self.conv3(cnn2))\n",
    "        cnn3 = self.batchnorm3(cnn3)\n",
    "        \n",
    "        rnn_input = cnn3.transpose(1, 2)\n",
    "        \n",
    "        _, (lstm_h, _) = self.lstm(rnn_input)\n",
    "#         _, gru_h = self.gru(rnn_input)\n",
    "        \n",
    "        linear_out = self.linear(lstm_h.squeeze())\n",
    "        \n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCnnRnn(\n",
      "  (word_embeddings): Embedding(18430, 512)\n",
      "  (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (lstm): LSTM(128, 64, batch_first=True)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model = ModelCnnRnn(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx))\n",
    "\n",
    "cnn_rnn_model.to(device)\n",
    "print(cnn_rnn_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer =  optim.RMSprop(cnn_rnn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.72100 | Acc: 0.0\n",
      "Epoch 002: | Loss: 1.61047 | Acc: 0.0\n",
      "Epoch 003: | Loss: 1.60679 | Acc: 0.0\n",
      "Epoch 004: | Loss: 1.59421 | Acc: 0.0\n",
      "Epoch 005: | Loss: 1.57659 | Acc: 0.0\n",
      "Epoch 006: | Loss: 1.55578 | Acc: 0.0\n",
      "Epoch 007: | Loss: 1.50802 | Acc: 0.0\n",
      "Epoch 008: | Loss: 1.46886 | Acc: 0.0\n",
      "Epoch 009: | Loss: 1.48100 | Acc: 0.0\n",
      "Epoch 010: | Loss: 1.40322 | Acc: 0.0\n",
      "Epoch 011: | Loss: 1.38041 | Acc: 0.0\n",
      "Epoch 012: | Loss: 1.32096 | Acc: 0.0\n",
      "Epoch 013: | Loss: 1.26236 | Acc: 0.0\n",
      "Epoch 014: | Loss: 1.51816 | Acc: 0.0\n",
      "Epoch 015: | Loss: 1.22656 | Acc: 0.0\n",
      "Epoch 016: | Loss: 1.13486 | Acc: 0.0\n",
      "Epoch 017: | Loss: 1.09870 | Acc: 100.0\n",
      "Epoch 018: | Loss: 1.06625 | Acc: 0.0\n",
      "Epoch 019: | Loss: 1.03135 | Acc: 100.0\n",
      "Epoch 020: | Loss: 1.00036 | Acc: 0.0\n",
      "Epoch 021: | Loss: 0.98340 | Acc: 0.0\n",
      "Epoch 022: | Loss: 1.00223 | Acc: 0.0\n",
      "Epoch 023: | Loss: 1.03807 | Acc: 0.0\n",
      "Epoch 024: | Loss: 0.99341 | Acc: 0.0\n",
      "Epoch 025: | Loss: 0.94285 | Acc: 0.0\n",
      "Epoch 026: | Loss: 0.94952 | Acc: 0.0\n",
      "Epoch 027: | Loss: 0.90751 | Acc: 100.0\n",
      "Epoch 028: | Loss: 0.76821 | Acc: 100.0\n",
      "Epoch 029: | Loss: 0.73587 | Acc: 100.0\n",
      "Epoch 030: | Loss: 0.64985 | Acc: 100.0\n",
      "Epoch 031: | Loss: 0.57532 | Acc: 100.0\n",
      "Epoch 032: | Loss: 0.51152 | Acc: 100.0\n",
      "Epoch 033: | Loss: 0.50321 | Acc: 100.0\n",
      "Epoch 034: | Loss: 0.46024 | Acc: 100.0\n",
      "Epoch 035: | Loss: 0.44389 | Acc: 100.0\n",
      "Epoch 036: | Loss: 0.42021 | Acc: 100.0\n",
      "Epoch 037: | Loss: 0.33839 | Acc: 100.0\n",
      "Epoch 038: | Loss: 0.31310 | Acc: 100.0\n",
      "Epoch 039: | Loss: 0.26076 | Acc: 100.0\n",
      "Epoch 040: | Loss: 0.24640 | Acc: 100.0\n",
      "Epoch 041: | Loss: 0.24729 | Acc: 100.0\n",
      "Epoch 042: | Loss: 0.18364 | Acc: 100.0\n",
      "Epoch 043: | Loss: 0.15960 | Acc: 100.0\n",
      "Epoch 044: | Loss: 0.17342 | Acc: 100.0\n",
      "Epoch 045: | Loss: 0.19007 | Acc: 100.0\n",
      "Epoch 046: | Loss: 0.18087 | Acc: 100.0\n",
      "Epoch 047: | Loss: 0.13325 | Acc: 100.0\n",
      "Epoch 048: | Loss: 0.18606 | Acc: 100.0\n",
      "Epoch 049: | Loss: 0.13357 | Acc: 100.0\n",
      "Epoch 050: | Loss: 0.11185 | Acc: 100.0\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = cnn_rnn_model(x_batch)\n",
    "        \n",
    "        loss = criterion(y_pred.squeeze(0), y_batch)\n",
    "        acc = multi_acc(y_pred.squeeze(0), y_batch)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_tags_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = map(list, zip(*batch))\n",
    "        x_batch = [torch.tensor(i).to(device) for i in x_batch]\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "        \n",
    "        y_pred = cnn_rnn_model(x_batch)\n",
    "        y_pred = torch.log_softmax(y_pred, dim = 0)\n",
    "        _, y_pred_tag = torch.max(y_pred, dim = 0)\n",
    "        y_out_tags_list.append(y_pred_tag.squeeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67 38  2 18 14]\n",
      " [43 33  3 23 27]\n",
      " [35 44 13 41 26]\n",
      " [29 30  3 19 28]\n",
      " [28 39  6 25 34]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_out_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_tags_list = [a.squeeze().tolist() for a in y_out_tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.48      0.39       139\n",
      "           1       0.18      0.26      0.21       129\n",
      "           2       0.48      0.08      0.14       159\n",
      "           3       0.15      0.17      0.16       109\n",
      "           4       0.26      0.26      0.26       132\n",
      "\n",
      "    accuracy                           0.25       668\n",
      "   macro avg       0.28      0.25      0.23       668\n",
      "weighted avg       0.29      0.25      0.23       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_out_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                                                                            : Class          \n",
      "\n",
      "windows worm travels with tetris users are being warned about windows virus that poses the hugely popular tetris game the cellery worm installs playable version the classic falling blocks game pcs that has infected while users play the game the worm spends its time using the machine search for new victims infect nearby networks the risk infection cellery thought very low few copies the worm have been found the wild the cellery worm does not spread via mail like many other viruses instead computer networks for pcs that have not shut off all the insecure ways they connect other machines when infects machine cellery installs version tetris that users can play the game starts the worm also starts music file accompany the same time the virus starts networks for other vulnerable machines the virus does damage machines but heavily infected networks could slow down scanning traffic builds productivity may suffer too users spend time playing tetris pcs running windows and could vulnerable the worm your company has culture allowing games played the office your staff may believe this simply new game that has been installed rather than something that should cause concern said graham cluley spokesman for anti virus firm sophos far the number people infected cellery thought very small and the risks further infection very low sophos urged users and companies update their anti virus software keep themselves protected:     0\n",
      "\n",
      "sociedad set rescue mladenovic rangers are set loan out favour midfielder mladenovic real sociedad despite the closure the january transfer window sociedad have been given special permission the spanish sign player due injury crisis mladenovic will effectively replace former rangers midfielder mikel arteta who has been loaned everton sociedad say they will pay rangers with option buy the serbia montenegro international mladenovic loan move subject him passing medical the year old who joined rangers from red star belgrade for the close season expected san sebastian later this week following his national side game against bulgaria sociedad are place the strong primera liga just two points above the relegation zone special permission from the spanish came after injury central defender igor the versatile mladenovic can also play the back four his agent said last month that rangers had told him find the player new club mladenovic time ibrox has been plagued with injury and has made just six starts six months with the glasgow club:     0\n",
      "\n",
      "butler strikes gold spain britain kathy butler continued her impressive year with victory sunday cross spain the scot who led world cross country bronze earlier this year moved away from the field with halfway into the race she then shrugged off her portuguese rival win minutes seconds meanwhile briton karl keska battled bravely finish seventh the men race time kenenisa bekele ethiopia the reigning world long and short course champion was never troubled any the opposition winning leisurely butler said her success felt great throughout the race and hope this good beginning for marvellous season for elsewhere ethiopia won the brussels iaaf cross country race sunday completing the course then crowned great day for ethiopia claiming victory the women race:     4\n",
      "\n",
      "robertson out retain euro lure hearts manager john robertson hopes place the knock out stages the uefa cup could help keep some his out contract players the club could help get through and have another european tie may encourage players stay least until the end the season said manage get through shows how well the club progressing they have think whether they are going get other clubs like that should they decide move win for robertson side against would put them through the last fail beat feyenoord very much the player prerogative but the fact that been playing european football for the last three four years obviously incentive added robertson but want players who want play for the football club who are committed and run europe always helps little bit with the game being played murrayfield instead tynecastle because uefa regulations robertson sees both positive and negative aspects the change venue the pitch not the greatest condition the heineken cup game was there the weekend and the pitch bit said not ideal but the same for both teams just have out and there and perform that the most important thing but added tynecastle could have hosted would have been fantastic but that one the benefits murrayfield allows bring even more our supporters into there will good atmosphere and the hearts fans have important role play need their encouragement need them get right behind the side and make good atmosphere possible hopefully the players will respond that and know they will because fantastic european night for the club:     3\n",
      "\n",
      "gardener wins double glasgow britain jason gardener enjoyed double success glasgow his first competitive outing since won relay gold the athens olympics gardener cruised home ahead scot nick smith win the invitational race the norwich union international then recovered from poor start the second race beat swede daniel and italy luca his times and seconds were well short american maurice greene world record secs from very hard record break but believe trained very well said the world indoor champion who hopes get closer the mark this season was important come out and make sure got maximum points last race was the olympic final and there was lot expectation this was just what needed and get some race fitness very excited about the next couple months double olympic champion marked her first appearance home soil since winning and gold athens with victory there was third success for britain when edged out russia and sweden jenny win the women race secs maduaka was unable repeat the feat the finishing down fourth took the win for russia and the year old also missed out podium place the relay the british quartet came fourth with russia setting new world indoor record there was setback for jade johnson she suffered her back injury the long jump russia won the meeting with final total points with britain second and france one point behind third led the way for russia producing major shock the high jump beat olympic champion stefan into second place end the swede event unbeaten record won the triple jump with leap with britain fourth won the men pole vault competition with clearance with britain nick buckfield his personal best third and won the women with britain jenny meadows third there was yet another russian victory the women finished well clear britain catherine murphy chris lambert had settle for fourth after fading the closing stages the men race sweden held off leslie france france won the men with brett fourth for britain took victory for sweden the women hurdles ahead russia irina shevchenko and britain sarah claxton who set new personal best italy grabbed their first victory the men kicked over the last metres hold off britain james thie and france alexis abraham changeover the relay cost britain men the chance add further points france claimed victory:     4\n",
      "\n",
      "conservative defects labour conservative and former minister has defected labour robert jackson for oxfordshire said was disillusioned with the party leadership and its dangerous views europe prime minister tony blair declared himself delighted saying jackson would warmly welcomed labour mps jackson who has clashed with his leaders over tuition fees and europe the past served higher education minister between and letter his constituency chairman wrote the country best interest that tony blair rather than michael howard should form the next government while saying admired blair courageous leadership the country bitterly criticised the conservatives stance europe the conservative party hostility europe has now hardened the point which advocates the britain treaty obligations wrote blair said jackson was decent fair minded and dedicated public servant who will warmly welcome labour mps and members rightly says the conservatives have learned nothing from their two election defeats and are anything drifting further added spokesman for michael howard said jackson views policy issues were very different from those the party leadership believes students should pay tuition fees that tony blair should not criticised over his handling the iraq war and that more powers should given europe the spokesman said added that was not surprising jackson had chosen leave the conservatives jackson due stand down the next election the third conservative labour since:     1\n",
      "\n",
      "mobile bets pocket office mobile has launched its latest pocket office third generation device which also has built high speed wireless net access unlike other devices where the user has check which high speed network available transfer data the device selects the fastest one itself the mda released the summer upgrade the company existing smartphone the mda iii reflects the push mobile firms for devices that are like mini laptops the device has display that can swivelled and angled can used like small computer conventional clamshell phone the microsoft mobile phone with two cameras and qwerty keyboard reflects the design similar all one models released this year such motorola mpx one five european workers are already mobile meaning they spend significant time travelling and out the office rene obermann mobile chief executive told press conference the gsm trade show cannes added what they need their office when they are out the office mobile said was seeing increasing take for what calls office pocket devices with mdas sold europe already response demand mobile also said would adding the latest phone shaped blackberry its mobile range reflecting the growing need connected outside the office announced would introduce flat fee month tariff for people the using its hotspots said would nearly double the number its hotspots places where access available globally from also announced was installing high speed certain train services such the london brighton service provide commuters fast net connection too the service which has been developed with southern trains nomad digital who provide the technology begins with free trial trains the route from early march the end april full service set follow the summer access points will connected wimax wireless network faster than running alongside the train tracks brian mcbride managing director mobile the said see growing trend for business users needing access mail securely the move are able offer this maintaining constant data session for the entire journey said this was something other similar train services such that offered gner trains did not offer yet obermann added that the mobile industry general was still growing with many more opportunities for more services which would bear fruit for mobile companies future thousands mobile industry experts are gathered cannes france for the gsm which runs from february:     0\n",
      "\n",
      "castaignede fires laporte warning former france fly half thomas castaignede has warned the pressure mounting coach bernard laporte following their defeat wales france suffered shock loss against the welsh the weekend after looking course for easy win castaignede told bbc sport the pressure big laporte after huge loss new zealand slim win over scotland and miracle against england but the french have get behind him and the team lansdowne road following victories over south africa and australia november france were deemed many the world leading side but they were then trounced new zealand and only just beat scotland after the scots had try disallowed their six nations opener then took some woeful spot kicking from charlie hodgson and olly barkley help them victory against england twickenham castaignede said you can say any those results have eased the pressure laporte had england kickers not been bad the position the six nations would very different now laporte has been criticised for france negative tactics their wins over scotland and england but his side played more free flowing style against wales making mockery the opposition defence the first half before suffering shock turnaround fortunes after the interval all the chat france has been about how france will play against ireland said castaignede ahead the march tie everyone wants see the sort play saw against wales but everyone also wants win castaignede veteran international caps admitted the french would underdogs against ireland going ireland never easy but the way they playing right now harder than ever said castaignede they very experienced and don often lose home they got some great forwards and some electric runners the break despite praising the irish claimed the welsh had the upper hand the six nations run ireland have such good pack but wales are something else the break added the weekend they were simply awesome frenchman was disappointing see but you had admire their commitment every cause can make them win this championship the year old also tipped yann delaigue start ahead frederic michalak number after impressive display paris last weekend delaigue played really well and admittedly michalak played well too said castaignede just glad not the one who has make the decision:     0\n",
      "\n",
      "fit again betsen france squad france have brought flanker serge betsen back into their squad face england twickenham sunday but the player who missed the victory over scotland through injury must attend disciplinary hearing wednesday after being cited wasps serge has good case are confident will play said france coach bernard laporte the inexperienced nicolas mas jimmy marlu and jean philippe grandclaude are also included man squad the trio have been called after villiers ludovic valbon and aurelien rougerie all picked injuries france win saturday laporte said was confident that betsen would cleared the panel investigating his alleged trip that broke wasps centre stuart abbott leg was suspended would call imanol harinordoquy thomas said laporte who has dropped patrick missed serge badly against scotland has now recovered from his thigh injury and played saturday with biarritz france regular back row combination betsen harinordoquy and olivier were all missing from france side the weekend because injury laporte expected announce france starting line wednesday forwards nicolas mas sylvain marconnet olivier milloud william servat sebastien bruno fabien pelous jerome thion gregory lamboley serge betsen julien bonnaire sebastien chabal yannick nyanga backs dimitri yachvili pierre mignoni frederic michalak yann delaigue damien traille brian liebenberg jean philippe grandclaude christophe dominici jimmy marlu pepito elhorga:     2\n",
      "\n",
      "china aviation seeks rescue deal scandal hit jet fuel supplier china aviation oil has offered repay its creditors the lost trading oil futures the firm said hoped pay now and another over eight years with assets and liabilities totalling needs creditors backing for the offer avoid going into bankruptcy the trading scandal the biggest hit singapore since the collapse barings bank chen chief executive china aviation oil cao was arrested airport singapore police december was returning from china where had headed when cao announced its trading late november the firm had been betting heavily fall the price oil during october but prices rose sharply instead among the creditors whose backing cao needs for its restructuring plan are banking giants such barclay capital and sumitomo mitsui well south korean firm energy the immediate payment the firm china biggest jet fuel supplier said would paying out its own resources the rest would come from its parent company china aviation oil holding company beijing the holding company owned the chinese government holds most cao singapore listed shares cut its holding from october:     1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{:80}: {:15}\\n'.format(\"Word\", \"Class\"))\n",
    "for sentence, tag in zip(X_test[:10], y_out_tags_list[:10]):\n",
    "    s = \" \".join([idx2word[w] for w in sentence])\n",
    "    print('{:80}: {:5}\\n'.format(s, tag))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
