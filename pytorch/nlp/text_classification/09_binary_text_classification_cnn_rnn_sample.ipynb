{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Binary Text Classification using CNN+RNN - Sample\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com)  \n",
    "\n",
    "This notebook takes you through a sample implementation of binary text classification in the form of sentiment analysis on yelp reviews using CNN+RNN in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1bbfcb9470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "1. https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"Ronaldo loves apples and mangoes.\".split(), \"Postive\"),\n",
    "    (\"Rooney hates apple and bananas.\".split(), \"Negative\")\n",
    "]\n",
    "\n",
    "sentence_list = [training_data[x][0] for x in range(len(training_data))]\n",
    "tag_list = [training_data[x][1] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ronaldo', 'loves', 'apples', 'and', 'mangoes.'],\n",
       " ['Rooney', 'hates', 'apple', 'and', 'bananas.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Postive', 'Negative']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the input data by converting it into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_list = []\n",
    "for sentence, tags in training_data:\n",
    "    clean_sentence = [x.lower().split('.')[0] for x in sentence]\n",
    "    data_clean_list += [(clean_sentence, tags)]\n",
    "\n",
    "    \n",
    "sentence_clean_list = [data_clean_list[x][0] for x in range(len(data_clean_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ronaldo', 'loves', 'apples', 'and', 'mangoes'],\n",
       " ['rooney', 'hates', 'apple', 'and', 'bananas']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word-vocablury: 9\n",
      "\n",
      "['apples', 'apple', 'loves', 'and', 'ronaldo', 'bananas', 'mangoes', 'hates', 'rooney']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentence_clean_list:\n",
    "    words += sentence\n",
    "words = list(set(words))\n",
    "print(f\"Size of word-vocablury: {len(words)}\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for input <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apples': 0, 'apple': 1, 'loves': 2, 'and': 3, 'ronaldo': 4, 'bananas': 5, 'mangoes': 6, 'hates': 7, 'rooney': 8}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab for output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tag-vocab: 2\n",
      "\n",
      "['Negative', 'Postive']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in tag_list:\n",
    "    tags.append(tag)\n",
    "tags = list(set(tags))\n",
    "print(f\"Size of tag-vocab: {len(tags)}\\n\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary for output <=> ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Negative': 0, 'Postive': 1}\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {word: i for i, word in enumerate(tags)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ronaldo', 'loves', 'apples', 'and', 'mangoes'],\n",
       "  ['rooney', 'hates', 'apple', 'and', 'bananas']],\n",
       " ['Postive', 'Negative'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean_list, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 0, 3, 6], [8, 7, 1, 3, 5]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[w] for w in s] for s in sentence_clean_list]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [tag2idx[t] for t in tag_list]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Params and Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input -> CNN -> Linear -> Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 5\n",
    "HIDDEN_SIZE = 2\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(torch.Tensor(X).to(torch.long), torch.Tensor(y).to(torch.float32))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 2, 0, 3, 6]]) tensor([1.])\n",
      "tensor([[8, 7, 1, 3, 5]]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_loader:\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnRnnModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, target_size):\n",
    "        super(CnnRnnModel, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_size)\n",
    "        self.cnn = nn.Conv1d(in_channels=embedding_size, out_channels=3, kernel_size=3, stride=1, padding = 1)\n",
    "        self.gru = nn.GRU(input_size = 3, hidden_size=hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(in_features = hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds_t = embeds.transpose(1, 2)\n",
    "        cnn = torch.relu(self.cnn(embeds_t))\n",
    "        gru_input = cnn.transpose(1, 2)\n",
    "        _, gru_hidden = self.gru(gru_input)\n",
    "        \n",
    "#         cnn, _ = torch.max(cnn, dim = 2)\n",
    "#         cnn = torch.relu(cnn)\n",
    "        linear = self.linear(gru_hidden)\n",
    "        \n",
    "        return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnRnnModel(\n",
      "  (word_embeddings): Embedding(9, 5)\n",
      "  (cnn): Conv1d(5, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (gru): GRU(3, 2, batch_first=True)\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model = CnnRnnModel(embedding_size=EMBEDDING_SIZE, vocab_size=len(word2idx), hidden_size=HIDDEN_SIZE, target_size=len(tag2idx))\n",
    "print(cnn_rnn_model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer =  optim.Adam(cnn_rnn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the output from the CNN model looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " tensor([[4, 2, 0, 3, 6]])\n",
      "\n",
      "Linear Output: torch.Size([1, 1, 1])\n",
      "tensor([[[-0.0270]]])\n",
      "\n",
      "Sigmoid Output:\n",
      "tensor([[[0.4933]]])\n",
      "\n",
      "Output Indices:\n",
      "tensor([[[0.]]])\n",
      "\n",
      "Actual Output:\n",
      "tensor([1.])\n",
      "==================================================\n",
      "Input: \n",
      " tensor([[8, 7, 1, 3, 5]])\n",
      "\n",
      "Linear Output: torch.Size([1, 1, 1])\n",
      "tensor([[[0.0384]]])\n",
      "\n",
      "Sigmoid Output:\n",
      "tensor([[[0.5096]]])\n",
      "\n",
      "Output Indices:\n",
      "tensor([[[1.]]])\n",
      "\n",
      "Actual Output:\n",
      "tensor([0.])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        print(\"Input: \\n\", x_batch)\n",
    "        y_out = cnn_rnn_model(x_batch)\n",
    "        \n",
    "        y_out_sigmoid = torch.sigmoid(y_out)\n",
    "        y_out_tags = torch.round(y_out_sigmoid)\n",
    "\n",
    "        \n",
    "        print(\"\\nLinear Output:\", y_out.shape)\n",
    "        print(y_out)\n",
    "        \n",
    "        print(\"\\nSigmoid Output:\")\n",
    "        print(y_out_sigmoid)\n",
    "        \n",
    "        print(\"\\nOutput Indices:\")\n",
    "        print(y_out_tags)\n",
    "        \n",
    "        print(\"\\nActual Output:\")\n",
    "        print(y_batch)\n",
    "        \n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 0.71090 | Accuracy: 0.0\n",
      "Epoch: 02 | Loss: 0.70929 | Accuracy: 0.0\n",
      "Epoch: 03 | Loss: 0.70835 | Accuracy: 0.0\n",
      "Epoch: 04 | Loss: 0.70749 | Accuracy: 0.0\n",
      "Epoch: 05 | Loss: 0.70666 | Accuracy: 0.0\n",
      "Epoch: 06 | Loss: 0.70585 | Accuracy: 0.0\n",
      "Epoch: 07 | Loss: 0.70505 | Accuracy: 0.0\n",
      "Epoch: 08 | Loss: 0.70427 | Accuracy: 0.0\n",
      "Epoch: 09 | Loss: 0.70349 | Accuracy: 0.0\n",
      "Epoch: 10 | Loss: 0.70271 | Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model.train()\n",
    "for e in range(1, EPOCH+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_out = cnn_rnn_model(x_batch)\n",
    "        \n",
    "        loss = criterion(y_out.squeeze(0), y_batch.unsqueeze(0))\n",
    "        acc = binary_acc(y_out.squeeze(0), y_batch.unsqueeze(0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch: {e+0:02} | Loss: {epoch_loss/len(train_loader):.5f} | Accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
